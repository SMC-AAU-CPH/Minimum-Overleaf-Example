
@inproceedings{shi_u-gat-vc_2022,
	title = {U-{GAT}-{VC}: {Unsupervised} {Generative} {Attentional} {Networks} for {Non}-{Parallel} {Voice} {Conversion}},
	shorttitle = {U-{GAT}-{VC}},
	url = {https://ieeexplore.ieee.org/document/9746992},
	doi = {10.1109/ICASSP43922.2022.9746992},
	abstract = {Non-parallel voice conversion (VC) is a technique of transfer-ring voice from one style to another without using a parallel corpus in model training. Various methods are proposed to approach non-parallel VC using deep neural networks. Among them, CycleGAN-VC and its variants have been widely accepted as benchmark methods. However, there is still a gap to bridge between the real target and converted voice and an increased number of parameters leads to slow convergence in training process. Inspired by recent advancements in unsupervised image translation, we propose a new end-to-end unsupervised framework U-GAT-VC that adopts a novel inter- and intra-attention mechanism to guide the voice conversion to focus on more important regions in spectrograms. We also introduce disentangle perceptual loss in our model to capture high-level spectral features. Subjective and objective evaluations shows our proposed model outperforms CycleGAN-VC2/3 in terms of conversion quality and voice naturalness.},
	urldate = {2023-10-06},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Shi, Sheng and Shao, Jiahao and Hao, Yifei and Du, Yangzhou and Fan, Jianping},
	month = may,
	year = {2022},
	pages = {7017--7021},
}

@article{liu_one-shot_2022-1,
	title = {One-shot voice conversion using a combination of {U2}-{Net} and vector quantization},
	volume = {199},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X22003887},
	doi = {10.1016/j.apacoust.2022.109014},
	abstract = {Researchers are paying more and more attention on one-shot voice conversion because of its superiority on collecting data, which can convert a timbre of one speech from one source speaker to another target speaker even for unseen speakers in the training dataset. In our previous work, a two-level nested U-structure was developed for one-shot voice conversion, called U2-VC. It was shown that the multi-scale features extracted by the U2-Net structure is promising in improving the naturalness of the converted speech. However, the converted speech of U2-VC still suffers from the source speaker timbre leakage problem, caused by only using the instance normalization for disentanglement. To solve this problem, the vector quantization is designed to disentangle the content and speaker identity features from the extracted multi-scale features. Meanwhile, instead of using one segment taken from one utterance as both the source and target signals, two non-overlapped segments cut from one utterance are used as the source and the target during the training phase. Both objective and subjective evaluation results show that the proposed voice conversion method effectively improves the converted speech quality when compared with the original U2-VC and other state-of-the-art baselines.},
	urldate = {2023-10-06},
	journal = {Applied Acoustics},
	author = {Liu, Fangkun and Wang, Hui and Ke, Yuxuan and Zheng, Chengshi},
	month = oct,
	year = {2022},
	keywords = {Multi-scale features, One-shot voice conversion, U-Net, U-VC, Vector quantization},
	pages = {109014},
}

@inproceedings{liu_non-parallel_2021-1,
	title = {Non-{Parallel} {Any}-to-{Many} {Voice} {Conversion} by {Replacing} {Speaker} {Statistics}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/liu21c_interspeech.html},
	doi = {10.21437/Interspeech.2021-557},
	abstract = {This paper proposes a non-parallel any-to-many voice conversion (VC) approach with a novel statistics replacement layer. Non-parallel VC is usually achieved by ﬁrstly disentangling linguistic and speaker representations, and then concatenating the linguistic content with the learned target speaker’s embedding at the conversion stage. While such a concatenation-based approach could introduce speaker-speciﬁc characteristics into the network, it is not very effective as it entirely relies on the network to learn to combine the linguistic content and the speaker characteristics. Inspired by X-vectors, where the statistics of hidden representation such as means and standard deviations are used for speaker differentiation, we propose a statistics replacement layer in VC systems to directly modify the hidden states to have the target speaker’s statistics. The speaker-speciﬁc statistics of hidden states are learned for each target speaker during training and are used as guidance for the statistics replacement layer during inference. Moreover, to better concentrate the speaker information into the statistics of hidden representation, a multitask training with X-vector based speaker classiﬁcation is also performed. Experimental results with Librispeech and VCTK datasets show that the proposed method can effectively improve the converted speech’s naturalness and similarity.},
	language = {en},
	urldate = {2023-10-06},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Liu, Yufei and Yu, Chengzhu and Shuai, Wang and Yang, Zhenchuan and Chao, Yang and Zhang, Weibin},
	month = aug,
	year = {2021},
	pages = {1369--1373},
}

@misc{wu_vqvc_2020,
	title = {{VQVC}+: {One}-{Shot} {Voice} {Conversion} by {Vector} {Quantization} and {U}-{Net} architecture},
	shorttitle = {{VQVC}+},
	url = {http://arxiv.org/abs/2006.04154},
	abstract = {Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content. It is still a challenging work, especially in a one-shot setting. Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers. The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN). However, the imperfect disentanglement may harm the quality of output speech. In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system. We find that to leverage the U-Net architecture, a strong information bottleneck is necessary. The VQ-based method, which quantizes the latent vectors, can serve the purpose. The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wu, Da-Yi and Chen, Yen-Hao and Lee, Hung-Yi},
	month = jun,
	year = {2020},
	doi = {10.48550/arXiv.2006.04154},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{sisman_group_2019,
	title = {Group {Sparse} {Representation} {With} {WaveNet} {Vocoder} {Adaptation} for {Spectrum} and {Prosody} {Conversion}},
	volume = {27},
	issn = {2329-9304},
	url = {https://ieeexplore.ieee.org/document/8688454},
	doi = {10.1109/TASLP.2019.2910637},
	abstract = {The statistical approach to voice conversion typically consists of a feature conversion module followed by a vocoder. So far, the feature conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by prosodic features, such as fundamental frequency (F0) and energy contour among others. In this paper, we study the transformation of speaker characteristics both in terms of spectrum and prosody. We propose two novel techniques that effectively use a limited amount of source-target training data and leverage a large general speech corpus to improve the voice conversion quality. First, we study the phonetic sparse representation under the group sparsity mathematical formulation. We use phonetic posteriorgrams (PPGs) together with spectral and prosody features to form tandem feature in the phonetic dictionary. The tandem feature allow us to estimate an activation matrix that is less dependent on source speakers, thus providing a better voice conversion quality. Second, we study the use of WaveNet vocoder that can be trained on general speech corpus from multiple speakers and adapted on target speaker data to improve the vocoding quality. We benefit from the large general speech databases that are used to train the PPG generator, and the WaveNet vocoder. The experiments show that the proposed conversion framework outperforms the traditional spectrum and prosody conversion techniques in both objective and subjective evaluations.},
	number = {6},
	urldate = {2023-10-06},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
	month = jun,
	year = {2019},
	pages = {1085--1097},
}

@misc{ocal_adversarially_2019,
	title = {Adversarially {Trained} {Autoencoders} for {Parallel}-{Data}-{Free} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1905.03864},
	abstract = {We present a method for converting the voices between a set of speakers. Our method is based on training multiple autoencoder paths, where there is a single speaker-independent encoder and multiple speaker-dependent decoders. The autoencoders are trained with an addition of an adversarial loss which is provided by an auxiliary classifier in order to guide the output of the encoder to be speaker independent. The training of the model is unsupervised in the sense that it does not require collecting the same utterances from the speakers nor does it require time aligning over phonemes. Due to the use of a single encoder, our method can generalize to converting the voice of out-of-training speakers to speakers in the training dataset. We present subjective tests corroborating the performance of our method.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Ocal, Orhan and Elibol, Oguz H. and Keskin, Gokce and Stephenson, Cory and Thomas, Anil and Ramchandran, Kannan},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1905.03864},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{long_enhancing_2022-1,
	title = {Enhancing {Zero}-{Shot} {Many} to {Many} {Voice} {Conversion} with {Self}-{Attention} {VAE}},
	url = {http://arxiv.org/abs/2203.16037},
	abstract = {Variational auto-encoder (VAE) is an effective neural network architecture to disentangle a speech utterance into speaker identity and linguistic content latent embeddings, then generate an utterance for a target speaker from that of a source speaker. This is possible by concatenating the identity embedding of the target speaker and the content embedding of the source speaker uttering a desired sentence. In this work, we propose to improve VAE models with self-attention and structural regularization (RGSM). Specifically, we found a suitable location of VAE's decoder to add a self-attention layer for incorporating non-local information in generating a converted utterance and hiding the source speaker's identity. We applied relaxed group-wise splitting method (RGSM) to regularize network weights and remarkably enhance generalization performance. In experiments of zero-shot many-to-many voice conversion task on VCTK data set, with the self-attention layer and relaxed group-wise splitting method, our model achieves a gain of speaker classification accuracy on unseen speakers by 28.3{\textbackslash}\% while slightly improved conversion voice quality in terms of MOSNet scores. Our encouraging findings point to future research on integrating more variety of attention structures in VAE framework while controlling model size and overfitting for advancing zero-shot many-to-many voice conversions.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Long, Ziang and Zheng, Yunling and Yu, Meng and Xin, Jack},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2203.16037},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{lee_duration_2022,
	title = {Duration {Controllable} {Voice} {Conversion} via {Phoneme}-{Based} {Information} {Bottleneck}},
	volume = {30},
	issn = {2329-9304},
	url = {https://ieeexplore.ieee.org/document/9729483},
	doi = {10.1109/TASLP.2022.3156757},
	abstract = {Several voice conversion (VC) methods using a simple autoencoder with a carefully designed information bottleneck have recently been studied. In general, they extract content information from a given speech through the information bottleneck between the encoder and the decoder, providing it to the decoder along with the target speaker information to generate the converted speech. However, their performance is highly dependent on the downsampling factor of an information bottleneck. In addition, such frame-by-frame conversion methods cannot convert speaking styles associated with the length of utterance, such as the duration. In this paper, we propose a novel duration controllable voice conversion (DCVC) model, which can transfer the speaking style and control the speed of the converted speech through a phoneme-based information bottleneck. The proposed information bottleneck does not need to find an appropriate downsampling factor, achieving a better audio quality and VC performance. In our experiments, DCVC outperformed the baseline models with a 3.78 MOS and a 3.83 similarity score. It can also smoothly control the speech duration while achieving a 39.35x speedup compared with a Seq2seq-based VC in terms of the inference speed.},
	urldate = {2023-10-06},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Lee, Sang-Hoon and Noh, Hyeong-Rae and Nam, Woo-Jeoung and Lee, Seong-Whan},
	year = {2022},
	pages = {1173--1183},
}

@inproceedings{sekii_fast_2017-1,
	address = {Porto, Portugal},
	title = {Fast {Many}-to-{One} {Voice} {Conversion} using {Autoencoders}:},
	isbn = {978-989-758-219-6 978-989-758-220-2},
	shorttitle = {Fast {Many}-to-{One} {Voice} {Conversion} using {Autoencoders}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006193301640174},
	doi = {10.5220/0006193301640174},
	abstract = {Voice Conversion, Autoencoder, Deep Learning, Deep Neural Network, Spectral Envelope.},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Sekii, Yusuke and Orihara, Ryohei and Kojima, Keisuke and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
	year = {2017},
	pages = {164--174},
}

@inproceedings{sekii_fast_2017,
	address = {Porto, Portugal},
	title = {Fast {Many}-to-{One} {Voice} {Conversion} using {Autoencoders}:},
	isbn = {978-989-758-219-6 978-989-758-220-2},
	shorttitle = {Fast {Many}-to-{One} {Voice} {Conversion} using {Autoencoders}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006193301640174},
	doi = {10.5220/0006193301640174},
	abstract = {Voice Conversion, Autoencoder, Deep Learning, Deep Neural Network, Spectral Envelope.},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Sekii, Yusuke and Orihara, Ryohei and Kojima, Keisuke and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
	year = {2017},
	pages = {164--174},
}

@misc{yeh_rhythm-flexible_2018,
	title = {Rhythm-{Flexible} {Voice} {Conversion} without {Parallel} {Data} {Using} {Cycle}-{GAN} over {Phoneme} {Posteriorgram} {Sequences}},
	url = {http://arxiv.org/abs/1808.03113},
	abstract = {Speaking rate refers to the average number of phonemes within some unit time, while the rhythmic patterns refer to duration distributions for realizations of different phonemes within different phonetic structures. Both are key components of prosody in speech, which is different for different speakers. Models like cycle-consistent adversarial network (Cycle-GAN) and variational auto-encoder (VAE) have been successfully applied to voice conversion tasks without parallel data. However, due to the neural network architectures and feature vectors chosen for these approaches, the length of the predicted utterance has to be fixed to that of the input utterance, which limits the flexibility in mimicking the speaking rates and rhythmic patterns for the target speaker. On the other hand, sequence-to-sequence learning model was used to remove the above length constraint, but parallel training data are needed. In this paper, we propose an approach utilizing sequence-to-sequence model trained with unsupervised Cycle-GAN to perform the transformation between the phoneme posteriorgram sequences for different speakers. In this way, the length constraint mentioned above is removed to offer rhythm-flexible voice conversion without requiring parallel data. Preliminary evaluation on two datasets showed very encouraging results.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Yeh, Cheng-chieh and Hsu, Po-chun and Chou, Ju-chieh and Lee, Hung-yi and Lee, Lin-shan},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.03113},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{mohammadi_investigation_2018,
	title = {Investigation of {Using} {Disentangled} and {Interpretable} {Representations} for {One}-shot {Cross}-lingual {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1808.05294},
	abstract = {We study the problem of cross-lingual voice conversion in non-parallel speech corpora and one-shot learning setting. Most prior work require either parallel speech corpora or enough amount of training data from a target speaker. However, we convert an arbitrary sentences of an arbitrary source speaker to target speaker's given only one target speaker training utterance. To achieve this, we formulate the problem as learning disentangled speaker-specific and context-specific representations and follow the idea of [1] which uses Factorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE on multi-speaker training data, given arbitrary source and target speakers' utterance, we estimate those latent representations and then reconstruct the desired utterance of converted voice to that of target speaker. We investigate the effectiveness of the approach by conducting voice conversion experiments with varying size of training utterances and it was able to achieve reasonable performance with even just one training utterance. We also examine the speech representation and show that World vocoder outperforms Short-time Fourier Transform (STFT) used in [1]. Finally, in the subjective tests, for one language and cross-lingual voice conversion, our approach achieved significantly better or comparable results compared to VAE-STFT and GMM baselines in speech quality and similarity.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Mohammadi, Seyed Hamidreza and Kim, Taehwan},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.05294},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{lumban_tobing_voice_2019,
	title = {Voice {Conversion} {With} {CycleRNN}-{Based} {Spectral} {Mapping} and {Finely} {Tuned} {WaveNet} {Vocoder}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8913551},
	doi = {10.1109/ACCESS.2019.2955978},
	abstract = {In this paper, we present a novel framework for a voice conversion (VC) system based on a cyclic recurrent neural network (CycleRNN) and a finely tuned WaveNet vocoder. Even though WaveNet is capable of producing natural speech waveforms when fed with natural speech features, it still suffers from speech quality degradation when fed with oversmoothed features, such as spectral parameters estimated from a statistical model. One way to address this problem is to introduce oversmoothed features while developing a WaveNet model. However, in a VC framework, providing oversmoothed spectral features of a target speaker for WaveNet modeling is not straightforward owing to the difference in the time-sequence alignment from that of a source speaker. To overcome this problem, we propose the use of a cyclic spectral conversion network, i.e., CycleRNN, capable of performing a conversion flow, i.e., source-to-target, and a cyclic flow, i.e., to generate self-predicted target spectra. The CycleRNN spectral model is trained using both conversion and weighted cyclic losses. To finely tune WaveNet, a pretrained multispeaker WaveNet model is optimized using the self-predicted features of the corresponding target speaker of a speaker conversion pair. The experimental results demonstrate that 1) the proposed CycleRNN-based spectral model for WaveNet fine-tuning significantly improves the naturalness of the converted speech waveforms, giving an overall mean opinion score of 3.50; and 2) the proposed model yields the highest speaker conversion accuracy, giving an overall speaker similarity score of 78.33\%, which is a significant improvement compared with conventional WaveNet fine-tuning using natural target features.},
	urldate = {2023-09-29},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Lumban Tobing, Patrick and Wu, Yi-Chiao and Hayashi, Tomoki and Kobayashi, Kazuhiro and Toda, Tomoki},
	year = {2019},
	pages = {171114--171125},
}

@inproceedings{ho_non-parallel_2019,
	address = {Lanzhou, China},
	title = {Non-parallel {Voice} {Conversion} with {Controllable} {Speaker} {Individuality} using {Variational} {Autoencoder}},
	isbn = {978-1-72813-248-8},
	url = {https://ieeexplore.ieee.org/document/9023264/},
	doi = {10.1109/APSIPAASC47483.2019.9023264},
	abstract = {We propose a ﬂexible non-parallel voice conversion (VC) system that is capable of both performing speaker adaptation and controlling speaker individuality. The proposed VC framework aims to tackle the inability to arbitrarily modify voice characteristics in the converted waveform of conventional VC model. To achieve this goal, we use the speaker embedding realized by a Variational Autoencoder (VAE) instead of onehot encoded vectors to represent and modify the target voice’s characteristics. Neither parallel training data, linguistic label nor time alignment procedure is required to train our system. After training on a multi-speaker speech database, the proposed VC system can adapt an arbitrary source speaker to any target speaker using only one sample from a target speaker. The speaker individuality of converted speech can be controlled by modifying the speaker embedding vectors; resulting in a ﬁctitious speaker individuality. The experimental results showed that our proposed system is similar to conventional non-parallel VAE-based VC and better than the parallel Gaussian Mixture Model (GMM) in both perceived speech naturalness and speaker similarity; even when our system only uses one sample from target speaker. Moreover, our proposed system can convert a source voice to a ﬁctitious target voice with well perceived speech naturalness of 3.1 MOS.},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Ho, Tuan Vu and Akagi, Masato},
	month = nov,
	year = {2019},
	pages = {106--111},
}

@misc{chou_multi-target_2018,
	title = {Multi-target {Voice} {Conversion} without {Parallel} {Data} by {Adversarially} {Learning} {Disentangled} {Audio} {Representations}},
	url = {http://arxiv.org/abs/1804.02812},
	abstract = {Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Chou, Ju-chieh and Yeh, Cheng-chieh and Lee, Hung-yi and Lee, Lin-shan},
	month = jun,
	year = {2018},
	doi = {10.48550/arXiv.1804.02812},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{miao_blstm_2019,
	title = {A {BLSTM} and {WaveNet}-{Based} {Voice} {Conversion} {Method} {With} {Waveform} {Collapse} {Suppression} by {Post}-{Processing}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8695725/},
	doi = {10.1109/ACCESS.2019.2912926},
	abstract = {In recent years, voice conversion methods based on neural networks have been developing rapidly. There are many different models and neural networks which can be used in parallel voice conversion. However, the problem of over-smoothing still affects the quality of the converted voice. To overcome this problem, we propose a bidirectional long short-term memory (BLSTM) and WaveNet based voice conversion method with waveform collapse suppression by post-processing. The method converts acoustic features through BLSTM and then synthesizes pre-converted voice with WaveNet. Subsequently, several alternative iterations of BLSTM post-processing is performed, and the ﬁnal converted voice is generated by WaveNet. The proposed method can directly generate converted audio waveforms and avoid the waveform-collapsed speech caused by a single WaveNet generation effectively. Experimental results indicate that acoustic features trained by using the BLSTM network could achieve better conversion results than conventional baselines. At the same time, the usage of WaveNet could solve the problem of oversmoothing, which contributes to improving the similarity and naturalness of the ﬁnal results of voice conversion.},
	language = {en},
	urldate = {2023-09-26},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Miao, Xiaokong and Zhang, Xiongwei and Sun, Meng and Zheng, Changyan and Cao, Tieyong},
	year = {2019},
	pages = {54321--54329},
}

@misc{kameoka_acvae-vc_2020,
	title = {{ACVAE}-{VC}: {Non}-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder},
	shorttitle = {{ACVAE}-{VC}},
	url = {http://arxiv.org/abs/1808.05092},
	abstract = {This paper proposes a non-parallel many-to-many voice conversion (VC) method using a variant of the conditional variational autoencoder (VAE) called an auxiliary classifier VAE (ACVAE). The proposed method has three key features. First, it adopts fully convolutional architectures to construct the encoder and decoder networks so that the networks can learn conversion rules that capture time dependencies in the acoustic feature sequences of source and target speech. Second, it uses an information-theoretic regularization for the model training to ensure that the information in the attribute class label will not be lost in the conversion process. With regular CVAEs, the encoder and decoder are free to ignore the attribute class label input. This can be problematic since in such a situation, the attribute class label will have little effect on controlling the voice characteristics of input speech at test time. Such situations can be avoided by introducing an auxiliary classifier and training the encoder and decoder so that the attribute classes of the decoder outputs are correctly predicted by the classifier. Third, it avoids producing buzzy-sounding speech at test time by simply transplanting the spectral details of the input speech into its converted version. Subjective evaluation experiments revealed that this simple method worked reasonably well in a non-parallel many-to-many speaker identity conversion task.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Kameoka, Hirokazu and Kaneko, Takuhiro and Tanaka, Kou and Hojo, Nobukatsu},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.1808.05092},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{kaneko_cyclegan-vc_2018,
	address = {Rome},
	title = {{CycleGAN}-{VC}: {Non}-parallel {Voice} {Conversion} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-90-827970-1-5},
	shorttitle = {{CycleGAN}-{VC}},
	url = {https://ieeexplore.ieee.org/document/8553236/},
	doi = {10.23919/EUSIPCO.2018.8553236},
	abstract = {We propose a non-parallel voice-conversion (VC) method that can learn a mapping from source to target speech without relying on parallel data. The proposed method is particularly noteworthy in that it is general purpose and high quality and works without any extra data, modules, or alignment procedure. Our method, called CycleGAN-VC, uses a cycleconsistent adversarial network (CycleGAN) with gated convolutional neural networks (CNNs) and an identity-mapping loss. A CycleGAN learns forward and inverse mappings simultaneously using adversarial and cycle-consistency losses. This makes it possible to ﬁnd an optimal pseudo pair from non-parallel data. Furthermore, the adversarial loss can bring the converted speech close to the target one on the basis of indistinguishability without explicit density estimation. This allows to avoid oversmoothing caused by statistical averaging, which occurs in many conventional statistical model-based VC methods that represent data distribution explicitly. We conﬁgure a CycleGAN with gated CNNs and train it with an identity-mapping loss. This allows the mapping function to capture sequential and hierarchical structures while preserving linguistic information. We evaluated our method on a non-parallel VC task. An objective evaluation showed that the converted feature sequence was near natural in terms of global variance and modulation spectra, which are structural indicators highly correlated with subjective evaluation. A subjective evaluation showed that the quality of the converted speech was comparable to that obtained with a Gaussian mixture model-based parallel VC method even though CycleGAN-VC is trained under disadvantageous conditions (non-parallel and half the amount of data).},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {2018 26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Kaneko, Takuhiro and Kameoka, Hirokazu},
	month = sep,
	year = {2018},
	pages = {2100--2104},
}

@misc{huang_investigation_2019,
	title = {Investigation of {F0} conditioning and {Fully} {Convolutional} {Networks} in {Variational} {Autoencoder} based {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1905.00615},
	abstract = {In this work, we investigate the effectiveness of two techniques for improving variational autoencoder (VAE) based voice conversion (VC). First, we reconsider the relationship between vocoder features extracted using the high quality vocoders adopted in conventional VC systems, and hypothesize that the spectral features are in fact F0 dependent. Such hypothesis implies that during the conversion phase, the latent codes and the converted features in VAE based VC are in fact source F0 dependent. To this end, we propose to utilize the F0 as an additional input of the decoder. The model can learn to disentangle the latent code from the F0 and thus generates converted F0 dependent converted features. Second, to better capture temporal dependencies of the spectral features and the F0 pattern, we replace the frame wise conversion structure in the original VAE based VC framework with a fully convolutional network structure. Our experiments demonstrate that the degree of disentanglement as well as the naturalness of the converted speech are indeed improved.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Huang, Wen-Chin and Wu, Yi-Chiao and Lo, Chen-Chou and Tobing, Patrick Lumban and Hayashi, Tomoki and Kobayashi, Kazuhiro and Toda, Tomoki and Tsao, Yu and Wang, Hsin-Min},
	month = jul,
	year = {2019},
	doi = {10.48550/arXiv.1905.00615},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{kaneko_cyclegan-vc2_2019,
	title = {{CycleGAN}-{VC2}: {Improved} {CycleGAN}-based {Non}-parallel {Voice} {Conversion}},
	shorttitle = {{CycleGAN}-{VC2}},
	url = {http://arxiv.org/abs/1904.04631},
	abstract = {Non-parallel voice conversion (VC) is a technique for learning the mapping from source to target speech without relying on parallel data. This is an important task, but it has been challenging due to the disadvantages of the training conditions. Recently, CycleGAN-VC has provided a breakthrough and performed comparably to a parallel VC method without relying on any extra data, modules, or time alignment procedures. However, there is still a large gap between the real target and converted speech, and bridging this gap remains a challenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved version of CycleGAN-VC incorporating three new techniques: an improved objective (two-step adversarial losses), improved generator (2-1-2D CNN), and improved discriminator (PatchGAN). We evaluated our method on a non-parallel VC task and analyzed the effect of each technique in detail. An objective evaluation showed that these techniques help bring the converted feature sequence closer to the target in terms of both global and local structures, which we assess by using Mel-cepstral distortion and modulation spectra distance, respectively. A subjective evaluation showed that CycleGAN-VC2 outperforms CycleGAN-VC in terms of naturalness and similarity for every speaker pair, including intra-gender and inter-gender pairs.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Kaneko, Takuhiro and Kameoka, Hirokazu and Tanaka, Kou and Hojo, Nobukatsu},
	month = apr,
	year = {2019},
	doi = {10.48550/arXiv.1904.04631},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{zhang_sequence--sequence_2019,
	title = {Sequence-to-{Sequence} {Acoustic} {Modeling} for {Voice} {Conversion}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1810.06865},
	doi = {10.1109/TASLP.2019.2892235},
	abstract = {In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition (ASR) model are appended as auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as acoustic models. This proposed method also outperformed our previous work which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.},
	number = {3},
	urldate = {2023-09-26},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Jing-Xuan and Ling, Zhen-Hua and Liu, Li-Juan and Jiang, Yuan and Dai, Li-Rong},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {631--644},
}

@misc{stephenson_semi-supervised_2019,
	title = {Semi-supervised voice conversion with amortized variational inference},
	url = {http://arxiv.org/abs/1910.00067},
	abstract = {In this work we introduce a semi-supervised approach to the voice conversion problem, in which speech from a source speaker is converted into speech of a target speaker. The proposed method makes use of both parallel and non-parallel utterances from the source and target simultaneously during training. This approach can be used to extend existing parallel data voice conversion systems such that they can be trained with semi-supervision. We show that incorporating semi-supervision improves the voice conversion performance compared to fully supervised training when the number of parallel utterances is limited as in many practical applications. Additionally, we find that increasing the number non-parallel utterances used in training continues to improve performance when the amount of parallel training data is held constant.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Stephenson, Cory and Keskin, Gokce and Thomas, Anil and Elibol, Oguz H.},
	month = sep,
	year = {2019},
	doi = {10.48550/arXiv.1910.00067},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{kaneko_stargan-vc2_2019,
	title = {{StarGAN}-{VC2}: {Rethinking} {Conditional} {Methods} for {StarGAN}-{Based} {Voice} {Conversion}},
	shorttitle = {{StarGAN}-{VC2}},
	url = {http://arxiv.org/abs/1907.12279},
	abstract = {Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data. This is important but challenging owing to the requirement of learning multiple mappings and the non-availability of explicit supervision. Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator. However, there is still a gap between real and converted speech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2. Particularly, we rethink conditional methods in two aspects: training objectives and network architectures. For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data. For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner. We evaluated our methods on non-parallel multi-speaker VC. An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures. Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity. The converted speech samples are provided at http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Kaneko, Takuhiro and Kameoka, Hirokazu and Tanaka, Kou and Hojo, Nobukatsu},
	month = aug,
	year = {2019},
	doi = {10.48550/arXiv.1907.12279},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{zhao_foreign_2019,
	title = {Foreign {Accent} {Conversion} by {Synthesizing} {Speech} from {Phonetic} {Posteriorgrams}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/zhao19f_interspeech.html},
	doi = {10.21437/Interspeech.2019-1778},
	abstract = {Methods for foreign accent conversion (FAC) aim to generate speech that sounds similar to a given non-native speaker but with the accent of a native speaker. Conventional FAC methods borrow excitation information (F0 and aperiodicity; produced by a conventional vocoder) from a reference (i.e., native) utterance during synthesis time. As such, the generated speech retains some aspects of the voice quality of the native speaker. We present a framework for FAC that eliminates the need for conventional vocoders (e.g., STRAIGHT, World) and therefore the need to use the native speaker’s excitation. Our approach uses an acoustic model trained on a native speech corpus to extract speaker-independent phonetic posteriorgrams (PPGs), and then train a speech synthesizer to map PPGs from the non-native speaker into the corresponding spectral features, which in turn are converted into the audio waveform using a high-quality neural vocoder. At runtime, we drive the synthesizer with the PPG extracted from a native reference utterance. Listening tests show that the proposed system produces speech that sounds more clear, natural, and similar to the non-native speaker compared with a baseline system, while significantly reducing the perceived foreign accent of nonnative utterances.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Zhao, Guanlong and Ding, Shaojin and Gutierrez-Osuna, Ricardo},
	month = sep,
	year = {2019},
	pages = {2843--2847},
}

@inproceedings{mohammadi_one-shot_2019,
	title = {One-{Shot} {Voice} {Conversion} with {Disentangled} {Representations} by {Leveraging} {Phonetic} {Posteriorgrams}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/mohammadi19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1798},
	abstract = {We propose voice conversion model from arbitrary source speaker to arbitrary target speaker with disentangled representations. Voice conversion is a task to convert the voice of spoken utterance of source speaker to that of target speaker. Most prior work require to know either source speaker or target speaker or both in training, with either parallel or non-parallel corpus. Instead, we study the problem of voice conversion in nonparallel speech corpora and one-shot learning setting. We convert an arbitrary sentences of an arbitrary source speaker to target speakers given only one or few target speaker training utterances. To achieve this, we propose to use disentangled representations of speaker identity and linguistic context. We use a recurrent neural network (RNN) encoder for speaker embedding and phonetic posteriorgram as linguistic context encoding, along with a RNN decoder to generate converted utterances. Ours is a simpler model without adversarial training or hierarchical model design and thus more efﬁcient. In the subjective tests, our approach achieved signiﬁcantly better results compared to baseline regarding similarity.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Mohammadi, Seyed Hamidreza and Kim, Taehwan},
	month = sep,
	year = {2019},
	pages = {704--708},
}

@misc{zhou_modularized_2019,
	title = {A {Modularized} {Neural} {Network} with {Language}-{Specific} {Output} {Layers} for {Cross}-lingual {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1910.00496},
	abstract = {This paper presents a cross-lingual voice conversion framework that adopts a modularized neural network. The modularized neural network has a common input structure that is shared for both languages, and two separate output modules, one for each language. The idea is motivated by the fact that phonetic systems of languages are similar because humans share a common vocal production system, but acoustic renderings, such as prosody and phonotactic, vary a lot from language to language. The modularized neural network is trained to map Phonetic PosteriorGram (PPG) to acoustic features for multiple speakers. It is conditioned on a speaker i-vector to generate the desired target voice. We validated the idea between English and Mandarin languages in objective and subjective tests. In addition, mixed-lingual PPG derived from a unified English-Mandarin acoustic model is proposed to capture the linguistic information from both languages. It is found that our proposed modularized neural network significantly outperforms the baseline approaches in terms of speech quality and speaker individuality, and mixed-lingual PPG representation further improves the conversion performance.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Zhou, Yi and Tian, Xiaohai and Yılmaz, Emre and Das, Rohan Kumar and Li, Haizhou},
	month = oct,
	year = {2019},
	doi = {10.48550/arXiv.1910.00496},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{liu_jointly_2019,
	title = {Jointly {Trained} {Conversion} {Model} and {WaveNet} {Vocoder} for {Non}-{Parallel} {Voice} {Conversion} {Using} {Mel}-{Spectrograms} and {Phonetic} {Posteriorgrams}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/liu19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1316},
	abstract = {The N10 system in the Voice Conversion Challenge 2018 (VCC 2018) has achieved high voice conversion (VC) performance in terms of speech naturalness and speaker similarity. We believe that further improvements can be gained from joint optimization (instead of separate optimization) of the conversion model and WaveNet vocoder, as well as leveraging information from the acoustic representation of the speech waveform, e.g. from Mel-spectrograms. In this paper, we propose a VC architecture to jointly train a conversion model that maps phonetic posteriorgrams (PPGs) to Mel-spectrograms and a WaveNet vocoder. The conversion model has a bottle-neck layer, whose outputs are concatenated with PPGs before being fed into the WaveNet vocoder as local conditioning. A weighted sum of a Melspectrogram prediction loss and a WaveNet loss is used as the objective function to jointly optimize parameters of the conversion model and the WaveNet vocoder. Objective and subjective evaluation results show that the proposed approach is capable of achieving signiﬁcantly improved quality in voice conversion in terms of speech naturalness and speaker similarity of the converted speech for both cross-gender and intra-gender conversions.},
	language = {en},
	urldate = {2023-09-21},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Liu, Songxiang and Cao, Yuewen and Wu, Xixin and Sun, Lifa and Liu, Xunying and Meng, Helen},
	month = sep,
	year = {2019},
	pages = {714--718},
}

@inproceedings{zhenye_voice_2019,
	title = {Voice {Conversion} from {Tibetan} {Amdo} {Dialect} to {Tibetan} {U}-tsang {Dialect} {Based} on {Generative} {Adversarial} {Networks}},
	doi = {10.1109/ITAIC.2019.8785447},
	abstract = {This paper proposes a Voice Conversion (VC) method from Tibetan Amdo dialect to Tibetan U-tsang dialect based on Generative Adversarial Networks (GANs). An inevitable problem with the traditional VC framework is that the acoustic feature vector output from the conversion model is over-smoothing, which leads to a drop in the quality of the converted speech. This is because in the training phase of acoustic model, a specific probability model is used to model the distribution of data, so that the output of a relatively average parameter of the model is considered to be optimal. Acoustic parameter over-smoothing occurs as long as the analytical form of the model distribution is artificially designed. In order to overcome this problem, the VC framework proposed in this paper uses GANs as the modeling network of the acoustic model, directly uses a generator model to learn the distribution of data, and guides the generator through a discriminator model. The training of the model makes the sample distribution of the model close to the distribution of the target speaker data samples, thus alleviating the problem of over-smoothing of the converted speech spectrum. The experimental results show that the proposed method is superior to VC based on Deep Neural Networks (DNNs) in the sound quality and similarity of the converted speech.},
	booktitle = {2019 {IEEE} 8th {Joint} {International} {Information} {Technology} and {Artificial} {Intelligence} {Conference} ({ITAIC})},
	author = {Zhenye, Gan and Guangying, Zhao and Hongwu, Yang and Xiaotian, Xing and Yi, Jiao},
	month = may,
	year = {2019},
	keywords = {Acoustics, Analytical models, Data models, Deep Neural Networks, Feature extraction, Gallium nitride, Generative Adversarial Networks, Generators, Training, Voice Conversion, over-smoothing},
	pages = {325--329},
}

@inproceedings{huang_refined_2019,
	title = {Refined {WaveNet} {Vocoder} for {Variational} {Autoencoder} {Based} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1811.11078},
	doi = {10.23919/EUSIPCO.2019.8902651},
	abstract = {This paper presents a refinement framework of WaveNet vocoders for variational autoencoder (VAE) based voice conversion (VC), which reduces the quality distortion caused by the mismatch between the training data and testing data. Conventional WaveNet vocoders are trained with natural acoustic features but conditioned on the converted features in the conversion stage for VC, and such a mismatch often causes significant quality and similarity degradation. In this work, we take advantage of the particular structure of VAEs to refine WaveNet vocoders with the self-reconstructed features generated by VAE, which are of similar characteristics with the converted features while having the same temporal structure with the target natural features. We analyze these features and show that the self-reconstructed features are similar to the converted features. Objective and subjective experimental results demonstrate the effectiveness of our proposed framework.},
	urldate = {2023-09-21},
	booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Huang, Wen-Chin and Wu, Yi-Chiao and Hwang, Hsin-Te and Tobing, Patrick Lumban and Hayashi, Tomoki and Kobayashi, Kazuhiro and Toda, Tomoki and Tsao, Yu and Wang, Hsin-Min},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1--5},
}

@inproceedings{zhao_fast_2019,
	title = {Fast {Learning} for {Non}-{Parallel} {Many}-to-{Many} {Voice} {Conversion} with {Residual} {Star} {Generative} {Adversarial} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/zhao19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-2067},
	abstract = {This paper proposes a fast learning framework for non-parallel many-to-many voice conversion with residual Star Generative Adversarial Networks (StarGAN). In addition to the state-ofthe-art StarGAN-VC approach that learns an unreferenced mapping between a group of speakers’ acoustic features for nonparallel many-to-many voice conversion, our method, which we call Res-StarGAN-VC, presents an enhancement by incorporating a residual mapping. The idea is to leverage on the shared linguistic content between source and target features during conversion. The residual mapping is realized by using identity shortcut connections from the input to the output of the generator in Res-StarGAN-VC. Such shortcut connections accelerate the learning process of the network with no increase of parameters and computational complexity. They also help generate high-quality fake samples at the very beginning of the adversarial training. Experiments and subjective evaluations show that the proposed method offers (1) signiﬁcantly faster convergence in adversarial training and (2) clearer pronunciations and better speaker similarity of converted speech, compared to the StarGAN-VC baseline on both mono-lingual and cross-lingual many-to-many voice conversion tasks.},
	language = {en},
	urldate = {2023-09-21},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Zhao, Shengkui and Nguyen, Trung Hieu and Wang, Hao and Ma, Bin},
	month = sep,
	year = {2019},
	pages = {689--693},
}

@inproceedings{patel_novel_2019,
	title = {Novel {Adaptive} {Generative} {Adversarial} {Network} for {Voice} {Conversion}},
	doi = {10.1109/APSIPAASC47483.2019.9023141},
	abstract = {Voice Conversion (VC) converts the speaking style of a source speaker to the speaking style of a target speaker by preserving the linguistic content of a given speech utterance. Recently, Cycle Consistent Adversarial Network (CycleGAN), and its variants have become popular for non-parallel VC tasks. However, CycleGAN uses two different generators and discriminators. In this paper, we introduce a novel Adaptive Generative Adversarial Network (AdaGAN) for non-parallel VC task, which effectively requires single generator, and two discriminators for transferring the style from one speaker to another while preserving the linguistic content in the converted voices. To the best of authors' knowledge, this is the first study of its kind to introduce a new Generative Adversarial Network (GAN)-based architecture (i.e., AdaGAN) in machine learning literature, and the first attempt to apply this architecture for nonparallel VC task. In this paper, we compared the results of the AdaGAN w.r.t. state-of-the-art CycleGAN architecture. Detailed subjective and objective tests are carried out on the publicly available VC Challenge 2018 corpus. In addition, we perform three statistical analysis which show effectiveness of AdaGAN over CycleGAN for parallel-data free one-to-one VC. For inter-gender and intra-gender VC, We observe that the AdaGAN yield objective results that are comparable to the CycleGAN, and are superior in terms of subjective evaluation. A subjective evaluation shows that AdaGAN outperforms CycleGAN-VC in terms of naturalness, sound quality, and speaker similarity. AdaGAN was preferred 58.33\% and 41\% time more over CycleGAN in terms of speaker similarity and sound quality, respectively.},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Patel, Maitreya and Parmar, Mihir and Doshi, Savan and Shah, Nirmesh J. and Patil, Hemant A.},
	month = nov,
	year = {2019},
	keywords = {AdaGAN, AdaIN, CycleGAN, Decoding, Feature extraction, GAN, Generative adversarial networks, Generators, Linguistics, Task analysis, Training, Voice Conversion},
	pages = {1273--1281},
}

@inproceedings{liu_unsupervised_2019,
	title = {Unsupervised {End}-to-{End} {Learning} of {Discrete} {Linguistic} {Units} for {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1905.11563},
	doi = {10.21437/Interspeech.2019-2048},
	abstract = {We present an unsupervised end-to-end training scheme where we discover discrete subword units from speech without using any labels. The discrete subword units are learned under an ASR-TTS autoencoder reconstruction setting, where an ASR-Encoder is trained to discover a set of common linguistic units given a variety of speakers, and a TTS-Decoder trained to project the discovered units back to the designated speech. We propose a discrete encoding method, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder differentiable. We found that the proposed encoding method offers automatic extraction of speech content from speaker style, and is sufficient to cover full linguistic content in a given language. Therefore, the TTS-Decoder can synthesize speech with the same content as the input of ASR-Encoder but with different speaker characteristics, which achieves voice conversion (VC). We further improve the quality of VC using adversarial training, where we train a TTS-Patcher that augments the output of TTS-Decoder. Objective and subjective evaluations show that the proposed approach offers strong VC results as it eliminates speaker identity while preserving content within speech. In the ZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low bitrate.},
	urldate = {2023-09-21},
	booktitle = {Interspeech 2019},
	author = {Liu, Andy T. and Hsu, Po-chun and Lee, Hung-yi},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1108--1112},
}

@article{wu_non-parallel_2020,
	title = {Non-parallel {Voice} {Conversion} {System} with {WaveNet} {Vocoder} and {Collapsed} {Speech} {Suppression}},
	volume = {8},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/2003.11750},
	doi = {10.1109/ACCESS.2020.2984007},
	abstract = {In this paper, we integrate a simple non-parallel voice conversion (VC) system with a WaveNet (WN) vocoder and a proposed collapsed speech suppression technique. The effectiveness of WN as a vocoder for generating high-fidelity speech waveforms on the basis of acoustic features has been confirmed in recent works. However, when combining the WN vocoder with a VC system, the distorted acoustic features, acoustic and temporal mismatches, and exposure bias usually lead to significant speech quality degradation, making WN generate some very noisy speech segments called collapsed speech. To tackle the problem, we take conventional-vocoder-generated speech as the reference speech to derive a linear predictive coding distribution constraint (LPCDC) to avoid the collapsed speech problem. Furthermore, to mitigate the negative effects introduced by the LPCDC, we propose a collapsed speech segment detector (CSSD) to ensure that the LPCDC is only applied to the problematic segments to limit the loss of quality to short periods. Objective and subjective evaluations are conducted, and the experimental results confirm the effectiveness of the proposed method, which further improves the speech quality of our previous non-parallel VC system submitted to Voice Conversion Challenge 2018.},
	urldate = {2023-09-21},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Wu, Yi-Chiao and Tobing, Patrick Lumban and Kobayashi, Kazuhiro and Hayashi, Tomoki and Toda, Tomoki},
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {62094--62106},
}

@article{zhang_non-parallel_2020,
	title = {Non-{Parallel} {Sequence}-to-{Sequence} {Voice} {Conversion} with {Disentangled} {Linguistic} and {Speaker} {Representations}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1906.10508},
	doi = {10.1109/TASLP.2019.2960721},
	abstract = {This paper presents a method of sequence-to-sequence (seq2seq) voice conversion using non-parallel training data. In this method, disentangled linguistic and speaker representations are extracted from acoustic features, and voice conversion is achieved by preserving the linguistic representations of source utterances while replacing the speaker representations with the target ones. Our model is built under the framework of encoder-decoder neural networks. A recognition encoder is designed to learn the disentangled linguistic representations with two strategies. First, phoneme transcriptions of training data are introduced to provide the references for leaning linguistic representations of audio signals. Second, an adversarial training strategy is employed to further wipe out speaker information from the linguistic representations. Meanwhile, speaker representations are extracted from audio signals by a speaker encoder. The model parameters are estimated by two-stage training, including a pretraining stage using a multi-speaker dataset and a fine-tuning stage using the dataset of a specific conversion pair. Since both the recognition encoder and the decoder for recovering acoustic features are seq2seq neural networks, there are no constrains of frame alignment and frame-by-frame conversion in our proposed method. Experimental results showed that our method obtained higher similarity and naturalness than the best non-parallel voice conversion method in Voice Conversion Challenge 2018. Besides, the performance of our proposed method was closed to the state-of-the-art parallel seq2seq voice conversion method.},
	urldate = {2023-09-21},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Jing-Xuan and Ling, Zhen-Hua and Dai, Li-Rong},
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {540--552},
}

@article{zhou_multi-task_2020,
	title = {Multi-{Task} {WaveRNN} {With} an {Integrated} {Architecture} for {Cross}-{Lingual} {Voice} {Conversion}},
	volume = {27},
	issn = {1558-2361},
	doi = {10.1109/LSP.2020.3010163},
	abstract = {Spoken languages are similar phonetically because humans have a common vocal production system. However, each language has a unique phonetic repertoire and phonotactic rule. In cross-lingual voice conversion, source speaker and target speaker speak different languages. The challenge is how to project the speaker identity of the source speaker to that of the target across two different phonetic systems. A typical voice conversion system employs a generator-vocoder pipeline, where the generator is responsible for conversion, and the vocoder is for waveform reconstruction. We propose a novel Multi-Task WaveRNN with an integrated architecture for cross-lingual voice conversion. The WaveRNN is trained on two sets of monolingual data via a two-task learning. The integrated architecture takes linguistic features as input and outputs speech waveform directly. Voice conversion experiments are conducted between English and Mandarin, which confirm the effectiveness of the proposed method in terms of speech quality and speaker similarity.},
	journal = {IEEE Signal Processing Letters},
	author = {Zhou, Yi and Tian, Xiaohai and Li, Haizhou},
	year = {2020},
	keywords = {Acoustics, Cross-lingual voice conversion (xVC), Generators, Linguistics, Pipelines, Task analysis, Training, Vocoders, WaveRNN, integrated architecture, multi-task learning (MTL)},
	pages = {1310--1314},
}

@inproceedings{liu_wavenet_2018,
	title = {{WaveNet} {Vocoder} with {Limited} {Training} {Data} for {Voice} {Conversion}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/liu18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1190},
	abstract = {This paper investigates the approaches of building WaveNet vocoders with limited training data for voice conversion (VC). Current VC systems using statistical acoustic models always suffer from the quality degradation of converted speech. One of the major causes is the use of hand-crafted vocoders for waveform generation. Recently, with the emergence of WaveNet for waveform modeling, speaker-dependent WaveNet vocoders have been proposed and they can reconstruct speech with better quality than conventional vocoders, such as STRAIGHT. Because training a WaveNet vocoder in the speaker-dependent way requires a relatively large training dataset, it remains a challenge to build a high-quality WaveNet vocoder for VC tasks when the training data of target speakers is limited. In this paper, we propose to build WaveNet vocoders by combining the initialization using a multi-speaker corpus and the adaptation using a small amount of target data, and evaluate this proposed method on the Voice Conversion Challenge (VCC) 2018 dataset which contains approximately 5 minute recordings for each target speaker. Experimental results show that the WaveNet vocoders built using our proposed method outperform conventional STRAIGHT vocoder. Furthermore, our system achieves an average naturalness MOS of 4.13 in VCC 2018, which is the highest among all submitted systems.},
	language = {en},
	urldate = {2023-09-21},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Liu, Li-Juan and Ling, Zhen-Hua and Jiang, Yuan and Zhou, Ming and Dai, Li-Rong},
	month = sep,
	year = {2018},
	pages = {1983--1987},
}

@article{choi_sequence--sequence_2021,
	title = {Sequence-to-{Sequence} {Emotional} {Voice} {Conversion} {With} {Strength} {Control}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3065460},
	abstract = {This paper proposes an improved emotional voice conversion (EVC) method with emotional strength and duration controllability. EVC methods without duration mapping generate emotional speech with identical duration to that of the neutral input speech. In reality, even the same sentences would have different speeds and rhythms depending on the emotions. To solve this, the proposed method adopts a sequence-to-sequence network with an attention module that enables the network to learn attention in the neutral input sequence should be focused on which part of the emotional output sequence. Besides, to capture the multi-attribute aspects of emotional variations, an emotion encoder is designed for transforming acoustic features into emotion embedding vectors. By aggregating the emotion embedding vectors for each emotion, a representative vector for the target emotion is obtained and weighted to reflect emotion strength. By introducing a speaker encoder, the proposed method can preserve speaker identity even after the emotion conversion. Objective and subjective evaluation results confirm that the proposed method is superior to other previous works. Especially, in emotion strength control, we achieve in getting successful results.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Choi, Heejin and Hahn, Minsoo},
	year = {2021},
	keywords = {Acoustics, Decoding, Feature extraction, Indexes, Linguistics, Training, Vocoders, Voice conversion, controllable emotional voice conversion, emotion strength, emotional voice conversion, sequence-to-sequence learning},
	pages = {42674--42687},
}

@inproceedings{tobing_cyclic_2020,
	title = {Cyclic {Spectral} {Modeling} for {Unsupervised} {Unit} {Discovery} into {Voice} {Conversion} with {Excitation} and {Waveform} {Modeling}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/tobing20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2559},
	abstract = {We present a novel approach of cyclic spectral modeling for unsupervised discovery of speech units into voice conversion with excitation network and waveform modeling. Speciﬁcally, we propose two spectral modeling techniques: 1) cyclic vectorquantized autoencoder (CycleVQVAE), and 2) cyclic variational autoencoder (CycleVAE). In CycleVQVAE, a discrete latent space is used for the speech units, whereas, in CycleVAE, a continuous latent space is used. The cyclic structure is developed using the reconstruction ﬂow and the cyclic reconstruction ﬂow of spectral features, where the latter is obtained by recycling the converted spectral features. This method is used to obtain a possible speaker-independent latent space because of marginalization on all possible speaker conversion pairs during training. On the other hand, speaker-dependent space is conditioned with a one-hot speaker-code. Excitation modeling is developed in a separate manner for CycleVQVAE, while it is in a joint manner for CycleVAE. To generate speech waveform, WaveNet-based waveform modeling is used. The proposed framework is entried for the ZeroSpeech Challenge 2020, and is capable of reaching a character error rate of 0.21, a speaker similarity score of 3.91, a mean opinion score of 3.84 for the naturalness of the converted speech in the 2019 voice conversion task.},
	language = {en},
	urldate = {2023-09-21},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Tobing, Patrick Lumban and Hayashi, Tomoki and Wu, Yi-Chiao and Kobayashi, Kazuhiro and Toda, Tomoki},
	month = oct,
	year = {2020},
	pages = {4861--4865},
}

@article{huang_unsupervised_2020,
	title = {Unsupervised {Representation} {Disentanglement} using {Cross} {Domain} {Features} and {Adversarial} {Learning} in {Variational} {Autoencoder} based {Voice} {Conversion}},
	volume = {4},
	issn = {2471-285X},
	url = {http://arxiv.org/abs/2001.07849},
	doi = {10.1109/TETCI.2020.2977678},
	abstract = {An effective approach for voice conversion (VC) is to disentangle linguistic content from other components in the speech signal. The effectiveness of variational autoencoder (VAE) based VC (VAE-VC), for instance, strongly relies on this principle. In our prior work, we proposed a cross-domain VAE-VC (CDVAE-VC) framework, which utilized acoustic features of different properties, to improve the performance of VAE-VC. We believed that the success came from more disentangled latent representations. In this paper, we extend the CDVAE-VC framework by incorporating the concept of adversarial learning, in order to further increase the degree of disentanglement, thereby improving the quality and similarity of converted speech. More specifically, we first investigate the effectiveness of incorporating the generative adversarial networks (GANs) with CDVAE-VC. Then, we consider the concept of domain adversarial training and add an explicit constraint to the latent representation, realized by a speaker classifier, to explicitly eliminate the speaker information that resides in the latent code. Experimental results confirm that the degree of disentanglement of the learned latent representation can be enhanced by both GANs and the speaker classifier. Meanwhile, subjective evaluation results in terms of quality and similarity scores demonstrate the effectiveness of our proposed methods.},
	number = {4},
	urldate = {2023-09-21},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Huang, Wen-Chin and Luo, Hao and Hwang, Hsin-Te and Lo, Chen-Chou and Peng, Yu-Huai and Tsao, Yu and Wang, Hsin-Min},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {468--479},
}

@article{reddy_dnn-based_2020,
	title = {{DNN}-based cross-lingual voice conversion using {Bottleneck} {Features}},
	volume = {51},
	issn = {1370-4621, 1573-773X},
	url = {http://arxiv.org/abs/1909.03974},
	doi = {10.1007/s11063-019-10149-y},
	abstract = {Cross-lingual voice conversion (CLVC) is a quite challenging task since the source and target speakers speak different languages. This paper proposes a CLVC framework based on bottleneck features and deep neural network (DNN). In the proposed method, the bottleneck features extracted from a deep auto-encoder (DAE) are used to represent speaker-independent features of speech signals from different languages. A DNN model is trained to learn the mapping between bottleneck features and the corresponding spectral features of the target speaker. The proposed method can capture speaker-specific characteristics of a target speaker, and hence requires no speech data from source speaker during training. The performance of the proposed method is evaluated using data from three Indian languages: Telugu, Tamil and Malayalam. The experimental results show that the proposed method outperforms the baseline Gaussian mixture model (GMM)-based CLVC approach.},
	number = {2},
	urldate = {2023-09-21},
	journal = {Neural Processing Letters},
	author = {Reddy, M. Kiran and Rao, K. Sreenivasa},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2029--2042},
}

@misc{shankar_multi-speaker_2020,
	title = {Multi-speaker {Emotion} {Conversion} via {Latent} {Variable} {Regularization} and a {Chained} {Encoder}-{Decoder}-{Predictor} {Network}},
	url = {http://arxiv.org/abs/2007.12937},
	abstract = {We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Shankar, Ravi and Hsieh, Hsi-Wei and Charon, Nicolas and Venkataraman, Archana},
	month = aug,
	year = {2020},
	doi = {10.48550/arXiv.2007.12937},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zhang_gazev_2020,
	title = {{GAZEV}: {GAN}-{Based} {Zero}-{Shot} {Voice} {Conversion} over {Non}-parallel {Speech} {Corpus}},
	shorttitle = {{GAZEV}},
	url = {http://arxiv.org/abs/2010.12788},
	abstract = {Non-parallel many-to-many voice conversion is recently attract-ing huge research efforts in the speech processing community. A voice conversion system transforms an utterance of a source speaker to another utterance of a target speaker by keeping the content in the original utterance and replacing by the vocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2, present promising results, only when speech corpus of the engaged speakers is available during model training. AUTOVCis able to perform voice conversion on unseen speakers, but it needs an external pretrained speaker verification model. In this paper, we present our new GAN-based zero-shot voice conversion solution, called GAZEV, which targets to support unseen speakers on both source and target utterances. Our key technical contribution is the adoption of speaker embedding loss on top of the GAN framework, as well as adaptive instance normalization strategy, in order to address the limitations of speaker identity transfer in existing solutions. Our empirical evaluations demonstrate significant performance improvement on output speech quality and comparable speaker similarity to AUTOVC.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Zhang, Zining and He, Bingsheng and Zhang, Zhenjie},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.12788},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wu_one-shot_2020,
	title = {One-{Shot} {Voice} {Conversion} by {Vector} {Quantization}},
	doi = {10.1109/ICASSP40776.2020.9053854},
	abstract = {In this paper, we propose a vector quantization (VQ) based one-shot voice conversion (VC) approach without any supervision on speaker label. We model the content embedding as a series of discrete codes and take the difference between quantize-before and quantize-after vector as the speaker embedding. We show that this approach has a strong ability to disentangle the content and speaker information with reconstruction loss only, and one-shot VC is thus achieved.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wu, Da-Yi and Lee, Hung-yi},
	month = may,
	year = {2020},
	keywords = {Acoustics, Conferences, Speech processing, Vector quantization, Visualization, disentangled representations, vector quantization, voice conversion},
	pages = {7734--7738},
}

@incollection{baas_stargan-zsvc_2020,
	title = {{StarGAN}-{ZSVC}: {Towards} {Zero}-{Shot} {Voice} {Conversion} in {Low}-{Resource} {Contexts}},
	volume = {1342},
	shorttitle = {{StarGAN}-{ZSVC}},
	url = {http://arxiv.org/abs/2106.00043},
	abstract = {Voice conversion is the task of converting a spoken utterance from a source speaker so that it appears to be said by a different target speaker while retaining the linguistic content of the utterance. Recent advances have led to major improvements in the quality of voice conversion systems. However, to be useful in a wider range of contexts, voice conversion systems would need to be (i) trainable without access to parallel data, (ii) work in a zero-shot setting where both the source and target speakers are unseen during training, and (iii) run in real time or faster. Recent techniques fulfil one or two of these requirements, but not all three. This paper extends recent voice conversion models based on generative adversarial networks (GANs), to satisfy all three of these conditions. We specifically extend the recent StarGAN-VC model by conditioning it on a speaker embedding (from a potentially unseen speaker). This allows the model to be used in a zero-shot setting, and we therefore call it StarGAN-ZSVC. We compare StarGAN-ZSVC against other voice conversion techniques in a low-resource setting using a small 9-minute training set. Compared to AutoVC – another recent neural zero-shot approach – we observe that StarGAN-ZSVC gives small improvements in the zero-shot setting, showing that real-time zero-shot voice conversion is possible even for a model trained on very little data. Further work is required to see whether scaling up StarGAN-ZSVC will also improve zero-shot voice conversion quality in high-resource contexts.},
	urldate = {2023-09-21},
	author = {Baas, Matthew and Kamper, Herman},
	year = {2020},
	doi = {10.1007/978-3-030-66151-9_5},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {69--84},
}

@misc{shankar_non-parallel_2020,
	title = {Non-parallel {Emotion} {Conversion} using a {Deep}-{Generative} {Hybrid} {Network} and an {Adversarial} {Pair} {Discriminator}},
	url = {http://arxiv.org/abs/2007.12932},
	abstract = {We introduce a novel method for emotion conversion in speech that does not require parallel training data. Our approach loosely relies on a cycle-GAN schema to minimize the reconstruction error from converting back and forth between emotion pairs. However, unlike the conventional cycle-GAN, our discriminator classifies whether a pair of input real and generated samples corresponds to the desired emotion conversion (e.g., A to B) or to its inverse (B to A). We will show that this setup, which we refer to as a variational cycle-GAN (VC-GAN), is equivalent to minimizing the empirical KL divergence between the source features and their cyclic counterpart. In addition, our generator combines a trainable deep network with a fixed generative block to implement a smooth and invertible transformation on the input features, in our case, the fundamental frequency (F0) contour. This hybrid architecture regularizes our adversarial training procedure. We use crowd sourcing to evaluate both the emotional saliency and the quality of synthesized speech. Finally, we show that our model generalizes to new speakers by modifying speech produced by Wavenet.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Shankar, Ravi and Sager, Jacob and Venkataraman, Archana},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{nakatani_cross-lingual_2020,
	title = {Cross-{Lingual} {Voice} {Conversion} using a {Cyclic} {Variational} {Auto}-encoder and a {WaveNet} {Vocoder}},
	abstract = {We propose a novel, cross-lingual voice conversion (VC) method using a cyclic variational auto-encoder (CycleVAE). Voice conversion is the transformation of the voice of one speaker into the voice of another speaker, while cross-lingual VC performs voice conversion between speakers who speak different languages. When using VC methods based on parallel learning, it is necessary to prepare accented speech uttered by the source or target speaker, using the pronunciation system of the speaker's mother tongue. On the other hand, VC methods which use a non-parallel learning approach can utilize the natural speech data of both the source and target speakers, produced in their own native languages. It then becomes necessary, however, to deal with the issues of time-alignment and language mismatches. To address these issues, we apply CycleVAE to cross-lingual VC as a sophisticated, non-parallel method of VC. We also apply the WaveNet vocoder in the waveform generation process of CycleVAE-VC to improve overall conversion quality. Our objective and subjective experimental results when performing cross-lingual VC from a native English speaker to a native Japanese speaker confirm that the proposed method achieves a higher level of naturalness and speaker similarity than a conventional RNN-based parallel VC method using accented speech.},
	booktitle = {2020 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Nakatani, Hikaru and Tobing, Patrick Lumban and Takeda, Kazuya and Toda, Tomoki},
	month = dec,
	year = {2020},
	keywords = {Acoustics, Decoding, Feature extraction, Speech processing, Task analysis, Training, Vocoders},
	pages = {520--526},
}

@inproceedings{elgaar_multi-speaker_2020,
	title = {Multi-{Speaker} and {Multi}-{Domain} {Emotional} {Voice} {Conversion} {Using} {Factorized} {Hierarchical} {Variational} {Autoencoder}},
	doi = {10.1109/ICASSP40776.2020.9054534},
	abstract = {Due to the complexity of emotional features, there has been limited success in emotional voice conversion. One major challenge is that conversion between more than two kinds of emotions often accompanies distortion of voice signal.The factorized hierarchical variational autoencoder (FHVAE) [1] was previously shown to have an ability, called sequence-level regularization, to generate disentangled representations of both sequence-level (such as speaker identity) and segment-level features. This study exploits the FHVAE pipeline to produce disentangled representations of emotion, making it possible to greatly facilitate emotional voice conversion.We propose three versions of algorithms for improving the quality of disentangled representation and audio synthesis. We conducted three mean opinion score (MOS) surveys to assess the performance of our models in terms of 1) speaker’s voice preservation, 2) emotion conversion, and 3) audio naturalness.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Elgaar, Mohamed and Park, Jungbae and Lee, Sang Wan},
	month = may,
	year = {2020},
	keywords = {Acoustic distortion, Acoustics, Complexity theory, Conferences, Disentangled Representation, Emotional Voice Conversion, Pipelines, Signal processing algorithms, Speech processing, Style Transfer, Variational Autoencoder},
	pages = {7769--7773},
}

@misc{chen_unsupervised_2020,
	title = {Unsupervised {Acoustic} {Unit} {Representation} {Learning} for {Voice} {Conversion} using {WaveNet} {Auto}-encoders},
	url = {http://arxiv.org/abs/2008.06892},
	abstract = {Unsupervised representation learning of speech has been of keen interest in recent years, which is for example evident in the wide interest of the ZeroSpeech challenges. This work presents a new method for learning frame level representations based on WaveNet auto-encoders. Of particular interest in the ZeroSpeech Challenge 2019 were models with discrete latent variable such as the Vector Quantized Variational Auto-Encoder (VQVAE). However these models generate speech with relatively poor quality. In this work we aim to address this with two approaches: first WaveNet is used as the decoder and to generate waveform data directly from the latent representation; second, the low complexity of latent representations is improved with two alternative disentanglement learning methods, namely instance normalization and sliced vector quantization. The method was developed and tested in the context of the recent ZeroSpeech challenge 2020. The system output submitted to the challenge obtained the top position for naturalness (Mean Opinion Score 4.06), top position for intelligibility (Character Error Rate 0.15), and third position for the quality of the representation (ABX test score 12.5). These and further analysis in this paper illustrates that quality of the converted speech and the acoustic units representation can be well balanced.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Chen, Mingjie and Hain, Thomas},
	month = aug,
	year = {2020},
	doi = {10.48550/arXiv.2008.06892},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{liu_transferring_2020,
	title = {Transferring {Source} {Style} in {Non}-{Parallel} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2005.09178},
	abstract = {Voice conversion (VC) techniques aim to modify speaker identity of an utterance while preserving the underlying linguistic information. Most VC approaches ignore modeling of the speaking style (e.g. emotion and emphasis), which may contain the factors intentionally added by the speaker and should be retained during conversion. This study proposes a sequence-to-sequence based non-parallel VC approach, which has the capability of transferring the speaking style from the source speech to the converted speech by explicitly modeling. Objective evaluation and subjective listening tests show superiority of the proposed VC approach in terms of speech naturalness and speaker similarity of the converted speech. Experiments are also conducted to show the source-style transferability of the proposed approach.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Liu, Songxiang and Cao, Yuewen and Kang, Shiyin and Hu, Na and Liu, Xunying and Su, Dan and Yu, Dong and Meng, Helen},
	month = may,
	year = {2020},
	doi = {10.48550/arXiv.2005.09178},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{zang_foreign_2022,
	title = {Foreign {Accent} {Conversion} using {Concentrated} {Attention}},
	doi = {10.1109/ICKG55886.2022.00056},
	abstract = {Foreign accent conversion is an important and challenging problem due to significant differences in the manner of articulation and the speech prosody of different regional speakers. In this paper, we propose a new method for the problem of foreign accent conversion that uses Phonetic Posteriorgrams (PPGs) and Log-scale Fundamental frequency (Log-F0) to address the mismatches of phonetic and prosody. Furthermore, we propose using concentrated attention to improve the alignment of input sequences and mel-spectrograms. The concentrated attention selects the top k highest score values in the attention matrix row by row. In this way, the attention weight of the content related to the current sequence will be the largest. Our approach first trains a PPG extractor using LibriSpeech Corpus, which uses an end-to-end hybrid CTC-attention model. Then, the modified Tacotron2 based on concentrated attention is trained to model the relationships between PPGs and mel-spectrograms. In our proposed framework, the input of Tacotron2 is the concatenation of PPG embedding and normalized Log-scale fundamental frequency (Log-F0). In the convert stage, WaveGlow is modeled to generate speech, which is a streaming structure. To better verify the effectiveness of our proposed method, we also add some objective evaluation methods. These include Mel spectral distance, Object\_MOS score, speaker similarity, and similarity in the embedding space of the entire speech. Experiments shows that our proposed concentrated attention method delivers comparable or better results than the previous foreign accent conversion method in terms of voice naturalness, speaker similarity to the source speaker, and accent similarity to the target speaker.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Knowledge} {Graph} ({ICKG})},
	author = {Zang, Xuexue and Xie, Fei and Weng, Fuliang},
	month = nov,
	year = {2022},
	keywords = {Frequency conversion, Phonetics, Task analysis, Vocoders, concentrated attention, foreign accent conversion, phonetic posteriorgrams, speech synthesis},
	pages = {386--391},
}

@inproceedings{cao_nonparallel_2020,
	title = {Nonparallel {Emotional} {Speech} {Conversion} {Using} {VAE}-{GAN}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cao20b_interspeech.html},
	doi = {10.21437/Interspeech.2020-1647},
	abstract = {This paper proposes a nonparallel emotional speech conversion (ESC) method based on Variational AutoEncoder-Generative Adversarial Network (VAE-GAN). Emotional speech conversion aims at transforming speech from one source emotion to that of a target emotion without changing the speaker’s identity and linguistic content. In this work, an encoder is trained to elicit the content-related representations from acoustic features. Emotion-related representations are extracted in a supervised manner. Then the transformation between emotionrelated representations from different domains is learned using an improved cycle-consistent Generative Adversarial Network (CycleGAN). Finally, emotion conversion is performed by eliciting and recombining the content-related representations of the source speech and the emotion-related representations of the target emotion. Subjective evaluation experiments are conducted and the results show that the proposed method outperforms the baseline in terms of voice quality and emotion conversion ability.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cao, Yuexin and Liu, Zhengchen and Chen, Minchuan and Ma, Jun and Wang, Shaojun and Xiao, Jing},
	month = oct,
	year = {2020},
	pages = {3406--3410},
}

@inproceedings{ding_improving_2020,
	title = {Improving the {Speaker} {Identity} of {Non}-{Parallel} {Many}-to-{Many} {Voice} {Conversion} with {Adversarial} {Speaker} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/ding20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1033},
	abstract = {Phonetic Posteriorgrams (PPGs) have received much attention for non-parallel many-to-many Voice Conversion (VC), and have been shown to achieve state-of-the-art performance. These methods implicitly assume that PPGs are speaker-independent and contain only linguistic information in an utterance. In practice, however, PPGs carry speaker individuality cues, such as accent, intonation, and speaking rate. As a result, these cues can leak into the voice conversion, making it sound similar to the source speaker. To address this issue, we propose an adversarial learning approach that can remove speaker-dependent information in VC models based on a PPG2speech synthesizer. During training, the encoder output of a PPG2speech synthesizer is fed to a classiﬁer trained to identify the corresponding speaker, while the encoder is trained to fool the classiﬁer. As a result, a more speaker-independent representation is learned. The proposed method is advantageous as it does not require pretraining the speaker classiﬁer, and the adversarial speaker classiﬁer is jointly trained with the PPG2speech synthesizer endto-end. We conduct objective and subjective experiments on the CSTR VCTK Corpus under standard and one-shot VC conditions. Results show that the proposed method signiﬁcantly improves the speaker identity of VC syntheses when compared with a baseline system trained without adversarial learning.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Ding, Shaojin and Zhao, Guanlong and Gutierrez-Osuna, Ricardo},
	month = oct,
	year = {2020},
	pages = {776--780},
}

@article{ding_accentron_2022,
	title = {Accentron: {Foreign} accent conversion to arbitrary non-native speakers using zero-shot learning},
	volume = {72},
	issn = {0885-2308},
	shorttitle = {Accentron},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230821001029},
	doi = {10.1016/j.csl.2021.101302},
	abstract = {Foreign accent conversion (FAC) aims to create a new voice that has the voice identity of a given second-language (L2) speaker but with a native (L1) accent. Previous FAC approaches usually require training a separate model for each L2 speaker and, more importantly, generally require considerable speech data from each L2 speaker for training. To address these limitations, we propose Accentron, an approach that can generate accent-converted speech for arbitrary L2 speakers unseen during training. In the proposed approach, we first train a speaker-independent acoustic model on L1 corpora to extract bottleneck features that represent the linguistic content of utterances. Then, we develop a speaker encoder and an accent encoder to generate embedding vectors for the desired voice identity (L2 speaker’s) and accent (L1 accent), respectively. Lastly, we use a sequence-to-sequence model to transform bottleneck-features to Mel-spectrograms, conditioned on the L2 speaker embedding and the L1 accent embedding. We conducted experiments on the L2-ARCTIC corpus under two testing conditions: the standard FAC setting where test L2 speakers were seen during training, and a zero-shot FAC setting where test L2 speakers were unseen during training. Accentron achieves over 27\% relative improvement in accentedness ratings compared to two state-of-the-art FAC systems in the standard FAC setting. More importantly, our results show that Accentron generalizes to the zero-shot FAC setting with no performance loss. Therefore, in practical use scenarios (e.g., computer-assisted pronunciation training software), Accentron can effectively avoid the need to adapt or retrain the model, which significantly reduces computations and the users’ waiting time.},
	urldate = {2023-09-19},
	journal = {Computer Speech \& Language},
	author = {Ding, Shaojin and Zhao, Guanlong and Gutierrez-Osuna, Ricardo},
	month = mar,
	year = {2022},
	keywords = {Computer-assisted pronunciation training, Foreign accent conversion, Speech synthesis},
	pages = {101302},
}

@article{al-radhi_effects_2021,
	title = {Effects of {Sinusoidal} {Model} on {Non}-{Parallel} {Voice} {Conversion} with {Adversarial} {Learning}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/16/7489},
	doi = {10.3390/app11167489},
	abstract = {Voice conversion (VC) transforms the speaking style of a source speaker to the speaking style of a target speaker by keeping linguistic information unchanged. Traditional VC techniques rely on parallel recordings of multiple speakers uttering the same sentences. Earlier approaches mainly ﬁnd a mapping between the given source–target speakers, which contain pairs of similar utterances spoken by different speakers. However, parallel data are computationally expensive and difﬁcult to collect. Non-parallel VC remains an interesting but challenging speech processing task. To address this limitation, we propose a method that allows a non-parallel many-to-many voice conversion by using a generative adversarial network. To the best of the authors’ knowledge, our study is the ﬁrst one that employs a sinusoidal model with continuous parameters to generate converted speech signals. Our method involves only several minutes of training examples without parallel utterances or time alignment procedures, where the source–target speakers are entirely unseen by the training dataset. Moreover, empirical study is carried out on the publicly available CSTR VCTK corpus. Our conclusions indicate that the proposed method reached the state-of-the-art results in speaker similarity to the utterance produced by the target speaker, while suggesting important structural ones to be further analyzed by experts.},
	language = {en},
	number = {16},
	urldate = {2023-09-19},
	journal = {Applied Sciences},
	author = {Al-Radhi, Mohammed Salah and Csapó, Tamás Gábor and Németh, Géza},
	month = aug,
	year = {2021},
	pages = {7489},
}

@inproceedings{li_non-parallel_2020,
	title = {Non-{Parallel} {Many}-to-{Many} {Voice} {Conversion} with {PSR}-{StarGAN}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/li20i_interspeech.html},
	doi = {10.21437/Interspeech.2020-1310},
	abstract = {Voice Conversion (VC) aims at modifying source speaker’s speech to sound like that of target speaker while preserving linguistic information of given speech. StarGAN-VC was recently proposed, which utilizes a variant of Generative Adversarial Networks (GAN) to perform non-parallel many-to-many VC. However, the quality of generated speech is not satisfactory enough. An improved method named “PSR-StarGAN-VC” is proposed in this paper by incorporating three improvements. Firstly, perceptual loss functions are introduced to optimize the generator in StarGAN-VC aiming to learn high-level spectral features. Secondly, considering that Switchable Normalization (SN) could learn different operations in different normalization layers of model, it is introduced to replace Batch Normalization (BN) in StarGAN-VC. Lastly, Residual Network (ResNet) is applied to establish the mapping of different layers between the encoder and decoder of generator aiming to retain more semantic features when converting speech, and to reduce the difﬁculty of training. Experiment results on the VCC 2018 datasets demonstrate superiority of the proposed method in terms of naturalness and speaker similarity.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Li, Yanping and Xu, Dongxiang and Zhang, Yan and Wang, Yang and Chen, Binbin},
	month = oct,
	year = {2020},
	pages = {781--785},
}

@misc{lee_many--many_2020,
	title = {Many-to-{Many} {Voice} {Conversion} using {Conditional} {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2002.06328},
	abstract = {Voice conversion (VC) refers to transforming the speaker characteristics of an utterance without altering its linguistic contents. Many works on voice conversion require to have parallel training data that is highly expensive to acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which does not require parallel training data, has been applied to voice conversion, showing the state-of-the-art performance. The CycleGAN based voice conversion, however, can be used only for a pair of speakers, i.e., one-to-one voice conversion between two speakers. In this paper, we extend the CycleGAN by conditioning the network on speakers. As a result, the proposed method can perform many-to-many voice conversion among multiple speakers using a single generative adversarial network (GAN). Compared to building multiple CycleGANs for each pair of speakers, the proposed method reduces the computational and spatial cost significantly without compromising the sound quality of the converted voice. Experimental results using the VCC2018 corpus confirm the efficiency of the proposed method.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Lee, Shindong and Ko, BongGu and Lee, Keonnyeong and Yoo, In-Chul and Yook, Dongsuk},
	month = feb,
	year = {2020},
	doi = {10.48550/arXiv.2002.06328},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{ho_cross-lingual_2021,
	title = {Cross-{Lingual} {Voice} {Conversion} {With} {Controllable} {Speaker} {Individuality} {Using} {Variational} {Autoencoder} and {Star} {Generative} {Adversarial} {Network}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3063519},
	abstract = {This paper proposes a non-parallel cross-lingual voice conversion (CLVC) model that can mimic voice while continuously controlling speaker individuality on the basis of the variational autoencoder (VAE) and star generative adversarial network (StarGAN). Most studies on CLVC only focused on mimicking a particular speaker voice without being able to arbitrarily modify the speaker individuality. In practice, the ability to generate speaker individuality may be more useful than just mimicking voice. Therefore, the proposed model reliably extracts the speaker embedding from different languages using a VAE. An F0 injection method is also introduced into our model to enhance the F0 modeling in the cross-lingual setting. To avoid the over-smoothing degradation problem of the conventional VAE, the adversarial training scheme of the StarGAN is adopted to improve the training-objective function of the VAE in a CLVC task. Objective and subjective measurements confirm the effectiveness of the proposed model and F0 injection method. Furthermore, speaker-similarity measurement on fictitious voices reveal a strong linear relationship between speaker individuality and interpolated speaker embedding, which indicates that speaker individuality can be controlled with our proposed model.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Ho, Tuan Vu and Akagi, Masato},
	year = {2021},
	keywords = {Acoustics, Decoding, Gallium nitride, Generative adversarial networks, Linguistics, Task analysis, Training, Voice conversion, controllable speaker individuality, cross-lingual, generative adversarial network, variational autoencoder},
	pages = {47503--47515},
}

@inproceedings{liu_non-parallel_2021,
	title = {Non-{Parallel} {Any}-to-{Many} {Voice} {Conversion} by {Replacing} {Speaker} {Statistics}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/liu21c_interspeech.html},
	doi = {10.21437/Interspeech.2021-557},
	abstract = {This paper proposes a non-parallel any-to-many voice conversion (VC) approach with a novel statistics replacement layer. Non-parallel VC is usually achieved by ﬁrstly disentangling linguistic and speaker representations, and then concatenating the linguistic content with the learned target speaker’s embedding at the conversion stage. While such a concatenation-based approach could introduce speaker-speciﬁc characteristics into the network, it is not very effective as it entirely relies on the network to learn to combine the linguistic content and the speaker characteristics. Inspired by X-vectors, where the statistics of hidden representation such as means and standard deviations are used for speaker differentiation, we propose a statistics replacement layer in VC systems to directly modify the hidden states to have the target speaker’s statistics. The speaker-speciﬁc statistics of hidden states are learned for each target speaker during training and are used as guidance for the statistics replacement layer during inference. Moreover, to better concentrate the speaker information into the statistics of hidden representation, a multitask training with X-vector based speaker classiﬁcation is also performed. Experimental results with Librispeech and VCTK datasets show that the proposed method can effectively improve the converted speech’s naturalness and similarity.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Liu, Yufei and Yu, Chengzhu and Shuai, Wang and Yang, Zhenchuan and Chao, Yang and Zhang, Weibin},
	month = aug,
	year = {2021},
	pages = {1369--1373},
}

@misc{shankar_diffeomorphic_2022-1,
	title = {A {Diffeomorphic} {Flow}-based {Variational} {Framework} for {Multi}-speaker {Emotion} {Conversion}},
	url = {http://arxiv.org/abs/2211.05071},
	abstract = {This paper introduces a new framework for non-parallel emotion conversion in speech. Our framework is based on two key contributions. First, we propose a stochastic version of the popular CycleGAN model. Our modified loss function introduces a Kullback Leibler (KL) divergence term that aligns the source and target data distributions learned by the generators, thus overcoming the limitations of sample wise generation. By using a variational approximation to this stochastic loss function, we show that our KL divergence term can be implemented via a paired density discriminator. We term this new architecture a variational CycleGAN (VCGAN). Second, we model the prosodic features of target emotion as a smooth and learnable deformation of the source prosodic features. This approach provides implicit regularization that offers key advantages in terms of better range alignment to unseen and out of distribution speakers. We conduct rigorous experiments and comparative studies to demonstrate that our proposed framework is fairly robust with high performance against several state-of-the-art baselines.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Shankar, Ravi and Hsieh, Hsi-Wei and Charon, Nicolas and Venkataraman, Archana},
	month = nov,
	year = {2022},
	doi = {10.48550/arXiv.2211.05071},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{li_high-quality_2021,
	title = {High-{Quality} {Many}-to-{Many} {Voice} {Conversion} {Using} {Transitive} {Star} {Generative} {Adversarial} {Networks} with {Adaptive} {Instance} {Normalization}},
	volume = {30},
	issn = {0218-1266},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218126621501887},
	doi = {10.1142/S0218126621501887},
	abstract = {This paper proposes a novel high-quality nonparallel many-to-many voice conversion method based on transitive star generative adversarial networks with adaptive instance normalization (Trans-StarGAN-VC with AdaIN). First, we improve the structure of generator with TransNets to make full use of hierarchical features associated with speech naturalness. In TransNets, many shortcut connections share hierarchical features between encoding and decoding part to capture sufficient linguistic and semantic information, which helps to provide natural sounding converted speech and accelerate the convergence of training process. Second, by incorporating AdaIN for style transfer, we enable the generator to learn sufficient speaker characteristic information directly from speech instead of using attribute labels, which also provides a promising framework for one-shot VC. Objective and subjective experiments with nonparallel training data show that our method significantly outperforms StarGAN-VC in both speech naturalness and speaker similarity. The mean values of mean opinion score (MOS) and ABX are increased by 24.5\% and 10.7\%, respectively. The comparison of spectrogram also shows that our method can provide more complete harmonic structures and details, and effectively bridge the gap between converted speech and target speech.},
	number = {10},
	urldate = {2023-09-19},
	journal = {Journal of Circuits, Systems and Computers},
	author = {Li, Yanping and He, Zhengtao and Zhang, Yan and Yang, Zhen},
	month = aug,
	year = {2021},
	keywords = {Adaptive instance normalization (AdaIN), Trans-StarGAN-VC, TransNets, high-quality, nonparallel, voice conversion (VC)},
	pages = {2150188},
}

@article{kang_connectionist_2021,
	title = {Connectionist temporal classification loss for vector quantized variational autoencoder in zero-shot voice conversion},
	volume = {116},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200421001494},
	doi = {10.1016/j.dsp.2021.103110},
	abstract = {Vector quantized variational autoencoder (VQ-VAE) has recently become an increasingly popular method in non-parallel zero-shot voice conversion (VC). The reason behind is that VQ-VAE is capable of disentangling the content and the speaker representations from the speech by using a content encoder and a speaker encoder, which is suitable for the VC task that makes the speech of a source speaker sound like the speech of the target speaker without changing the linguistic content. However, the converted speech is not satisfying because it is difficult to disentangle the pure content representations from the acoustic features due to the lack of linguistic supervision for the content encoder. To address this issue, under the framework of VQ-VAE, connectionist temporal classification (CTC) loss is proposed to guide the content encoder to learn pure content representations by using an auxiliary network. Based on the fact that the CTC loss is not affected by the sequence length of the output of the content encoder, adding the linguistic supervision to the content encoder can be much easier. This non-parallel many-to-many voice conversion model is named as CTC-VQ-VAE. VC experiments on the CMU ARCTIC and VCTK corpus are carried out to evaluate the proposed method. Both the objective and the subjective results show that the proposed approach significantly improves the speech quality and speaker similarity of the converted speech, compared with the traditional VQ-VAE method.},
	urldate = {2023-09-19},
	journal = {Digital Signal Processing},
	author = {Kang, Xiao and Huang, Hao and Hu, Ying and Huang, Zhihua},
	month = sep,
	year = {2021},
	keywords = {Connectionist temporal classification, VQ-VAE, Voice conversion, Zero-shot},
	pages = {103110},
}

@article{lee_many--many_2021,
	title = {Many-to-{Many} {Unsupervised} {Speech} {Conversion} {From} {Nonparallel} {Corpora}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3058382},
	abstract = {We address a nonparallel data-driven many-to-many speech modeling and multimodal style conversion method. In this work, we train a speech conversion model for multiple domains rather than a specific source and target domain pair, and we generate diverse output speech signals from a given source domain speech by transferring some speech style-related characteristics while preserving its linguistic content information. The proposed method comprises a variational autoencoder (VAE)-based many-to-many speech conversion network with a Wasserstein generative adversarial network (WGAN) and a skip-connected autoencoder-based self-supervised learning network. The proposed conversion network trains the models by decomposing the spectral features of the input speech signal into a content factor that represents domain-invariant information and a style factor that represents domain-related information to automatically estimate the various speech styles of each domain, and the network converts the input speech signal to another domain using the computed content factor with the target style factor we want to change. Diverse and multimodal outputs can be generated by sampling different style factors. We also train models in a stable manner and improve the quality of generated outputs by sharing the discriminator of the VAE-based speech conversion network and that of the self-supervised learning network. We apply the proposed method to speaker conversion and perform the perceptual evaluations. Experimental results revealed that the proposed method obtained high accuracy of converted spectra, significantly improved the sound quality and speaker similarity of the converted speech, and contributed to stable model training.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Lee, Yun Kyung and Kim, Hyun Woo and Park, Jeon Gue},
	year = {2021},
	keywords = {Data models, Decoding, Gallium nitride, Generative adversarial networks, Speech, Speech conversion (SC), Training, Wasserstein generative adversarial network (WGAN), many-to-many SC, non-parallel SC, self-supervised learning, variational auto-encoder (VAE)},
	pages = {27278--27286},
}

@inproceedings{huang_winvc_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{WINVC}: {One}-{Shot} {Voice} {Conversion} with {Weight} {Adaptive} {Instance} {Normalization}},
	isbn = {978-3-030-89363-7},
	shorttitle = {{WINVC}},
	doi = {10.1007/978-3-030-89363-7_42},
	abstract = {This paper proposes a one-shot voice conversion (VC) solution. In many one-shot voice conversion solutions (e.g., Auto-encoder-based VC methods), performances have dramatically been improved due to instance normalization and adaptive instance normalization. However, one-shot voice conversion fluency is still lacking, and the similarity is not good enough. This paper introduces the weight adaptive instance normalization strategy to improve the naturalness and similarity of one-shot voice conversion. Experimental results prove that under the VCTK data set, the MOS score of our proposed model, weight adaptive instance normalization voice conversion (WINVC), reaches 3.97 with five scales, and the SMOS reaches 3.31 with four scales. Besides, WINVC can achieve a MOS score of 3.44 and a SMOS score of 3.11 respectively for one-shot voice conversion under a small data set of 80 speakers with 5 pieces of utterances per person.},
	language = {en},
	booktitle = {{PRICAI} 2021: {Trends} in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Huang, Shengjie and Chen, Mingjie and Xu, Yanyan and Ke, Dengfeng and Hain, Thomas},
	editor = {Pham, Duc Nghia and Theeramunkong, Thanaruk and Governatori, Guido and Liu, Fenrong},
	year = {2021},
	keywords = {Generative adversarial networks (GANs), One-shot voice conversion, Weight adaptive instance normalization},
	pages = {559--573},
}

@misc{wang_accent_2020,
	title = {Accent and {Speaker} {Disentanglement} in {Many}-to-many {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2011.08609},
	abstract = {This paper proposes an interesting voice and accent joint conversion approach, which can convert an arbitrary source speaker's voice to a target speaker with non-native accent. This problem is challenging as each target speaker only has training data in native accent and we need to disentangle accent and speaker information in the conversion model training and re-combine them in the conversion stage. In our recognition-synthesis conversion framework, we manage to solve this problem by two proposed tricks. First, we use accent-dependent speech recognizers to obtain bottleneck features for different accented speakers. This aims to wipe out other factors beyond the linguistic information in the BN features for conversion model training. Second, we propose to use adversarial training to better disentangle the speaker and accent information in our encoder-decoder based conversion model. Specifically, we plug an auxiliary speaker classifier to the encoder, trained with an adversarial loss to wipe out speaker information from the encoder output. Experiments show that our approach is superior to the baseline. The proposed tricks are quite effective in improving accentedness and audio quality and speaker similarity are well maintained.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Wang, Zhichao and Ge, Wenshuo and Wang, Xiong and Yang, Shan and Gan, Wendong and Chen, Haitao and Li, Hai and Xie, Lei and Li, Xiulin},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.2011.08609},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{sakamoto_stargan-vcasr_2021,
	title = {{StarGAN}-{VC}+{ASR}: {StarGAN}-based {Non}-{Parallel} {Voice} {Conversion} {Regularized} by {Automatic} {Speech} {Recognition}},
	shorttitle = {{StarGAN}-{VC}+{ASR}},
	url = {http://arxiv.org/abs/2108.04395},
	doi = {10.21437/Interspeech.2021-492},
	abstract = {Preserving the linguistic content of input speech is essential during voice conversion (VC). The star generative adversarial network-based VC method (StarGAN-VC) is a recently developed method that allows non-parallel many-to-many VC. Although this method is powerful, it can fail to preserve the linguistic content of input speech when the number of available training samples is extremely small. To overcome this problem, we propose the use of automatic speech recognition to assist model training, to improve StarGAN-VC, especially in low-resource scenarios. Experimental results show that using our proposed method, StarGAN-VC can retain more linguistic information than vanilla StarGAN-VC.},
	urldate = {2023-09-18},
	booktitle = {Interspeech 2021},
	author = {Sakamoto, Shoki and Taniguchi, Akira and Taniguchi, Tadahiro and Kameoka, Hirokazu},
	month = aug,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	pages = {1359--1363},
}

@misc{luong_many--many_2021,
	title = {Many-to-{Many} {Voice} {Conversion} based {Feature} {Disentanglement} using {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/2107.06642},
	abstract = {Voice conversion is a challenging task which transforms the voice characteristics of a source speaker to a target speaker without changing linguistic content. Recently, there have been many works on many-to-many Voice Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results, however, these methods lack the ability to disentangle speaker identity and linguistic content to achieve good performance on unseen speaker scenarios. In this paper, we propose a new method based on feature disentanglement to tackle many to many voice conversion. The method has the capability to disentangle speaker identity and linguistic content from utterances, it can convert from many source speakers to many target speakers with a single autoencoder network. Moreover, it naturally deals with the unseen target speaker scenarios. We perform both objective and subjective evaluations to show the competitive performance of our proposed method compared with other state-of-the-art models in terms of naturalness and target speaker similarity.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Luong, Manh and Tran, Viet Anh},
	month = jul,
	year = {2021},
	doi = {10.48550/arXiv.2107.06642},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{broughton_investigating_2021,
	title = {Investigating {Deep} {Neural} {Structures} and their {Interpretability} in the {Domain} of {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2102.11420},
	abstract = {Generative Adversarial Networks (GANs) are machine learning networks based around creating synthetic data. Voice Conversion (VC) is a subset of voice translation that involves translating the paralinguistic features of a source speaker to a target speaker while preserving the linguistic information. The aim of non-parallel conditional GANs for VC is to translate an acoustic speech feature sequence from one domain to another without the use of paired data. In the study reported here, we investigated the interpretability of state-of-the-art implementations of non-parallel GANs in the domain of VC. We show that the learned representations in the repeating layers of a particular GAN architecture remain close to their original random initialised parameters, demonstrating that it is the number of repeating layers that is more responsible for the quality of the output. We also analysed the learned representations of a model trained on one particular dataset when used during transfer learning on another dataset. This showed extremely high levels of similarity across the entire network. Together, these results provide new insight into how the learned representations of deep generative networks change during learning and the importance in the number of layers.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Broughton, Samuel J. and Jalal, Md Asif and Moore, Roger K.},
	month = feb,
	year = {2021},
	doi = {10.48550/arXiv.2102.11420},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zhou_seen_2021,
	title = {Seen and {Unseen} emotional style transfer for voice conversion with a new emotional speech dataset},
	url = {http://arxiv.org/abs/2010.14794},
	abstract = {Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
	month = feb,
	year = {2021},
	doi = {10.48550/arXiv.2010.14794},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{huang_any--one_2020,
	title = {Any-to-{One} {Sequence}-to-{Sequence} {Voice} {Conversion} using {Self}-{Supervised} {Discrete} {Speech} {Representations}},
	url = {http://arxiv.org/abs/2010.12231},
	abstract = {We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Huang, Wen-Chin and Wu, Yi-Chiao and Hayashi, Tomoki and Toda, Tomoki},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.12231},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wang_learning_2021,
	title = {Learning {Explicit} {Prosody} {Models} and {Deep} {Speaker} {Embeddings} for {Atypical} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2011.01678},
	abstract = {Though significant progress has been made for the voice conversion (VC) of typical speech, VC for atypical speech, e.g., dysarthric and second-language (L2) speech, remains a challenge, since it involves correcting for atypical prosody while maintaining speaker identity. To address this issue, we propose a VC system with explicit prosodic modelling and deep speaker embedding (DSE) learning. First, a speech-encoder strives to extract robust phoneme embeddings from atypical speech. Second, a prosody corrector takes in phoneme embeddings to infer typical phoneme duration and pitch values. Third, a conversion model takes phoneme embeddings and typical prosody features as inputs to generate the converted speech, conditioned on the target DSE that is learned via speaker encoder or speaker adaptation. Extensive experiments demonstrate that speaker adaptation can achieve higher speaker similarity, and the speaker encoder based conversion model can greatly reduce dysarthric and non-native pronunciation patterns with improved speech intelligibility. A comparison of speech recognition results between the original dysarthric speech and converted speech show that absolute reduction of 47.6\% character error rate (CER) and 29.3\% word error rate (WER) can be achieved.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Wang, Disong and Liu, Songxiang and Sun, Lifa and Wu, Xixin and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2011.01678},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{nercessian_end--end_2021,
	address = {New Paltz, NY, USA},
	title = {End-to-{End} {Zero}-{Shot} {Voice} {Conversion} {Using} a {DDSP} {Vocoder}},
	isbn = {978-1-66544-870-3},
	url = {https://ieeexplore.ieee.org/document/9632754/},
	doi = {10.1109/WASPAA52581.2021.9632754},
	abstract = {In this paper, we propose a zero-shot voice conversion algorithm using a neural vocoder based on differential digital signal processing. The vocoder does not require auto-regression, and its lightweight, differentiable nature allows the proposed system to be trained in an end-to-end fashion. This enables the use of more perceptually relevant objective functions for model training, and allows feature conversion and vocoder sub-networks to internally learn their own acoustic representation in a data-driven manner. We illustrate the effectiveness of the proposed algorithm by both qualitative and quantitative means, with comparisons to some of our previous works.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {2021 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Nercessian, Shahan},
	month = oct,
	year = {2021},
	pages = {1--5},
}

@misc{he_improved_2021,
	title = {An {Improved} {StarGAN} for {Emotional} {Voice} {Conversion}: {Enhancing} {Voice} {Quality} and {Data} {Augmentation}},
	shorttitle = {An {Improved} {StarGAN} for {Emotional} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2107.08361},
	abstract = {Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2\% in Micro-F1 and 5\% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {He, Xiangheng and Chen, Junjie and Rizos, Georgios and Schuller, Björn W.},
	month = jul,
	year = {2021},
	doi = {10.48550/arXiv.2107.08361},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zhou_vaw-gan_2020,
	title = {{VAW}-{GAN} for {Disentanglement} and {Recomposition} of {Emotional} {Elements} in {Speech}},
	url = {http://arxiv.org/abs/2011.02314},
	abstract = {Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Zhou, Kun and Sisman, Berrak and Li, Haizhou},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.2011.02314},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{nguyen_accent_2022,
	title = {Accent {Conversion} using {Pre}-trained {Model} and {Synthesized} {Data} from {Voice} {Conversion}},
	url = {https://www.isca-speech.org/archive/interspeech_2022/nguyen22d_interspeech.html},
	doi = {10.21437/Interspeech.2022-10729},
	abstract = {Accent conversion (AC) aims to generate synthetic audios by changing the pronunciation pattern and prosody of source speakers (in source audios) while preserving voice quality and linguistic content. There has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents, the authors hence work on a solution to synthesize one as training input. The training pipeline is conducted via two steps. First, a voice conversion (VC) model is constructed to synthesize a training data set, containing pairs of audios in the same voice but two different accents. Second, an AC model is trained with the synthesized data to convert a source accented speech to a target accented speech. Given the recognized success of self-supervised learning speech representation (wav2vec 2.0) on certain speech problems such as VC, speech recognition, speech translation, and speech-tospeech translation, we adopt this architecture with some customization to train the AC model in the second step. With just 9-hour synthesized training data, the encoder initialized by the weight of the pre-trained wav2vec 2.0 model outperforms the LSTM-based encoder.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Nguyen, Tuan Nam and Pham, Ngoc-Quan and Waibel, Alexander},
	month = sep,
	year = {2022},
	pages = {2583--2587},
}

@inproceedings{tan_zero-shot_2021,
	title = {Zero-{Shot} {Voice} {Conversion} with {Adjusted} {Speaker} {Embeddings} and {Simple} {Acoustic} {Features}},
	doi = {10.1109/ICASSP39728.2021.9414975},
	abstract = {Zero-shot voice conversion (VC) where both source and target speakers are unseen in the training dataset has become a new research direction. Using speaker embeddings instead of one-hot vectors to represent speaker identity is a key point, which makes VC models work on unseen speakers. In our work, a newly designed neural network was used to adjust the speaker embeddings of unseen speakers. This enables speaker embeddings to perform better on zero-shot VC. In addition, disentangled representation of features is the mainstream method to achieve zero-shot VC. In terms of input features of VC model, we use Mel-cepstral and F0 as simple acoustic features (SAF) rather than Mel-spectrograms. This avoids F0 conflicts in decoder that existed in the previous methods. The evaluations demonstrate that our proposed methods improve the quality of converted speech in terms of naturalness and similarity.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Tan, Zhiyuan and Wei, Jianguo and Xu, Junhai and He, Yuqing and Lu, Wenhuan},
	month = jun,
	year = {2021},
	keywords = {Acoustics, Conferences, Decoding, Generators, Neural networks, Signal processing, Training, Voice conversion (VC), disentangled representation, speaker embedding, zero-shot VC},
	pages = {5964--5968},
}

@article{jafaryani_parallel_2022,
	title = {Parallel voice conversion with limited training data using stochastic variational deep kernel learning},
	volume = {115},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197622003335},
	doi = {10.1016/j.engappai.2022.105279},
	abstract = {There are two types of voice conversion methods: statistical and deep learning-based. Although statistical methods can train with limited data, they face challenges, including spectral oversmoothing and time-domain discontinuity. On the other hand, extensively researched deep learning-based methods rely primarily on massive amounts of data, which limits their practical applicability. Given that voice conversion is an engineering problem with limited training data, it is crucial to develop techniques that can produce satisfactory results in terms of quality and similarity in the absence of a large amount of data. This paper proposes a voice conversion model based on stochastic variational deep kernel learning (SVDKL), which works with limited training data. The model allows the use of both the deep neural network’s expressive capability and the high flexibility of the Gaussian process, which is a Bayesian and non-parametric method. The model utilizes a cascade of a deep neural network and a conventional kernel as the covariance function, which enables it to estimate non-smooth and more complex functions. Furthermore, the model’s sparse variational Gaussian process solves the scalability problem of exact inference and enables the learning of a global mapping function for the entire acoustic space. One of the most important aspects of the proposed scheme is that the model parameters are trained using marginal likelihood optimization, which takes into account both data fitting and model complexity. Considering model complexity reduces the training data by increasing the robustness to overfitting. To evaluate the proposed scheme, we examined the model’s performance with as little as approximately 80 s of training data. The results indicated that our method obtains a higher mean opinion score, smaller spectral distortion, and better preference tests than the state-of-the-art limited data methods.},
	urldate = {2023-09-17},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Jafaryani, Mohamadreza and Sheikhzadeh, Hamid and Pourahmadi, Vahid},
	month = oct,
	year = {2022},
	keywords = {Limited training data, Spectral mapping, Stochastic variational deep kernel learning, Voice conversion},
	pages = {105279},
}

@article{himawan_jointly_2022,
	title = {Jointly {Trained} {Conversion} {Model} {With} {LPCNet} for {Any}-to-{One} {Voice} {Conversion} {Using} {Speaker}-{Independent} {Linguistic} {Features}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3226350},
	abstract = {We propose a joint training scheme of an any-to-one voice conversion (VC) system with LPCNet to improve the speech naturalness, speaker similarity, and intelligibility of the converted speech. Recent advancements in neural-based vocoders, such as LPCNet, have enabled the production of more natural and clear speech. However, other components in typical VC systems are often designed independently, such as the conversion model. Hence, separate training strategies are used for each component that is not in direct correlation to the training objective of the vocoder preventing exploitation of the full potential of LPCNet. This problem is addressed by proposing a jointly trained conversion model and LPCNet. To accurately capture the linguistic contents of the given utterance, we use speaker-independent (SI) features derived from an automatic speech recognition (ASR) model trained using a mixed-language speech corpus. Subsequently, a conversion model maps the SI features to the acoustic representations used as input features to LPCNet. The possibility to synthesize cross-language speech using the proposed approach is also explored in this paper. Experimental results show that the proposed model can achieve real-time VC, unlocking the full potential of LPCNet and outperforming the state of the art.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Himawan, Ivan and Wang, Ruizhe and Sridharan, Sridha and Fookes, Clinton},
	year = {2022},
	keywords = {Acoustics, Automatic speech recognition, Feature extraction, Neural networks, Predictive models, Real-time systems, Vocoders, conversion model, joint training, neural vocoder, voice conversion},
	pages = {134029--134037},
}

@article{noauthor_voice_2022,
	title = {Voice {Frequency} {Synthesis} using {VAW}-{GAN} based {Amplitude} {Scaling} for {Emotion} {Transformation}},
	volume = {16},
	issn = {19767277},
	url = {http://itiis.org/digital-library/25315},
	doi = {10.3837/tiis.2022.02.018},
	abstract = {Mostly, artificial intelligence does not show any definite change in emotions. For this reason, it is hard to demonstrate empathy in communication with humans. If frequency modification is applied to neutral emotions, or if a different emotional frequency is added to them, it is possible to develop artificial intelligence with emotions. This study proposes the emotion conversion using the Generative Adversarial Network (GAN) based voice frequency synthesis. The proposed method extracts a frequency from speech data of twenty-four actors and actresses. In other words, it extracts voice features of their different emotions, preserves linguistic features, and converts emotions only. After that, it generates a frequency in variational auto-encoding Wasserstein generative adversarial network (VAW-GAN) in order to make prosody and preserve linguistic information. That makes it possible to learn speech features in parallel. Finally, it corrects a frequency by employing Amplitude Scaling. With the use of the spectral conversion of logarithmic scale, it is converted into a frequency in consideration of human hearing features. Accordingly, the proposed technique provides the emotion conversion of speeches in order to express emotions in line with artificially generated voices or speeches.},
	language = {en},
	number = {2},
	urldate = {2023-09-17},
	journal = {KSII Transactions on Internet and Information Systems},
	month = feb,
	year = {2022},
}

@article{liu_one-shot_2022,
	title = {One-shot voice conversion using a combination of {U2}-{Net} and vector quantization},
	volume = {199},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X22003887},
	doi = {10.1016/j.apacoust.2022.109014},
	abstract = {Researchers are paying more and more attention on one-shot voice conversion because of its superiority on collecting data, which can convert a timbre of one speech from one source speaker to another target speaker even for unseen speakers in the training dataset. In our previous work, a two-level nested U-structure was developed for one-shot voice conversion, called U2-VC. It was shown that the multi-scale features extracted by the U2-Net structure is promising in improving the naturalness of the converted speech. However, the converted speech of U2-VC still suffers from the source speaker timbre leakage problem, caused by only using the instance normalization for disentanglement. To solve this problem, the vector quantization is designed to disentangle the content and speaker identity features from the extracted multi-scale features. Meanwhile, instead of using one segment taken from one utterance as both the source and target signals, two non-overlapped segments cut from one utterance are used as the source and the target during the training phase. Both objective and subjective evaluation results show that the proposed voice conversion method effectively improves the converted speech quality when compared with the original U2-VC and other state-of-the-art baselines.},
	urldate = {2023-09-17},
	journal = {Applied Acoustics},
	author = {Liu, Fangkun and Wang, Hui and Ke, Yuxuan and Zheng, Chengshi},
	month = oct,
	year = {2022},
	keywords = {Multi-scale features, One-shot voice conversion, U-Net, U-VC, Vector quantization},
	pages = {109014},
}

@article{meftah_arabic_2022,
	title = {Arabic {Emotional} {Voice} {Conversion} {Using} {English} {Pre}-{Trained} {StarGANv2}-{VC}-{Based} {Model}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/23/12159},
	doi = {10.3390/app122312159},
	abstract = {The goal of emotional voice conversion (EVC) is to convert the emotion of a speaker’s voice from one state to another while maintaining the original speaker’s identity and the linguistic substance of the message. Research on EVC in the Arabic language is well behind that conducted on languages with a wider distribution, such as English. The primary objective of this study is to determine whether Arabic emotions may be converted using a model trained for another language. In this work, we used an unsupervised many-to-many non-parallel generative adversarial network (GAN) voice conversion (VC) model called StarGANv2-VC to perform an Arabic EVC (A-EVC). The latter is realized by using pre-trained phoneme-level automatic speech recognition (ASR) and fundamental frequency (F0) models in the English language. The generated voice is evaluated by prosody and spectrum conversion in addition to automatic emotion recognition and speaker identification using a convolutional recurrent neural network (CRNN). The results of the evaluation indicated that male voices were scored higher than female voices and that the evaluation score for the conversion from neutral to other emotions was higher than the evaluation scores for the conversion of other emotions.},
	language = {en},
	number = {23},
	urldate = {2023-09-17},
	journal = {Applied Sciences},
	author = {Meftah, Ali H. and Alotaibi, Yousef A. and Selouani, Sid-Ahmed},
	month = jan,
	year = {2022},
	keywords = {Arabic, GANs, StarGAN, emotion, emotional voice conversion, voice conversion},
	pages = {12159},
}

@inproceedings{chen_improving_2022,
	title = {Improving {Recognition}-{Synthesis} {Based} any-to-one {Voice} {Conversion} with {Cyclic} {Training}},
	doi = {10.1109/ICASSP43922.2022.9747140},
	abstract = {In recognition-synthesis based any-to-one voice conversion (VC), an automatic speech recognition (ASR) model is employed to extract content-related features and a synthesizer is built to predict the acoustic features of the target speaker from the content-related features of any source speakers at the conversion stage. Since source speakers are unknown at the training stage, we have to use the content-related features of the target speaker to estimate the parameters of the synthesizer. This inconsistency between conversion and training stages constrains the speaker similarity of converted speech. To address this issue, a cyclic training method is proposed in this paper. This method designs pseudo-source acoustic features, which are generated by converting the training data of the target speaker towards multiple speakers in a reference corpus. Then, these pseudo-source acoustic features are used as the input of the synthesizer at the training stage to predict the acoustic features of the target speaker and a cyclic reconstruction loss is derived. Experimental results show that our proposed method achieved more consistent accuracy of acoustic feature prediction for various source speakers than the baseline method. It also achieved better similarity of converted speech, especially for the pairs of source and target speakers with distant speaker characteristics.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chen, Yan-Nian and Liu, Li-Juan and Hu, Ya-Jun and Jiang, Yuan and Ling, Zhen-Hua},
	month = may,
	year = {2022},
	keywords = {Feature extraction, Predictive models, Signal processing, Synthesizers, Target recognition, Training, Training data, Voice conversion, any-to-one, cyclic training, recognition-synthesis},
	pages = {7007--7011},
}

@misc{lian_towards_2022,
	title = {Towards {Improved} {Zero}-shot {Voice} {Conversion} with {Conditional} {DSVAE}},
	url = {http://arxiv.org/abs/2205.05227},
	abstract = {Disentangling content and speaking style information is essential for zero-shot non-parallel voice conversion (VC). Our previous study investigated a novel framework with disentangled sequential variational autoencoder (DSVAE) as the backbone for information decomposition. We have demonstrated that simultaneous disentangling content embedding and speaker embedding from one utterance is feasible for zero-shot VC. In this study, we continue the direction by raising one concern about the prior distribution of content branch in the DSVAE baseline. We find the random initialized prior distribution will force the content embedding to reduce the phonetic-structure information during the learning process, which is not a desired property. Here, we seek to achieve a better content embedding with more phonetic information preserved. We propose conditional DSVAE, a new model that enables content bias as a condition to the prior modeling and reshapes the content embedding sampled from the posterior distribution. In our experiment on the VCTK dataset, we demonstrate that content embeddings derived from the conditional DSVAE overcome the randomness and achieve a much better phoneme classification accuracy, a stabilized vocalization and a better zero-shot VC performance compared with the competitive DSVAE baseline.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Lian, Jiachen and Zhang, Chunlei and Anumanchipalli, Gopala Krishna and Yu, Dong},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2205.05227},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{liang_pyramid_2022,
	title = {Pyramid {Attention} {CycleGAN} for {Non}-{Parallel} {Voice} {Conversion}},
	doi = {10.1109/ICCC56324.2022.10065952},
	abstract = {Non-parallel voice conversion (VC) is a voice mapping technology that uses non-parallel corpus to convert source speeches into target speeches while maintaining semantic information unchanged. Cycle-consistent adversarial network-based VC with Filling in Frames (MaskCycleGAN-VC) is proposed and generally accepted as a current benchmark method. While it solves the problem of time-frequency structures consistency, the performance of voice conversion is not satisfactory enough. There is still a large gap between target and converted voice in terms of naturalness and similarity. In addition, the performance of MaskCycleGAN-VC seriously deteriorates because of a limited amount of training data. In order to solve above problems, we propose Pyramid Attention CycleGAN (PACycleGAN) for voice conversion which integrates pyramid structure and attention mechanism. We use a method named differentiable augmentation to improve the data efficiency of GANs and make training more stable. We evaluate the performance of PACycleGAN on inter-gender and intra-gender non-parallel VC. Subjective and objective evaluations in naturalness and speaker similarity show that PACycleGAN-VC outperforms MaskCycleGAN-VC for every VC pair.11https://chenpaopao.github.io/chenpaopao/Cyclegan/index.html},
	booktitle = {2022 {IEEE} 8th {International} {Conference} on {Computer} and {Communications} ({ICCC})},
	author = {Liang, Xianchen and Bie, Zhisong and Ma, Shiwei},
	month = dec,
	year = {2022},
	keywords = {Convolution, Fuses, Logic gates, Semantics, Time-frequency analysis, Training, Training data, attention mechanism, differentiable augmentation, pyramid, voice conversion},
	pages = {139--143},
}

@misc{kim_u-gat-it_2020,
	title = {U-{GAT}-{IT}: {Unsupervised} {Generative} {Attentional} {Networks} with {Adaptive} {Layer}-{Instance} {Normalization} for {Image}-to-{Image} {Translation}},
	shorttitle = {U-{GAT}-{IT}},
	url = {http://arxiv.org/abs/1907.10830},
	abstract = {We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Kim, Junho and Kim, Minjae and Kang, Hyeonwoo and Lee, Kwanghee},
	month = apr,
	year = {2020},
	doi = {10.48550/arXiv.1907.10830},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{lian_robust_2022,
	title = {Robust {Disentangled} {Variational} {Speech} {Representation} {Learning} for {Zero}-shot {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2203.16705},
	abstract = {Traditional studies on voice conversion (VC) have made progress with parallel training data and known speakers. Good voice conversion quality is obtained by exploring better alignment modules or expressive mapping functions. In this study, we investigate zero-shot VC from a novel perspective of self-supervised disentangled speech representation learning. Specifically, we achieve the disentanglement by balancing the information flow between global speaker representation and time-varying content representation in a sequential variational autoencoder (VAE). A zero-shot voice conversion is performed by feeding an arbitrary speaker embedding and content embeddings to the VAE decoder. Besides that, an on-the-fly data augmentation training strategy is applied to make the learned representation noise invariant. On TIMIT and VCTK datasets, we achieve state-of-the-art performance on both objective evaluation, i.e., speaker verification (SV) on speaker embedding and content embedding, and subjective evaluation, i.e., voice naturalness and similarity, and remains to be robust even with noisy source/target utterances.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Lian, Jiachen and Zhang, Chunlei and Yu, Dong},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.16705},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@inproceedings{du_high_2023,
	address = {New York, NY, USA},
	series = {{CSAI} '22},
	title = {High {Quality} and {Similarity} {One}-{Shot} {Voice} {Conversion} {Using} {End}-to-{End} {Model}},
	isbn = {978-1-4503-9777-3},
	url = {https://dl.acm.org/doi/10.1145/3577530.3577575},
	doi = {10.1145/3577530.3577575},
	abstract = {Voice Conversion (VC) is becoming increasingly popular in speech synthesis applications. Most methods focus on many-to-many VC which can not be used for unseen speakers. One-shot (any-to-any) VC allows the source and the target speakers can be both unseen in the inference phase. This relies on an additional model to disengage linguistic information and speaker information. Most previous works were based on two-stage VC, which can lead to mismatches between the acoustic model and the vocoder, and the generated speech has poor quality or similarity. In this work, we proposed a novel method trained end-to-end for one-shot voice conversion. Unlike other one-shot methods, we use a combination of multiple ASV models to obtain more accurate and robust speaker embedding that can achieve high quality and similarity conversion. Experiment results demonstrate that our proposed method outperforms all considered baselines in different gender setups.},
	urldate = {2023-09-17},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Du, Renmingyue and Yao, Jixun},
	month = mar,
	year = {2023},
	keywords = {end-to-end, generative adversarial network, one-shot, voice conversion},
	pages = {284--288},
}

@inproceedings{fu_c-cycletransgan_2022,
	title = {C-{CycleTransGAN}: {A} {Non}-parallel {Controllable} {Cross}-gender {Voice} {Conversion} {Model} with {CycleGAN} and {Transformer}},
	shorttitle = {C-{CycleTransGAN}},
	doi = {10.23919/APSIPAASC55919.2022.9979821},
	abstract = {In this study, we propose a conversion intensity controllable model for the cross-gender voice conversion (VC)11Demo page can be found at https://cz26.github.io/DemoPage-c-CycleTransGAN-VoiceConversion/. In particular, we combine the CycleGAN and transformer module, and build a condition embedding network as an intensity controller. The model is firstly pre-trained with self-supervised learning on the single-gender voice reconstruction task, with the condition set to male-to-male or female-to-female. Then, we fine-tune the model on the cross-gender voice conversion task after the pretraining is completed, with the condition set to male-to-female or female-to-male. In the testing procedure, the condition is expected to be employed as a controllable parameter (scale) to adjust the conversion intensity. The proposed method was evaluated on the Voice Conversion Challenge dataset and compared to two baselines (CycleGAN, CycleTransGAN) with objective and subjective evaluations. The results show that our proposed model is able to equip the model with an additional function of cross-gender controllability and without hurting the voice conversion performance.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Fu, Changzeng and Liu, Chaoran and Ishi, Carlos Toshinori and Ishiguro, Hiroshi},
	month = nov,
	year = {2022},
	keywords = {Controllability, Information processing, Self-supervised learning, Task analysis, Testing, Training, Transformers, controllable cross-gender voice conversion, cycle-consistent adversarial networks, transformer},
	pages = {553--559},
}

@inproceedings{kishida_non-parallel_2022,
	title = {Non-{Parallel} {Voice} {Conversion} {Based} on {Free}-{Energy} {Minimization} of {Speaker}-{Conditional} {Restricted} {Boltzmann} {Machine}},
	doi = {10.23919/APSIPAASC55919.2022.9980151},
	abstract = {In this paper, we propose a non-parallel voice conversion method based on the minimization of the free energy of a restricted Boltzmann machine (RBM). The proposed method uses an RBM that learns the generative probability of acoustic features conditioned on a target speaker, and it iteratively updates the input acoustic features until their free energy reaches a local minimum to obtain converted features. Since it is based on the RBM, only a few hyperparameters need to be set, and the number of training parameters is very small. Therefore, training is stable. In determining the step size of the update formula in accordance with the Newton-Raphson method to obtain the feature that gives the local minimum of the free energy, we found that the Hesse matrix of the free energy can be approximated by a diagonal matrix, and the update can be performed efficiently with a small amount of calculation. In objective evaluation experiments, the proposed method outperforms StarGAN-VC in Mel-cepstral distortions. In subjective evaluation experiments, the performance of the proposed method is comparable to that of StarGAN-VC in similarity MOS.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Kishida, Takuya and Nakashika, Toru},
	month = nov,
	year = {2022},
	keywords = {Acoustic distortion, Acoustics, Adaptation models, Information processing, Minimization, Newton method, Training},
	pages = {251--255},
}

@inproceedings{ke_new_2022,
	title = {A {New} {Spoken} {Language} {Teaching} {Tech}: {Combining} {Multi}-attention and {AdaIN} for {One}-shot {Cross} {Language} {Voice} {Conversion}},
	shorttitle = {A {New} {Spoken} {Language} {Teaching} {Tech}},
	doi = {10.1109/ISCSLP57327.2022.10038137},
	abstract = {Computer aided pronunciation training(CAPT) plays an important role in oral language teaching. The main methods of traditional computer-assisted oral teaching include mispronunciation detection and pronunciation scoring and assessment.However, these two techniques only give negative feedback information such as scores or error categories. In this case,it is difficult for learners to refine their pronunciation through these two indicators without the guidance of correct speech.To tackle this problem, we proposed a cross language voice conversion(VC) framework that can generate speech with template speech content and learners’ own timbre,which can guide the learner’s pronunciation.To improve VC effect,we apply AdaIN in the fore-end and after the Value matrix in multi-head attention once respectively,called attention-AdaIN,which can improve the style transfer and sequence generation ability.We used attention-AdaIN to construct VC framework based on VAE.Experiments conducted on the AISHELL-3 and VCTK corpus showed that this new aprroach improved the baseline VAE-VC.},
	booktitle = {2022 13th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Ke, Dengfeng and Yao, Wenhan and Hu, Ruixin and Huang, Liangjie and Luo, Qi and Shu, Wentao},
	month = dec,
	year = {2022},
	keywords = {AdaIN, Adaptation models, Data models, Education, Fuses, Negative feedback, Transfer functions, Transforms, Voice conversion, multi-head attention},
	pages = {101--104},
}

@misc{kim_assem-vc_2021,
	title = {Assem-{VC}: {Realistic} {Voice} {Conversion} by {Assembling} {Modern} {Speech} {Synthesis} {Techniques}},
	shorttitle = {Assem-{VC}},
	url = {http://arxiv.org/abs/2104.00931},
	abstract = {Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training. Code and audio samples are available at https://github.com/mindslab-ai/assem-vc.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Kim, Kang-wook and Park, Seung-won and Lee, Junhyeok and Joe, Myun-chul},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2104.00931},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{long_enhancing_2022,
	title = {Enhancing {Zero}-{Shot} {Many} to {Many} {Voice} {Conversion} with {Self}-{Attention} {VAE}},
	url = {http://arxiv.org/abs/2203.16037},
	abstract = {Variational auto-encoder (VAE) is an effective neural network architecture to disentangle a speech utterance into speaker identity and linguistic content latent embeddings, then generate an utterance for a target speaker from that of a source speaker. This is possible by concatenating the identity embedding of the target speaker and the content embedding of the source speaker uttering a desired sentence. In this work, we propose to improve VAE models with self-attention and structural regularization (RGSM). Specifically, we found a suitable location of VAE's decoder to add a self-attention layer for incorporating non-local information in generating a converted utterance and hiding the source speaker's identity. We applied relaxed group-wise splitting method (RGSM) to regularize network weights and remarkably enhance generalization performance. In experiments of zero-shot many-to-many voice conversion task on VCTK data set, with the self-attention layer and relaxed group-wise splitting method, our model achieves a gain of speaker classification accuracy on unseen speakers by 28.3{\textbackslash}\% while slightly improved conversion voice quality in terms of MOSNet scores. Our encouraging findings point to future research on integrating more variety of attention structures in VAE framework while controlling model size and overfitting for advancing zero-shot many-to-many voice conversions.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Long, Ziang and Zheng, Yunling and Yu, Meng and Xin, Jack},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2203.16037},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{yang_streamable_2022,
	title = {Streamable {Speech} {Representation} {Disentanglement} and {Multi}-{Level} {Prosody} {Modeling} for {Live} {One}-{Shot} {Voice} {Conversion}},
	url = {https://www.isca-speech.org/archive/interspeech_2022/yang22p_interspeech.html},
	doi = {10.21437/Interspeech.2022-10277},
	abstract = {This paper takes efforts to tackle the challenge of “live” oneshot voice conversion (VC), which performs conversion across arbitrary speakers in a streaming way while retaining high intelligibility and naturalness. We propose a hybrid unsupervised and supervised learning based VC model with a two-stage model training strategy. Specially, we first employ an unsupervised disentanglement framework to separate speech representations of different granularities Experimental results demonstrate that our proposed method achieves comparable performance on speech naturalness, intelligibility and speaker similarity with offline VC solutions, with sufficient efficiency for practical real-time applications. Audio samples are available online for demonstration1.},
	language = {en},
	urldate = {2023-09-17},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Yang, Haoquan and Deng, Liqun and Yeung, Yu Ting and Zheng, Nianzu and Xu, Yong},
	month = sep,
	year = {2022},
	pages = {2578--2582},
}

@misc{wang_one-shot_2022,
	title = {One-shot {Voice} {Conversion} {For} {Style} {Transfer} {Based} {On} {Speaker} {Adaptation}},
	url = {http://arxiv.org/abs/2111.12277},
	abstract = {One-shot style transfer is a challenging task, since training on one utterance makes model extremely easy to over-fit to training data and causes low speaker similarity and lack of expressiveness. In this paper, we build on the recognition-synthesis framework and propose a one-shot voice conversion approach for style transfer based on speaker adaptation. First, a speaker normalization module is adopted to remove speaker-related information in bottleneck features extracted by ASR. Second, we adopt weight regularization in the adaptation process to prevent over-fitting caused by using only one utterance from target speaker as training data. Finally, to comprehensively decouple the speech factors, i.e., content, speaker, style, and transfer source style to the target, a prosody module is used to extract prosody representation. Experiments show that our approach is superior to the state-of-the-art one-shot VC systems in terms of style and speaker similarity; additionally, our approach also maintains good speech quality.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Wang, Zhichao and Xie, Qicong and Li, Tao and Du, Hongqiang and Xie, Lei and Zhu, Pengcheng and Bi, Mengxiao},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2111.12277},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{ding_study_2022,
	title = {A {Study} on {Low}-{Latency} {Recognition}-{Synthesis}-{Based} {Any}-to-{One} {Voice} {Conversion}},
	doi = {10.23919/APSIPAASC55919.2022.9980091},
	abstract = {Some application scenarios of voice conversion, such as identity disguise in voice communication, require low-latency generation of converted speech. In traditional conversion methods, both history and future information in input speech are utilized to predict the converted acoustic features at each frame, which leads to long latency of voice conversion. Therefore, this paper proposes a low-latency recognition-synthesis-based any-to-one voice conversion method. Bottleneck (BN) features are extracted by an automatic speech recognition (ASR) acoustic model for frame-by-frame phoneme classification. A minimum mutual information (MMI) loss is introduced to reduce the speaker information in BNs caused by the low-latency configuration. The BN features are sent into a speaker-dependent low-latency LSTM-based acoustic feature predictor and the speech waveforms are reconstructed by an LPCNet vocoder from predicted acoustic features. The total latency of our proposed voice conversion method is 190ms, which is less than the delay requirement for comfortable communication in ITU-T G.114. The naturalness of converted speech is comparable with the upper-bound model trained without low-latency constraints.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Ding, Yi-Yang and Liu, Li-Juan and Hu, Yu and Ling, Zhen-Hua},
	month = nov,
	year = {2022},
	keywords = {Acoustics, Data mining, Delay effects, Delays, Feature extraction, Information processing, Vocoders, deep learning, low-lantency, non-parallel, voice conversion},
	pages = {455--460},
}

@misc{zhang_sig-vc_2023,
	title = {{SIG}-{VC}: {A} {Speaker} {Information} {Guided} {Zero}-shot {Voice} {Conversion} {System} for {Both} {Human} {Beings} and {Machines}},
	shorttitle = {{SIG}-{VC}},
	url = {http://arxiv.org/abs/2111.03811},
	abstract = {Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Zhang, Haozhe and Cai, Zexin and Qin, Xiaoyi and Li, Ming},
	month = apr,
	year = {2023},
	doi = {10.48550/arXiv.2111.03811},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{li_asgan-vc_2022,
	title = {{ASGAN}-{VC}: {One}-{Shot} {Voice} {Conversion} with {Additional} {Style} {Embedding} and {Generative} {Adversarial} {Networks}},
	shorttitle = {{ASGAN}-{VC}},
	doi = {10.23919/APSIPAASC55919.2022.9979975},
	abstract = {In this paper, we present a voice conversion system that improves the quality of generated voice and its similarity to the target voice style significantly. Many VC systems use feature-disentangle-based learning techniques to separate speakers' voices from their linguistic content in order to translate a voice into another style. This is the approach we are taking. To prevent speaker-style information from obscuring the content embedding, some previous works quantize or reduce the dimension of the embedding. However, an imperfect disentanglement would damage the quality and similarity of the sound. In this paper, to further improve quality and similarity in voice conversion, we propose a novel style transfer method within an autoencoder-based VC system that involves generative adversarial training. The conversion process was objectively evaluated using the fair third-party speaker verification system, the results shows that ASGAN-VC outperforms VQVC + and AGAINVC in terms of speaker similarity. A subjectively observing that our proposal outperformed the VQVC + and AGAINVC in terms of naturalness and speaker similarity.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Li, WeiCheng and Wei, Tzer-Jen},
	month = nov,
	year = {2022},
	keywords = {Convolution, Generative adversarial networks, Information processing, Linguistics, Proposals, Training},
	pages = {1932--1937},
}

@inproceedings{van_niekerk_comparison_2022,
	title = {A {Comparison} of {Discrete} and {Soft} {Speech} {Units} for {Improved} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2111.02392},
	doi = {10.1109/ICASSP43922.2022.9746484},
	abstract = {The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech. Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code available at https://github.com/bshall/soft-vc/.},
	urldate = {2023-09-15},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {van Niekerk, Benjamin and Carbonneau, Marc-André and Zaïdi, Julian and Baas, Mathew and Seuté, Hugo and Kamper, Herman},
	month = may,
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {6562--6566},
}

@article{yang_mel-s3r_2023,
	title = {Mel-{S3R}: {Combining} {Mel}-spectrogram and self-supervised speech representation with {VQ}-{VAE} for any-to-any voice conversion},
	volume = {151},
	issn = {0167-6393},
	shorttitle = {Mel-{S3R}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167639323000663},
	doi = {10.1016/j.specom.2023.05.004},
	abstract = {The self-supervised speech representation (S3R) has succeeded in many downstream tasks, such as speaker recognition and voice conversion thanks to its high-level information. Voice conversion (VC) is a task to convert the source speech into a target speaker’s voice. Though S3R features effectively encode content and speaker information, spectral features contain low-level acoustic information that is complementary to the S3R. As a result, solely relying on the S3R features for VC may not be optimal. In order to seek speech representation carrying both high-level learned information and low-level spectral details for VC, we proposed a three-level attention to combine Mel-spectrogram (Mel) and S3R, denoted as Mel-S3R. In particular, S3R features are high-level learned representations extracted by a pre-trained network with self-supervised learning. Whereas Mel is the spectral feature representing the acoustic information. Then the proposed Mel-S3R is used as the input of any-to-any VQ-VAE-based VC and the experiments are performed as a downstream task. Objective metrics and subjective listening tests have demonstrated that the proposed Mel-S3R speech representation facilitates the VC framework to achieve robust performance in terms of both speech quality and speaker similarity.},
	urldate = {2023-09-15},
	journal = {Speech Communication},
	author = {Yang, Jichen and Zhou, Yi and Huang, Hao},
	month = jun,
	year = {2023},
	pages = {52--63},
}

@misc{shankar_diffeomorphic_2022,
	title = {A {Diffeomorphic} {Flow}-based {Variational} {Framework} for {Multi}-speaker {Emotion} {Conversion}},
	url = {http://arxiv.org/abs/2211.05071},
	abstract = {This paper introduces a new framework for non-parallel emotion conversion in speech. Our framework is based on two key contributions. First, we propose a stochastic version of the popular CycleGAN model. Our modified loss function introduces a Kullback Leibler (KL) divergence term that aligns the source and target data distributions learned by the generators, thus overcoming the limitations of sample wise generation. By using a variational approximation to this stochastic loss function, we show that our KL divergence term can be implemented via a paired density discriminator. We term this new architecture a variational CycleGAN (VCGAN). Second, we model the prosodic features of target emotion as a smooth and learnable deformation of the source prosodic features. This approach provides implicit regularization that offers key advantages in terms of better range alignment to unseen and out of distribution speakers. We conduct rigorous experiments and comparative studies to demonstrate that our proposed framework is fairly robust with high performance against several state-of-the-art baselines.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Shankar, Ravi and Hsieh, Hsi-Wei and Charon, Nicolas and Venkataraman, Archana},
	month = nov,
	year = {2022},
	doi = {10.48550/arXiv.2211.05071},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{dang_training_2022,
	title = {Training {Robust} {Zero}-{Shot} {Voice} {Conversion} {Models} with {Self}-supervised {Features}},
	url = {http://arxiv.org/abs/2112.04424},
	abstract = {Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker characteristic of an utterance to match an unseen target speaker without relying on parallel training data. Recently, self-supervised learning of speech representation has been shown to produce useful linguistic units without using transcripts, which can be directly passed to a VC model. In this paper, we showed that high-quality audio samples can be achieved by using a length resampling decoder, which enables the VC model to work in conjunction with different linguistic feature extractors and vocoders without requiring them to operate on the same sequence length. We showed that our method can outperform many baselines on the VCTK dataset. Without modifying the architecture, we further demonstrated that a) using pairs of different audio segments from the same speaker, b) adding a cycle consistency loss, and c) adding a speaker classification loss can help to learn a better speaker embedding. Our model trained on LibriTTS using these techniques achieves the best performance, producing audio samples transferred well to the target speaker's voice, while preserving the linguistic content that is comparable with actual human utterances in terms of Character Error Rate.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Dang, Trung and Tran, Dung and Chin, Peter and Koishida, Kazuhito},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2112.04424},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{nguyen_nvc-net_2021,
	title = {{NVC}-{Net}: {End}-to-{End} {Adversarial} {Voice} {Conversion}},
	shorttitle = {{NVC}-{Net}},
	url = {http://arxiv.org/abs/2106.00992},
	abstract = {Voice conversion has gained increasing popularity in many applications of speech synthesis. The idea is to change the voice identity from one speaker into another while keeping the linguistic content unchanged. Many voice conversion approaches rely on the use of a vocoder to reconstruct the speech from acoustic features, and as a consequence, the speech quality heavily depends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end adversarial network, which performs voice conversion directly on the raw audio waveform of arbitrary length. By disentangling the speaker identity from the speech content, NVC-Net is able to perform non-parallel traditional many-to-many voice conversion as well as zero-shot voice conversion from a short utterance of an unseen target speaker. Importantly, NVC-Net is non-autoregressive and fully convolutional, achieving fast inference. Our model is capable of producing samples at a rate of more than 3600 kHz on an NVIDIA V100 GPU, being orders of magnitude faster than state-of-the-art methods under the same hardware configurations. Objective and subjective evaluations on non-parallel many-to-many voice conversion tasks show that NVC-Net obtains competitive results with significantly fewer parameters.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Nguyen, Bac and Cardinaux, Fabien},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2106.00992},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{luo_decoupling_2023,
	title = {Decoupling {Speaker}-{Independent} {Emotions} for {Voice} {Conversion} via {Source}-{Filter} {Networks}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3190715},
	abstract = {Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Luo, Zhaojie and Lin, Shoufeng and Liu, Rui and Baba, Jun and Yoshikawa, Yuichiro and Ishiguro, Hiroshi},
	year = {2023},
	keywords = {Acoustics, Auto-encoder, Codes, Larynx, Rhythm, Speech processing, Timbre, Training, emotional voice conversion, prosody, source-filter networks, valence arousal},
	pages = {11--24},
}

@inproceedings{tanaka_distilling_2023,
	title = {Distilling {Sequence}-to-{Sequence} {Voice} {Conversion} {Models} for {Streaming} {Conversion} {Applications}},
	doi = {10.1109/SLT54892.2023.10023432},
	abstract = {This paper describes a method for distilling a recurrent-based sequence-to-sequence (S2S) voice conversion (VC) model. Although the performance of recent VCs is becoming higher quality, streaming conversion is still a challenge when considering practical applications. To achieve streaming VC, the conversion model needs a streamable structure, a causal layer rather than a non-causal layer. Motivated by this constraint and recent advances in S2S learning, we apply the teacher-student framework to recurrent-based S2S- VC models. A major challenge is how to minimize degradation due to the use of causal layers which masks future input information. Experimental evaluations show that except for male-to-female speaker conversion, our approach is able to maintain the teacher model's performance in terms of subjective evaluations despite the streamable student model structure. Audio samples can be accessed on http://www.kecl.ntt.co.jp/people/tanaka.ko/projects/dists2svc.},
	booktitle = {2022 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Tanaka, Kou and Kameoka, Hirokazu and Kaneko, Takuhiro and Seki, Shogo},
	month = jan,
	year = {2023},
	keywords = {Conferences, Decoding, Degradation, Real-time systems, Self-supervised learning, Training, Voice conversion, distillation, sequence-to-sequence learning, streaming conversion},
	pages = {1022--1028},
}

@misc{lu_disentangled_2022,
	title = {Disentangled {Speech} {Representation} {Learning} for {One}-{Shot} {Cross}-lingual {Voice} {Conversion} {Using} \${\textbackslash}beta\$-{VAE}},
	url = {http://arxiv.org/abs/2210.13771},
	abstract = {We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by \${\textbackslash}beta\$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Lu, Hui and Wang, Disong and Wu, Xixin and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2210.13771},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{chun_non-parallel_2023,
	title = {Non-{Parallel} {Voice} {Conversion} {Using} {Cycle}-{Consistent} {Adversarial} {Networks} with {Self}-{Supervised} {Representations}},
	doi = {10.1109/CCNC51644.2023.10060510},
	abstract = {Numerous voice conversion techniques using non-parallel data have been presented. Among these, there are many algorithms related to style transfer. This is because the voice conversion problem can be determined as a style transfer problem, where the linguistic and speaker information can be regarded as domains and styles, respectively. Here, the group of CycleGAN-VC series has considerable achievement, and thus we examine the feasibility of CycleGAN-VC for self-supervised representations. In other words, we incorporate analysis features extracted from wav2vec into the CycleGAN-VC model. Objective experiments showed that the quality of the converted speech is comparable to that of the original speech, and the source speech was successfully transformed into the voice of the target speech while preserving the linguistic information.},
	booktitle = {2023 {IEEE} 20th {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	author = {Chun, Chanjun and Lee, Young Han and Lee, Geon Woo and Jeon, Moongu and Kim, Hong Kook},
	month = jan,
	year = {2023},
	keywords = {Analytical models, CycleGAN-VC, Feature extraction, Linguistics, Voice conversion, generative adversarial networks, self-supervised learning, wav2vec},
	pages = {931--932},
}

@inproceedings{gu_voice_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Voice {Conversion} {Using} {Learnable} {Similarity}-{Guided} {Masked} {Autoencoder}},
	isbn = {978-3-031-25115-3},
	doi = {10.1007/978-3-031-25115-3_4},
	abstract = {Voice conversion (VC) is an important voice forgery method that poses a serious threat to personal privacy protection, especially with remarkable achievements in timbre modification. To support forensic research on converted speech and further enrich the sources of fake speech, it is imperative to investigate new robust VC methods. VC is also considered a typical style transfer task, where style refers to speaker identity, suggesting that achieving sufficient feature decoupling is the key to obtaining robust performance. However, mainstream decoupling methods based on information-constrained bottlenecks still fail to obtain robust content-style trade-offs. In this paper, we propose a learnable similarity-guided mask (LSGM) algorithm to address the robustness problem. First, to make feature decoupling independent of specific language constructs and more applicable to diverse content, LSGM performs inter-frame feature compression only relying on the similarity of adjacent frames instead of complex inter-frame content correlation. Second, we implement feature compression by masking instead of dimensionality reduction, so no additional modules are needed to convey the speech frame length information. Moreover, we propose MAE-VC by using LSGM, which is an end-to-end masked autoencoder (MAE) with self-supervised representation learning. Experimental results indicate that MAE-VC performs comparable to state-of-the-art methods on speaker similarity and significantly improves the performance on content consistency.},
	language = {en},
	booktitle = {Digital {Forensics} and {Watermarking}},
	publisher = {Springer Nature Switzerland},
	author = {Gu, Yewei and Zhao, Xianfeng and Yi, Xiaowei and Xiao, Junchao},
	editor = {Zhao, Xianfeng and Tang, Zhenjun and Comesaña-Alfaro, Pedro and Piva, Alessandro},
	year = {2023},
	keywords = {Feature decoupling, Learnable similarity-guided mask, Masked autoencoder, Style transfer, Voice conversion},
	pages = {53--67},
}

@inproceedings{lu_towards_2021,
	title = {Towards {Unseen} {Speakers} {Zero}-{Shot} {Voice} {Conversion} with {Generative} {Adversarial} {Networks}},
	abstract = {Zero-shot many-to-many voice conversion receives wide-spread attention but remains a challenging task. Recently, AUTOVC, which is based on conditional autoencoders, achieves state-of-the-art results in zero-shot voice conversion. However, the carefully designed bottleneck and the autoencoders based framework of AUTOVC limit further improvement of zero-shot voice conversion. In this paper, we propose a Generative Adversarial Network(GAN) based framework to disentangle the timbre and content of speech, and to synthesize new speech given unseen speakers and corpora. Towards unseen speaker, our framework extracts timbre embedding from an input speech with timbre encoder and produces content distribution embedding from any other speech with content encoder. Our framework learns to synthesize new speeches via conversion-reconstruction cycle training and to enhance the quality of conversion with adversarial training. Our experiments demonstrate that the proposed framework can generate outputs with comparable quality even for speakers that are not seen in the training dataset.},
	booktitle = {2021 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Lu, Weirui and Xing, Xiaofen and Xu, Xiangmin and Zhang, Weibin},
	month = dec,
	year = {2021},
	keywords = {Convergence, Generative adversarial networks, Information processing, Speech enhancement, Task analysis, Timbre, Training},
	pages = {854--858},
}

@inproceedings{zhou_voice_2018,
	title = {Voice {Conversion} with {Conditional} {SampleRNN}},
	url = {http://arxiv.org/abs/1808.08311},
	doi = {10.21437/Interspeech.2018-1121},
	abstract = {Here we present a novel approach to conditioning the SampleRNN generative model for voice conversion (VC). Conventional methods for VC modify the perceived speaker identity by converting between source and target acoustic features. Our approach focuses on preserving voice content and depends on the generative network to learn voice style. We first train a multi-speaker SampleRNN model conditioned on linguistic features, pitch contour, and speaker identity using a multi-speaker speech corpus. Voice-converted speech is generated using linguistic features and pitch contour extracted from the source speaker, and the target speaker identity. We demonstrate that our system is capable of many-to-many voice conversion without requiring parallel data, enabling broad applications. Subjective evaluation demonstrates that our approach outperforms conventional VC methods.},
	urldate = {2023-09-15},
	booktitle = {Interspeech 2018},
	author = {Zhou, Cong and Horgan, Michael and Kumar, Vivek and Vasco, Cristina and Darcy, Dan},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1973--1977},
}

@inproceedings{li_spectro-temporal_2017,
	title = {Spectro-{Temporal} {Modelling} with {Time}-{Frequency} {LSTM} and {Structured} {Output} {Layer} for {Voice} {Conversion}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/li17l_interspeech.html},
	doi = {10.21437/Interspeech.2017-1122},
	abstract = {From speech, speaker identity can be mostly characterized by the spectro-temporal structures of spectrum. Although recent researches have demonstrated the effectiveness of employing long short-term memory (LSTM) recurrent neural network (RNN) in voice conversion, traditional LSTM-RNN based approaches usually focus on temporal evolutions of speech features only. In this paper, we improve the conventional LSTM-RNN method for voice conversion by employing the two-dimensional time-frequency LSTM (TFLSTM) to model spectro-temporal warping along both time and frequency axes. A multi-task learned structured output layer (SOL) is afterward adopted to capture the dependencies between spectral and pitch parameters for further improvement, where spectral parameter targets are conditioned upon pitch parameters prediction. Experimental results show the proposed approach outperforms conventional systems in speech quality and speaker similarity.},
	language = {en},
	urldate = {2023-09-15},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Li, Runnan and Wu, Zhiyong and Ning, Yishuang and Sun, Lifa and Meng, Helen and Cai, Lianhong},
	month = aug,
	year = {2017},
	pages = {3409--3413},
}

@inproceedings{xu_two-pathway_2021,
	title = {Two-{Pathway} {Style} {Embedding} for {Arbitrary} {Voice} {Conversion}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/xu21d_interspeech.html},
	doi = {10.21437/Interspeech.2021-506},
	abstract = {Arbitrary voice conversion, also referred to as zero-shot voice conversion, has recently attracted increased attention in the literature. Although disentangling the linguistic and style representations for acoustic features is an effective way to achieve zero-shot voice conversion, the problem of how to convert to a natural speaker style is challenging because of the intrinsic variabilities of speech and the difﬁculties of completely decoupling them. For this reason, in this paper, we propose a Two-Pathway Style Embedding Voice Conversion framework (TPSE-VC) for realistic and natural speech conversion. The novel feature of this method is to simultaneously embed sentence-level and phoneme-level style information. A novel attention mechanism is proposed to implement the implicit alignment for timbre style and phoneme content, further embedding a phoneme-level style representation. In addition, we consider embedding the complete set of time steps of audio style into a ﬁxed-length vector to obtain the sentence-level style representation. Moreover, TPSEVC does not require any pre-trained models, and is only trained with non-parallel speech data. Experimental results demonstrate that the proposed TPSE-VC outperforms the state-of-theart results on zero-shot voice conversion.},
	language = {en},
	urldate = {2023-09-15},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Xu, Xuexin and Shi, Liang and Chen, Jinhui and Chen, Xunquan and Lian, Jie and Lin, Pingyuan and Zhang, Zhihong and Hancock, Edwin R.},
	month = aug,
	year = {2021},
	pages = {1364--1368},
}

@misc{bonnici_timbre_2021,
	title = {Timbre {Transfer} with {Variational} {Auto} {Encoding} and {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2109.02096},
	abstract = {This research project investigates the application of deep learning to timbre transfer, where the timbre of a source audio can be converted to the timbre of a target audio with minimal loss in quality. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Furthermore, variations of the adopted approach are trained, and generalised performance is compared using the metrics SSIM (Structural Similarity Index) and FAD (Frech{\textbackslash}'et Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, and that the adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space. It was also found that the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Bonnici, Russell Sammut and Saitis, Charalampos and Benning, Martin},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2109.02096},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{kameoka_convs2s-vc_2020,
	title = {{ConvS2S}-{VC}: {Fully} convolutional sequence-to-sequence voice conversion},
	shorttitle = {{ConvS2S}-{VC}},
	url = {http://arxiv.org/abs/1811.01609},
	abstract = {This paper proposes a voice conversion (VC) method using sequence-to-sequence (seq2seq or S2S) learning, which flexibly converts not only the voice characteristics but also the pitch contour and duration of input speech. The proposed method, called ConvS2S-VC, has three key features. First, it uses a model with a fully convolutional architecture. This is particularly advantageous in that it is suitable for parallel computations using GPUs. It is also beneficial since it enables effective normalization techniques such as batch normalization to be used for all the hidden layers in the networks. Second, it achieves many-to-many conversion by simultaneously learning mappings among multiple speakers using only a single model instead of separately learning mappings between each speaker pair using a different model. This enables the model to fully utilize available training data collected from multiple speakers by capturing common latent features that can be shared across different speakers. Owing to this structure, our model works reasonably well even without source speaker information, thus making it able to handle any-to-many conversion tasks. Third, we introduce a mechanism, called the conditional batch normalization that switches batch normalization layers in accordance with the target speaker. This particular mechanism has been found to be extremely effective for our many-to-many conversion model. We conducted speaker identity conversion experiments and found that ConvS2S-VC obtained higher sound quality and speaker similarity than baseline methods. We also found from audio examples that it could perform well in various tasks including emotional expression conversion, electrolaryngeal speech enhancement, and English accent conversion.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Kameoka, Hirokazu and Tanaka, Kou and Kwasny, Damian and Kaneko, Takuhiro and Hojo, Nobukatsu},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.1811.01609},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{hasunuma_non-parallel_2018,
	title = {Non-parallel {Voice} {Conversion} {Using} {Generative} {Adversarial} {Networks}},
	doi = {10.1109/SMC.2018.00283},
	abstract = {Considering the ease of data collection, it is desirable to build a voice conversion system (VCS) from non-parallel voice data, with different sentences read by the source and target speakers. Previous non-parallel VCSs have used either a mel-cepstrum or spectral envelope as an input feature. However, these features have different acoustic characteristics that play important roles in speaker recognition. Thus, we propose a non-parallel VCS that efficiently uses both mel-cepstrum and spectral envelopes as input features. Our method is based on three key strategies: 1) we use generative adversarial networks for voice conversion; 2) we add noise to facilitate the training of the formant part; and 3) we integrate the acoustic features to generate high-quality converted voices. Subjective evaluations in the Voice Conversion Challenge 2016 (VCC 2016) revealed that our model outperformed the previous approaches in terms of the naturalness and similarity of the converted voice.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Hasunuma, Yuta and Hirayama, Chiaki and Kobayashi, Masayuki and Nagao, Tomoharu},
	month = oct,
	year = {2018},
	keywords = {Acoustics, Feature extraction, Gallium nitride, Generative adversarial networks, Generators, Mathematical model, Training, deep neural network, generative adversarial networks, non-parallel voice conversion},
	pages = {1635--1640},
}

@inproceedings{nercessian_improved_2020,
	title = {Improved {Zero}-{Shot} {Voice} {Conversion} {Using} {Explicit} {Conditioning} {Signals}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/nercessian20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1889},
	abstract = {In this paper, we propose a zero-shot voice conversion algorithm adding a number of conditioning signals to explicitly transfer prosody, linguistic content, and dynamics to conversion results. We show that the proposed approach improves overall conversion quality and generalization to out-of-domain samples relative to a baseline implementation of AutoVC, as the inclusion of conditioning signals can help reduce the burden on the model’s encoder to implicitly learn all of the different aspects involved in speech production. An ablation analysis illustrates the effectiveness of the proposed method.},
	language = {en},
	urldate = {2023-09-15},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Nercessian, Shahan},
	month = oct,
	year = {2020},
	pages = {4711--4715},
}

@misc{wang_adversarially_2021,
	title = {Adversarially learning disentangled speech representations for robust multi-factor voice conversion},
	url = {http://arxiv.org/abs/2102.00184},
	abstract = {Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speechfactors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment,which however is hard to ensure robust speech representationdisentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP)network inspired by BERT. The adversarial network is used tominimize the correlations between the speech representations,by randomly masking and predicting one of the representationsfrom the others. Experimental results show that the proposedframework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to3.30 and decreasing the MCD from 3.89 to 3.58.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Wang, Jie and Li, Jingbei and Zhao, Xintao and Wu, Zhiyong and Kang, Shiyin and Meng, Helen},
	month = aug,
	year = {2021},
	doi = {10.48550/arXiv.2102.00184},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wang_controllable_2022,
	title = {Controllable {Speech} {Representation} {Learning} {Via} {Voice} {Conversion} and {AIC} {Loss}},
	doi = {10.1109/ICASSP43922.2022.9747590},
	abstract = {Speech representation learning transforms speech into features that are suitable for downstream tasks, e.g. speech recognition, phoneme classification, or speaker identification. For such recognition tasks, a representation can be lossy (non-invertible), which is typical of BERT-like self-supervised models. However, when used for synthesis tasks, we find these lossy representations prove to be insufficient to plausibly reconstruct the input signal. This paper introduces a method for invertible and controllable speech representation learning based on disentanglement. The representation can be decoded into a signal perceptually identical to the original. Moreover, its disentangled components (content, pitch, speaker identity, and energy) can be controlled independently to alter the synthesis result. Our model builds upon a zero-shot voice conversion model AutoVC-F0, in which we introduce alteration invariant content loss (AIC loss) and adversarial training (GAN). Through objective measures and subjective tests, we show that our formulation offers significant improvement in voice conversion sound quality as well as more precise control over the disentangled features.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Yunyun and Su, Jiaqi and Finkelstein, Adam and Jin, Zeyu},
	month = may,
	year = {2022},
	keywords = {Codes, Conferences, Representation learning, Speech coding, Speech recognition, Training, Transforms, representation learning, voice conversion},
	pages = {6682--6686},
}

@misc{chan_speechsplit_2022,
	title = {{SpeechSplit} 2.0: {Unsupervised} speech disentanglement for voice conversion {Without} tuning autoencoder {Bottlenecks}},
	shorttitle = {{SpeechSplit} 2.0},
	url = {http://arxiv.org/abs/2203.14156},
	abstract = {SpeechSplit can perform aspect-specific voice conversion by disentangling speech into content, rhythm, pitch, and timbre using multiple autoencoders in an unsupervised manner. However, SpeechSplit requires careful tuning of the autoencoder bottlenecks, which can be time-consuming and less robust. This paper proposes SpeechSplit 2.0, which constrains the information flow of the speech component to be disentangled on the autoencoder input using efficient signal processing methods instead of bottleneck tuning. Evaluation results show that SpeechSplit 2.0 achieves comparable performance to SpeechSplit in speech disentanglement and superior robustness to the bottleneck size variations. Our code is available at https://github.com/biggytruck/SpeechSplit2.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Chan, Chak Ho and Qian, Kaizhi and Zhang, Yang and Hasegawa-Johnson, Mark},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.14156},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wang_drvc_2022,
	title = {{DRVC}: {A} {Framework} of {Any}-to-{Any} {Voice} {Conversion} with {Self}-{Supervised} {Learning}},
	shorttitle = {{DRVC}},
	url = {http://arxiv.org/abs/2202.10976},
	abstract = {Any-to-any voice conversion problem aims to convert voices for source and target speakers, which are out of the training data. Previous works wildly utilize the disentangle-based models. The disentangle-based model assumes the speech consists of content and speaker style information and aims to untangle them to change the style information for conversion. Previous works focus on reducing the dimension of speech to get the content information. But the size is hard to determine to lead to the untangle overlapping problem. We propose the Disentangled Representation Voice Conversion (DRVC) model to address the issue. DRVC model is an end-to-end self-supervised model consisting of the content encoder, timbre encoder, and generator. Instead of the previous work for reducing speech size to get content, we propose a cycle for restricting the disentanglement by the Cycle Reconstruct Loss and Same Loss. The experiments show there is an improvement for converted speech on quality and voice similarity.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Wang, Qiqi and Zhang, Xulong and Wang, Jianzong and Cheng, Ning and Xiao, Jing},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.10976},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{choi_neural_2021,
	title = {Neural {Analysis} and {Synthesis}: {Reconstructing} {Speech} from {Self}-{Supervised} {Representations}},
	shorttitle = {Neural {Analysis} and {Synthesis}},
	url = {http://arxiv.org/abs/2110.14513},
	abstract = {We present a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. Most of the previous works have focused on using information bottleneck to disentangle analysis features for controllable synthesis, which usually results in poor reconstruction quality. We address this issue by proposing a novel training strategy based on information perturbation. The idea is to perturb information in the original input signal (e.g., formant, pitch, and frequency response), thereby letting synthesis networks selectively take essential attributes to reconstruct the input signal. Because NANSY does not need any bottleneck structures, it enjoys both high reconstruction quality and controllability. Furthermore, NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. Taking advantage of fully self-supervised training, NANSY can be easily extended to a multilingual setting by simply training it with a multilingual dataset. The experiments show that NANSY can achieve significant improvement in performance in several applications such as zero-shot voice conversion, pitch shift, and time-scale modification.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Choi, Hyeong-Seok and Lee, Juheon and Kim, Wansoo and Lee, Jie Hwan and Heo, Hoon and Lee, Kyogu},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2110.14513},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wang_vqmivc_2021,
	title = {{VQMIVC}: {Vector} {Quantization} and {Mutual} {Information}-{Based} {Unsupervised} {Speech} {Representation} {Disentanglement} for {One}-shot {Voice} {Conversion}},
	shorttitle = {{VQMIVC}},
	url = {http://arxiv.org/abs/2106.10132},
	abstract = {One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement. Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance. To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner. Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics. In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems. Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Wang, Disong and Deng, Liqun and Yeung, Yu Ting and Chen, Xiao and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2106.10132},
	keywords = {Computer Science - Computation and Language, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@inproceedings{ezzine_non-parallel_2022,
	title = {Non-{Parallel} {Voice} {Conversion} {System} {Using} {An} {Auto}-{Regressive} {Model}},
	doi = {10.1109/IC_ASET53395.2022.9765886},
	abstract = {Much existing voice conversion (VC) systems are attractive owing to their high performance in terms of voice quality and speaker similarity. Nevertheless, without parallel training data, some generated waveform trajectories are not yet smooth, leading to degraded sound quality and mispronunciation issues in the converted speech. To address these shortcomings, this paper proposes a non-parallel VC system based on an auto-regressive model, Phonetic PosteriorGrams (PPGs), and an LPCnet vocoder to generate high-quality converted speech. The proposed auto-regressive structure makes our system able to produce the next step outputs from the previous step acoustic features. Further, the use of PPGs aims to convert any unknown source speaker into a specific target speaker due to their speaker-independent properties. We evaluate the effectiveness of our system by performing any-to-one conversion pairs between native English speakers. Objective and subjective measures show that our method outperforms the best non-parallel VC method of Voice Conversion Challenge 2018 in terms of naturalness and speaker similarity.},
	booktitle = {2022 5th {International} {Conference} on {Advanced} {Systems} and {Emergent} {Technologies} ({IC}\_ASET)},
	author = {Ezzine, Kadria and Frikha, Mondher and Di Martino, Joseph},
	month = mar,
	year = {2022},
	keywords = {Acoustic measurements, Acoustics, LPCNet vocoder, Non-parallel voice conversion, Phonetic PosteriorGrams (PPGs), Phonetics, Task analysis, Training data, Trajectory, Vocoders, auto-regressive model},
	pages = {500--504},
}

@misc{du_disentanglement_2022,
	title = {Disentanglement of {Emotional} {Style} and {Speaker} {Identity} for {Expressive} {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2110.10326},
	abstract = {Expressive voice conversion performs identity conversion for emotional speakers by jointly converting speaker identity and emotional style. Due to the hierarchical structure of speech emotion, it is challenging to disentangle the emotional style for different speakers. Inspired by the recent success of speaker disentanglement with variational autoencoder (VAE), we propose an any-to-any expressive voice conversion framework, that is called StyleVC. StyleVC is designed to disentangle linguistic content, speaker identity, pitch, and emotional style information. We study the use of style encoder to model emotional style explicitly. At run-time, StyleVC converts both speaker identity and emotional style for arbitrary speakers. Experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Du, Zongyang and Sisman, Berrak and Zhou, Kun and Li, Haizhou},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2110.10326},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{huang_flowcpcvc_2022,
	title = {{FlowCPCVC}: {A} {Contrastive} {Predictive} {Coding} {Supervised} {Flow} {Framework} for {Any}-to-{Any} {Voice} {Conversion}},
	shorttitle = {{FlowCPCVC}},
	url = {https://www.isca-speech.org/archive/interspeech_2022/huang22c_interspeech.html},
	doi = {10.21437/Interspeech.2022-577},
	abstract = {Recently, the research of any-to-any voice conversion(VC) has been developed rapidly. However, they often suffer from unsatisfactory quality and require two stages for training, in which a spectrum generation process is indispensable. In this paper, we propose the FlowCPCVC system, which results in higher speech naturalness and timbre similarity. FlowCPCVC is the first one-stage training system for any-to-any task in our knowledge by taking advantage of VAE and contrastive learning. We employ a speaker encoder to extract timbre information, and a contrastive predictive coding(CPC) based content extractor to guide the flow module to discard the timbre and keeping the linguistic information. Our method directly incorporates the vocoder into the training, thus avoiding the loss of spectral information as in two-stage training. With a fancy method in training any-to-any task, we can also get robust results when using it in any-to-many conversion. Experiments show that FlowCPCVC achieves obvious improvement when compared to VQMIVC which is current state-of-the-art any-to-any voice conversion system. Our demo is available online 1.},
	language = {en},
	urldate = {2023-09-15},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Huang, Jiahong and Xu, Wen and Li, Yule and Liu, Junshi and Ma, Dongpeng and Xiang, Wei},
	month = sep,
	year = {2022},
	pages = {2558--2562},
}

@article{vekkot_emotional_2020,
	title = {Emotional {Voice} {Conversion} {Using} a {Hybrid} {Framework} {With} {Speaker}-{Adaptive} {DNN} and {Particle}-{Swarm}-{Optimized} {Neural} {Network}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2988781},
	abstract = {We propose a hybrid network-based learning framework for speaker-adaptive vocal emotion conversion, tested on three different datasets (languages), namely, EmoDB (German), IITKGP (Telugu), and SAVEE (English). The optimized learning model introduced is unique because of its ability to synthesize emotional speech with an acceptable perceptive quality while preserving speaker characteristics. The multilingual model is extremely beneficial in scenarios wherein emotional training data from a specific target speaker are sparsely available. The proposed model uses speaker-normalized mel-generalized cepstral coefficients for spectral training with data adaptation using the seed data from the target speaker. The fundamental frequency (F0) is transformed using a wavelet synchrosqueezed transform prior to mapping to obtain a sharpened time-frequency representation. Moreover, a feedforward artificial neural network, together with particle swarm optimization, was used for F0 training. Additionally, static-intensity modification was also performed for each test utterance. Using the framework, we were able to capture the spectral and pitch contour variabilities of emotional expression better than with other state-of-the-art methods used in this study. Considering the overall performance scores across datasets, an average melcepstral distortion (MCD) of 4.98 and root mean square error (RMSE-F0) of 10.67 were obtained in objective evaluations, and an average comparative mean opinion score (CMOS) of 3.57 and speaker similarity score of 3.70 were obtained for the proposed framework. Particularly, the best MCD of 4.09 (EmoDB-happiness) and RMSE-F0 of 9.00 (EmoDB-anger) were obtained, along with the maximum CMOS of 3.7 and speaker similarity of 4.6, thereby highlighting the effectiveness of the hybrid network model.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Vekkot, Susmitha and Gupta, Deepa and Zakariah, Mohammed and Alotaibi, Yousef Ajami},
	year = {2020},
	keywords = {ANN, Adaptation models, CMOS, DNN, Data models, Hidden Markov models, MCD, MGCEP, Neural networks, PSO, RMSE-F0, Training, Training data, Transforms, WSST, emotion, speaker similarity score, speaker-adaptation},
	pages = {74627--74647},
}

@inproceedings{xie_frame_2018,
	title = {Frame {Selection} in {SI}-{DNN} {Phonetic} {Space} with {WaveNet} {Vocoder} for {Voice} {Conversion} without {Parallel} {Training} {Data}},
	doi = {10.1109/ISCSLP.2018.8706660},
	abstract = {In this paper, we propose a frame selection approach to voice conversion with speaker independent deep neural network (SIDNN) and Kullback-Leibler divergence (KLD). The acoustic difference between source and target speaker is equalized with SI-DNN in the ASR senone phonetic space. KLD is used as an ideal distortion measure to select the corresponding target frame given the source frame. Acoustic trajectory of the selected frames is rendered with maximum probability trajectory generation algorithm. WaveNet based vocoder is applied on the converted acoustic trajectory to get the final speech waveform. From the subjective results we find that 1) the proposed method can achieve better performance than the phonetic cluster based selection method [16]; 2) by applying WaveNet vocoder the naturalness and speaker similarity can be significantly improved compared with linear predictive coding (LPC) based vocoder; 3) WaveNet vocoder trained only with spectral features i.e., line spectrum pairs (LSP) can better maintain the pitch pattern towards target speaker than WaveNet vocoder trained with both spectral features i.e., LSP and prosodic features (F0 and Unvoiced/Voiced flag).},
	booktitle = {2018 11th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Xie, Feng-Long and Soong, Frank K. and Wang, Xi and He, Lei and Haifeng, Li},
	month = nov,
	year = {2018},
	keywords = {Acoustics, Convolution, Hidden Markov models, Kullback-Leibler divergence, Phonetics, Training data, Trajectory, Vocoders, WaveNet vocoder, deep neural network, voice conversion},
	pages = {56--60},
}

@inproceedings{zhang_one-shot_2021,
	title = {One-{Shot} {Voice} {Conversion} {Based} on {Speaker} {Aware} {Module}},
	doi = {10.1109/ICASSP39728.2021.9414081},
	abstract = {Voice conversion (VC) is a task to convert the voice of speech while preserving its linguistic content. Although several methods have been proposed to enable VC with non-parallel data, it is still difficult to model the voice without a great number of data or an adaptive process. In this paper, we propose a speaker-aware voice conversion (SAVC) system realizing one-shot voice conversion without an adaptation stage. The SAVC utilizes a speaker aware module (SAM) to disentangle speaker embeddings. The SAM comprises a dynamic reference encoder, a static speaker knowledge block (SKB), and a multi-head attention layer. The reference encoder is used to compress a variable-length utterance to a fixed-length vector, the SKB is made up of pre-extraction x-vectors, and the multi-head attention layer is designed to generate weighted combined speaker embeddings. Subsequently, phonetic pos- teriorgrams (PPGs) as context encoding are concatenated with speaker embeddings and sent to the decoder module for generating acoustic features. Experimental results on the Aishell-1 corpus show that the proposed method can improve speaker similarity and converted utterances' speech quality.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Ying and Che, Hao and Li, Jie and Li, Chenxing and Wang, Xiaorui and Wang, Zhongyuan},
	month = jun,
	year = {2021},
	keywords = {Acoustics, Conferences, Data models, Databases, Encoding, Phonetics, Signal processing, one-shot, phonetic posteriorgrams, speaker aware voice conversion, x-vector},
	pages = {5959--5963},
}

@misc{lin_s2vc_2021,
	title = {{S2VC}: {A} {Framework} for {Any}-to-{Any} {Voice} {Conversion} with {Self}-{Supervised} {Pretrained} {Representations}},
	shorttitle = {{S2VC}},
	url = {http://arxiv.org/abs/2104.02901},
	abstract = {Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training. Various any-to-any VC approaches have been proposed like AUTOVC, AdaINVC, and FragmentVC. AUTOVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features. FragmentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content. Moreover, pre-trained features are adopted. AUTOVC used dvector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information. Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for VC model. Supervised phoneme posteriororgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features. The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Lin, Jheng-hao and Lin, Yist Y. and Chien, Chung-Ming and Lee, Hung-yi},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2104.02901},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{yang_speech_2022,
	title = {Speech {Representation} {Disentanglement} with {Adversarial} {Mutual} {Information} {Learning} for {One}-shot {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2208.08757},
	abstract = {One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Yang, SiCheng and Tantrawenith, Methawee and Zhuang, Haolin and Wu, Zhiyong and Sun, Aolan and Wang, Jianzong and Cheng, Ning and Tang, Huaizhen and Zhao, Xintao and Wang, Jie and Meng, Helen},
	month = aug,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{xie_end--end_2022,
	title = {End-to-{End} {Voice} {Conversion} with {Information} {Perturbation}},
	url = {http://arxiv.org/abs/2206.07569},
	abstract = {The ideal goal of voice conversion is to convert the source speaker's speech to sound naturally like the target speaker while maintaining the linguistic content and the prosody of the source speech. However, current approaches are insufficient to achieve comprehensive source prosody transfer and target speaker timbre preservation in the converted speech, and the quality of the converted speech is also unsatisfied due to the mismatch between the acoustic model and the vocoder. In this paper, we leverage the recent advances in information perturbation and propose a fully end-to-end approach to conduct high-quality voice conversion. We first adopt information perturbation to remove speaker-related information in the source speech to disentangle speaker timbre and linguistic content and thus the linguistic information is subsequently modeled by a content encoder. To better transfer the prosody of the source speech to the target, we particularly introduce a speaker-related pitch encoder which can maintain the general pitch pattern of the source speaker while flexibly modifying the pitch intensity of the generated speech. Finally, one-shot voice conversion is set up through continuous speaker space modeling. Experimental results indicate that the proposed end-to-end approach significantly outperforms the state-of-the-art models in terms of intelligibility, naturalness, and speaker similarity.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Xie, Qicong and Yang, Shan and Lei, Yi and Xie, Lei and Su, Dan},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.07569},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wang_zero-shot_2022,
	title = {Zero-shot {Voice} {Conversion} via {Self}-supervised {Prosody} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2110.14422},
	abstract = {Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an attractive research topic as it enables a range of applications like voice customizing, animation production, and others. Recent work in this area made progress with disentanglement methods that separate utterance content and speaker characteristics from speech audio recordings. However, many of these methods are subject to the leakage of prosody (e.g., pitch, volume), causing the speaker voice in the synthesized speech to be different from the desired target speakers. To prevent this issue, we propose a novel self-supervised approach that effectively learns disentangled pitch and volume representations that can represent the prosody styles of different speakers. We then use the learned prosodic representations as conditional information to train and enhance our VC model for zero-shot conversion. In our experiments, we show that our prosody representations are disentangled and rich in prosody information. Moreover, we demonstrate that the addition of our prosody representations improves our VC performance and surpasses state-of-the-art zero-shot VC performances.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Wang, Shijun and Kostadinov, Dimche and Borth, Damian},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2110.14422},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{hwang_stylevc_2022,
	title = {{StyleVC}: {Non}-{Parallel} {Voice} {Conversion} with {Adversarial} {Style} {Generalization}},
	shorttitle = {{StyleVC}},
	doi = {10.1109/ICPR56361.2022.9956613},
	abstract = {Voice conversion converts the voice while maintaining the language information. It uses two samples to synthesize speech: the source sample is used for content, the target sample is used for style representation. Therefore, VC has been progressed to design information flow to disentangle content and style in a speech. However, separated representations are damaged while passing sparse subspace. Besides, VC models suffer from the training-inference mismatch problem: they only use one sample in training. Accordingly, the model extracts inappropriate content and style representation and generates low-quality speech during inference. To address the mismatch scenario problem, we propose a StyleVC, which utilizes adversarial style generalization. First, we propose style generalization, which captures global style representation and restricts the model from copying information. Second, we use a pitch predictor to estimate pitch information according to content and style representation. Third, we further use adversarial training to make the model generate more realistic speech. Finally, we demonstrate our proposed model can generate high-quality speech. The experimental results also show that the proposed StyleVC significantly outperforms to extract the desired features and improve audio quality during inference.},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Hwang, In-Sun and Lee, Sang-Hoon and Lee, Seong-Whan},
	month = aug,
	year = {2022},
	keywords = {Decoding, Feature extraction, Pattern recognition, Training},
	pages = {23--30},
}

@misc{zhao_disentangleing_2022,
	title = {Disentangleing {Content} and {Fine}-grained {Prosody} {Information} via {Hybrid} {ASR} {Bottleneck} {Features} for {Voice} {Conversion}},
	url = {http://arxiv.org/abs/2203.12813},
	abstract = {Non-parallel data voice conversion (VC) have achieved considerable breakthroughs recently through introducing bottleneck features (BNFs) extracted by the automatic speech recognition(ASR) model. However, selection of BNFs have a significant impact on VC result. For example, when extracting BNFs from ASR trained with Cross Entropy loss (CE-BNFs) and feeding into neural network to train a VC system, the timbre similarity of converted speech is significantly degraded. If BNFs are extracted from ASR trained using Connectionist Temporal Classification loss (CTC-BNFs), the naturalness of the converted speech may decrease. This phenomenon is caused by the difference of information contained in BNFs. In this paper, we proposed an any-to-one VC method using hybrid bottleneck features extracted from CTC-BNFs and CE-BNFs to complement each other advantages. Gradient reversal layer and instance normalization were used to extract prosody information from CE-BNFs and content information from CTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate high-quality waveform. Experimental results show that our proposed method achieves higher similarity, naturalness, quality than baseline method and reveals the differences between the information contained in CE-BNFs and CTC-BNFs as well as the influence they have on the converted speech.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Zhao, Xintao and Liu, Feng and Song, Changhe and Wu, Zhiyong and Kang, Shiyin and Tuo, Deyi and Meng, Helen},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.12813},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{meftah_english_2023,
	title = {English {Emotional} {Voice} {Conversion} {Using} {StarGAN} {Model}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3292003},
	abstract = {The StarGANv2-VC model is a many-to-many non-parallel generative adversarial network (GAN) voice conversion (VC) model that has proven effective in style conversion tasks. This study aimed to investigate the scalability and diversity of the model for English emotional voice conversion (EVC) across different speakers and emotions. We carried out many experiments using an Emotional Speech Database (ESD), comprising a single speaker-multi-emotion experiment, a multi-speakers-multi-emotions experiment (gender-dependent), and a multi-speakers-multi-emotions experiment (gender-independent). We also assessed the effect of training set size and compared the performance of the StarGANv2-VC model with a CycleGAN model. Our study found that the StarGANv2-VC model accurately converted the pitch of the voice across all four emotions (neutral, happy, sad, and angry). However, the model’s efficiency in converting multi-emotions to multi-speakers was not as high as its efficiency in voice conversion for multi-speakers. Further research is needed in this area. We objectively assessed the quality of the converted speech using Mel-frequency cepstral distortion (MCD) and root-mean-square error (RMSE) for spectrum and prosody, respectively. Additionally, we conducted cross-emotion recognition using a convolutional recurrent neural network (CRNN).},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Meftah, Ali Hamid and Alashban, Adal A. and Alotaibi, Yousef A. and Selouani, Sid Ahmed},
	year = {2023},
	keywords = {Databases, ESD, Electrostatic discharges, Emotion recognition, Emotional voice conversion, English, GANs, Generative adversarial networks, Generators, Speech recognition, StarGAN, Virtual assistants, emotion},
	pages = {67835--67849},
}

@inproceedings{xie_improved_2020,
	title = {An {Improved} {Frame}-{Unit}-{Selection} {Based} {Voice} {Conversion} {System} {Without} {Parallel} {Training} {Data}},
	doi = {10.1109/ICASSP40776.2020.9054010},
	abstract = {A frame-unit-selection based voice conversion system proposed earlier by us is revisited here to enhance its performance in both speech naturalness and speaker similarity. Speaker independent, bilingual (Mandarin Chinese and American English) deep neural net (DNN) acoustic model’s output, frame-level phone posterior probability (PPP), is used to represent the phonetic information. The corresponding frame-level F0 is used as the prosodic information. Kullback-Leibler divergence (KLD) between source and target PPPs (phonetic distortion) and the absolute difference between normalized source and target F0 (prosodic distortion) are used for selecting target frame candidates to construct a search lattice. The optimal target unit trajectory is obtained by Viterbi algorithm which tries to minimize the dynamic acoustic difference between the acoustic trajectory of the source speech and target candidates. The obtained spectral trajectory together with the enhanced pitch period and pitch correlation trajectory are sent to LPCNet vocoder to synthesize the converted waveforms. Compared with the top-rank system in Voice Conversion Challenge 2018, our new system can achieve on-par performance on studio to studio American English VC test, and better performance on non-studio to studio Mandarin Chinese VC test, in both speech naturalness MOS and speaker similarity DMOS.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Xie, Feng-Long and Li, Xin-Hui and Liu, Bo and Zheng, Yi-Bin and Meng, Li and Lu, Li and Soong, Frank K.},
	month = may,
	year = {2020},
	keywords = {Acoustic distortion, Acoustics, KL divergence, LPCNet, Phonetics, Training data, Trajectory, Viterbi algorithm, Vocoders, absolute F0 difference, dynamic acoustic difference, unit selection},
	pages = {7754--7758},
}

@inproceedings{muradeli_differentiable_2022,
	title = {Differentiable {Time}–frequency {Scattering} on {GPU}},
	url = {https://www.dafx.de/paper-archive/details.php?id=P6aCKB5-DTz8bmYUcjsXkA},
	abstract = {Muradeli, J.; Vahidi, C.; Wang, C.; Han, H.; Lostanlen, V.; Lagrange, M.; Fazekas, G.: Differentiable Time–frequency Scattering on GPU, 2022},
	urldate = {2023-08-27},
	author = {Muradeli, John and Vahidi, Cyrus and Wang, Changhong and Han, Han and Lostanlen, Vincent and Lagrange, Mathieu and Fazekas, George},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@incollection{huzaifah2021aimusic,
	title = {Deep generative models for musical audio synthesis},
	booktitle = {Handbook of artificial intelligence for music},
	publisher = {Springer},
	author = {Huzaifah, Muhammad and Wyse, Lonce},
	year = {2021},
	pages = {639--678},
}

@inproceedings{hershey_deep_2016,
	title = {Deep clustering: {Discriminative} embeddings for segmentation and separation},
	shorttitle = {Deep clustering},
	doi = {10.1109/ICASSP.2016.7471631},
	abstract = {We address the problem of "cocktail-party" source separation in a deep learning framework called deep clustering. Previous deep network approaches to separation have shown promising performance in scenarios with a fixed number of sources, each belonging to a distinct signal class, such as speech and noise. However, for arbitrary source classes and number, "class-based" methods are not suitable. Instead, we train a deep network to assign contrastive embedding vectors to each time-frequency region of the spectrogram in order to implicitly predict the segmentation labels of the target spectrogram from the input mixtures. This yields a deep network-based analogue to spectral clustering, in that the embeddings form a low-rank pair-wise affinity matrix that approximates the ideal affinity matrix, while enabling much faster performance. At test time, the clustering step "decodes" the segmentation implicit in the embeddings by optimizing K-means with respect to the unknown assignments. Preliminary experiments on single-channel mixtures from multiple speakers show that a speaker-independent model trained on two-speaker mixtures can improve signal quality for mixtures of held-out speakers by an average of 6dB. More dramatically, the same model does surprisingly well with three-speaker mixtures.},
	booktitle = {2016 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Hershey, John R. and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
	month = mar,
	year = {2016},
	note = {ISSN: 2379-190X},
	keywords = {Indexes, Machine learning, Neural networks, Spectrogram, Speech, Time-frequency analysis, Training, clustering, deep learning, embedding, speech separation},
	pages = {31--35},
}

@inproceedings{engel_ddsp_2020,
	address = {Addis Ababa, Ethiopia},
	title = {{DDSP}: {Differentiable} {Digital} {Signal} {Processing}},
	shorttitle = {{DDSP}},
	abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not...},
	urldate = {2020-01-29},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}},
	author = {Engel, Jesse and Hantrakul, Lamtharn (Hanoi) and Gu, Chenjie and Roberts, Adam},
	month = apr,
	year = {2020},
}

@inproceedings{kim_crepe_2018,
	title = {Crepe: {A} {Convolutional} {Representation} for {Pitch} {Estimation}},
	shorttitle = {Crepe},
	url = {https://nyuscholars.nyu.edu/en/publications/crepe-a-convolutional-representation-for-pitch-estimation},
	doi = {10.1109/ICASSP.2018.8461329},
	urldate = {2020-04-23},
	booktitle = {2018 {IEEE} international conference on acoustics, speech, and signal processing, {ICASSP} 2018 - proceedings},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
	month = sep,
	year = {2018},
	pages = {161--165},
}

@article{li_creating_2019,
	title = {Creating a multitrack classical music performance dataset for multimodal music analysis: {Challenges}, insights, and applications},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	shorttitle = {Creating a multitrack classical music performance dataset for multimodal music analysis},
	url = {https://ieeexplore.ieee.org/document/8411155/},
	doi = {10.1109/TMM.2018.2856090},
	number = {2},
	urldate = {2021-04-07},
	journal = {IEEE Transactions on Multimedia},
	author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
	month = feb,
	year = {2019},
	pages = {522--535},
}

@inproceedings{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2019-10-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, unread, ⛔ No DOI found},
}

@article{deutsch_absolute_2013,
	title = {Absolute pitch among students at the {Shanghai} {Conservatory} of {Music}: {A} large-scale direct-test study},
	volume = {134},
	issn = {0001-4966},
	shorttitle = {Absolute pitch among students at the {Shanghai} {Conservatory} of {Music}},
	url = {http://asa.scitation.org/doi/10.1121/1.4824450},
	doi = {10.1121/1.4824450},
	number = {5},
	urldate = {2019-10-08},
	journal = {The Journal of the Acoustical Society of America},
	author = {Deutsch, Diana and Li, Xiaonuo and Shen, Jing},
	month = nov,
	year = {2013},
	pages = {3853--3859},
}

@article{theusch_absolute_2011,
	title = {Absolute {Pitch} {Twin} {Study} and {Segregation} {Analysis}},
	volume = {14},
	issn = {1832-4274, 1839-2628},
	url = {https://www.cambridge.org/core/product/identifier/S1832427400011312/type/journal%5Farticle},
	doi = {10.1375/twin.14.2.173},
	abstract = {Absolute pitch is a rare pitch-naming ability with unknown etiology. Some scientists maintain that its manifestation depends solely on environmental factors, while others suggest that genetic factors contribute to it. We sought to further investigate the hypothesis that genetic factors support the acquisition of absolute pitch and to better elucidate the inheritance pattern of this trait. To this end, we conducted a twin study and a segregation analysis using data collected from a large population of absolute pitch possessors. The casewise concordance rate of 14 monozygotic twin pairs, 78.6\%, was significantly different from that of 31 dizygotic twin pairs, 45.2\%, assuming single ascertainment ( x 2 = 5.57, 1 df, p = .018), supporting a role for genetics in the development of absolute pitch. Segregation analysis of 1463 families, assuming single ascertainment, produced a segregation ratio p D = .089 with SE p D = 0.006. Unlike an earlier segregation analysis on a small number of absolute pitch probands from musically educated families, our study indicates that absolute pitch is not inherited in a simple Mendelian fashion. Based on these data, absolute pitch is likely genetically heterogeneous, with environmental, epigenetic, and stochastic factors also perhaps contributing to its genesis. These findings are in agreement with the results of our recent linkage analysis.},
	number = {2},
	urldate = {2019-10-08},
	journal = {Twin Research and Human Genetics},
	author = {Theusch, Elizabeth and Gitschier, Jane},
	month = apr,
	year = {2011},
	pages = {173--178},
}

@article{liu_intonation_2010,
	title = {Intonation processing in congenital amusia: discrimination, identification and imitation},
	volume = {133},
	issn = {1460-2156, 0006-8950},
	shorttitle = {Intonation processing in congenital amusia},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awq089},
	doi = {10.1093/brain/awq089},
	number = {6},
	urldate = {2019-10-08},
	journal = {Brain : a journal of neurology},
	author = {Liu, Fang and Patel, Aniruddh D. and Fourcin, Adrian and Stewart, Lauren},
	month = jun,
	year = {2010},
	pages = {1682--1693},
}

@article{mcdermott_is_2008,
	title = {Is {Relative} {Pitch} {Specific} to {Pitch}?},
	volume = {19},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.2008.02235.x},
	doi = {10.1111/j.1467-9280.2008.02235.x},
	abstract = {Melodies, speech, and other stimuli that vary in pitch are processed largely in terms of the relative pitch differences between sounds. Relative representations permit recognition of pitch patterns despite variations in overall pitch level between instruments or speakers. A key component of relative pitch is the sequence of pitch increases and decreases from note to note, known as the melodic contour. Here we report that contour representations are also produced by patterns in loudness and brightness (an aspect of timbre). The representations of contours in different dimensions evidently have much in common, as contours in one dimension can be readily recognized in other dimensions. Moreover, contours in loudness and brightness are nearly as useful as pitch contours for recognizing familiar melodies that are normally conveyed via pitch. Our results indicate that relative representations via contour extraction are a general feature of the auditory system, and may have a common central locus.},
	number = {12},
	urldate = {2019-10-08},
	journal = {Psychological Science},
	author = {McDermott, Josh H. and Lehr, Andriana J. and Oxenham, Andrew J.},
	month = dec,
	year = {2008},
	pages = {1263--1271},
}

@article{schellenberg_is_2008,
	title = {Is {There} an {Asian} {Advantage} for {Pitch} {Memory}?},
	volume = {25},
	issn = {07307829, 15338312},
	url = {http://mp.ucpress.edu/cgi/doi/10.1525/mp.2008.25.3.241},
	doi = {10.1525/mp.2008.25.3.241},
	number = {3},
	urldate = {2019-10-08},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Schellenberg, E. Glenn and Trehub, Sandra E.},
	month = feb,
	year = {2008},
	pages = {241--252},
}

@article{krumhansl_key_1982,
	title = {Key distance effects on perceived harmonic structure in music},
	volume = {32},
	issn = {0031-5117, 1532-5962},
	url = {http://www.springerlink.com/index/10.3758/BF03204269},
	doi = {10.3758/BF03204269},
	number = {2},
	urldate = {2019-10-08},
	journal = {Perception \& Psychophysics},
	author = {Krumhansl, Carol L. and Bharucha, Jamshed and Castellano, Mary A.},
	month = mar,
	year = {1982},
	pages = {96--108},
}

@article{plantinga_memory_2005,
	title = {Memory for melody: infants use a relative pitch code},
	volume = {98},
	issn = {00100277},
	shorttitle = {Memory for melody},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027704001878},
	doi = {10.1016/j.cognition.2004.09.008},
	abstract = {Pitch perception is fundamental to melody in music and prosody in speech. Unlike many animals, the vast majority of human adults store melodic information primarily in terms of relative not absolute pitch, and readily recognize a melody whether rendered in a high or a low pitch range. We show that at 6 months infants are also primarily relative pitch processors. Infants familiarized with a melody for 7 days preferred, on the eighth day, to listen to a novel melody in comparison to the familiarized one, regardless of whether the melodies at test were presented at the same pitch as during familiarization or transposed up or down by a perfect ﬁfth (7/12th of an octave) or a tritone (1/2 octave). On the other hand, infants showed no preference for a transposed over original-pitch version of the familiarized melody, indicating that either they did not remember the absolute pitch, or it was not as salient to them as the relative pitch.},
	number = {1},
	urldate = {2019-10-08},
	journal = {Cognition},
	author = {Plantinga, Judy and Trainor, Laurel J.},
	month = nov,
	year = {2005},
	pages = {1--11},
}

@article{miyazaki_musical_1988,
	title = {Musical pitch identification by absolute pitch possessors},
	volume = {44},
	issn = {0031-5117, 1532-5962},
	url = {http://www.springerlink.com/index/10.3758/BF03207484},
	doi = {10.3758/BF03207484},
	number = {6},
	urldate = {2019-10-08},
	journal = {Perception \& Psychophysics},
	author = {Miyazaki, Ken’ichi},
	month = nov,
	year = {1988},
	pages = {501--512},
}

@article{gjerdingen_cognitive_1992,
	title = {Cognitive {Foundations} of {Musical} {Pitch} {Carol} {L}. {Krumhansl}},
	volume = {9},
	issn = {07307829, 15338312},
	url = {http://mp.ucpress.edu/cgi/doi/10.2307/40285567},
	doi = {10.2307/40285567},
	number = {4},
	urldate = {2019-10-08},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Gjerdingen, Robert O.},
	month = jul,
	year = {1992},
	pages = {476--492},
}

@article{baharloo_familial_2000,
	title = {Familial {Aggregation} of {Absolute} {Pitch}},
	volume = {67},
	issn = {00029297},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002929707632625},
	doi = {10.1086/303057},
	number = {3},
	urldate = {2019-10-08},
	journal = {The American Journal of Human Genetics},
	author = {Baharloo, Siamak and Service, Susan K. and Risch, Neil and Gitschier, Jane and Freimer, Nelson B.},
	month = sep,
	year = {2000},
	pages = {755--758},
}

@article{johnsrude_functional_2000,
	title = {Functional specificity in the right human auditory cortex for perceiving pitch direction},
	volume = {123},
	issn = {1460-2156, 0006-8950},
	url = {https://academic.oup.com/brain/article/123/1/155/269199},
	doi = {10.1093/brain/123.1.155},
	number = {1},
	urldate = {2019-10-08},
	journal = {Brain : a journal of neurology},
	author = {Johnsrude, Ingrid S. and Penhune, Virginia B. and Zatorre, Robert J.},
	month = jan,
	year = {2000},
	pages = {155--163},
}

@article{schellenberg_good_2003,
	title = {Good {Pitch} {Memory} {Is} {Widespread}},
	volume = {14},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/1467-9280.03432},
	doi = {10.1111/1467-9280.03432},
	abstract = {Here we show that good pitch memory is widespread among adults with no musical training. We tested unselected college students on their memory for the pitch level of instrumental soundtracks from familiar television programs. Participants heard 5-s excerpts either at the original pitch level or shifted upward or downward by 1 or 2 semitones. They successfully identiﬁed the original pitch levels. Other participants who heard comparable excerpts from unfamiliar recordings could not do so. These ﬁndings reveal that ordinary listeners retain ﬁne-grained information about pitch level over extended periods. Adults’ reportedly poor memory for pitch is likely to be a by-product of their inability to name isolated pitches.},
	number = {3},
	urldate = {2019-10-08},
	journal = {Psychological Science},
	author = {Schellenberg, E. Glenn and Trehub, Sandra E.},
	month = may,
	year = {2003},
	pages = {262--266},
}

@article{trainor_long-term_2004,
	title = {Long-term memory for music: infants remember tempo and timbre},
	volume = {7},
	issn = {1363-755X, 1467-7687},
	shorttitle = {Long-term memory for music},
	url = {http://doi.wiley.com/10.1111/j.1467-7687.2004.00348.x},
	doi = {10.1111/j.1467-7687.2004.00348.x},
	abstract = {We show that infants’ long-term memory representations for melodies are not just reduced to the structural features of relative pitches and durations, but contain surface or performance tempo- and timbre-speciﬁc information. Using a head turn preference procedure, we found that after a one week exposure to an old English folk song, infants preferred to listen to a novel folk song, indicating that they remembered the familiarized melody. However, if the tempo (25\% faster or slower) or instrument timbre (harp vs. piano) of the familiarized melody was changed at test, infants showed no preference, indicating that they remembered the speciﬁc tempo and timbre of the melodies. The results are consistent with an exemplar-based model of memory in infancy rather than one in which structural features are extracted and performance features forgotten.},
	number = {3},
	urldate = {2019-10-14},
	journal = {Developmental Science},
	author = {Trainor, Laurel J. and Wu, Luann and Tsang, Christine D.},
	month = jun,
	year = {2004},
	pages = {289--296},
}

@book{s3_american_1973,
	title = {American {National} {Standard} {Psychoacoustical} {Terminology}},
	publisher = {American National Standards Institute},
	author = {S3, American National Standards Institute Committee on Bioacoustics and Institute, American National Standards and America, Acoustical Society of},
	year = {1973},
}

@article{honing_without_2015,
	title = {Without it no music: cognition, biology and evolution of musicality},
	volume = {370},
	issn = {0962-8436},
	shorttitle = {Without it no music},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4321129/},
	doi = {10.1098/rstb.2014.0088},
	abstract = {Musicality can be defined as a natural, spontaneously developing trait based on and constrained by biology and cognition. Music, by contrast, can be defined as a social and cultural construct based on that very musicality. One critical challenge is to delineate the constituent elements of musicality. What biological and cognitive mechanisms are essential for perceiving, appreciating and making music? Progress in understanding the evolution of music cognition depends upon adequate characterization of the constituent mechanisms of musicality and the extent to which they are present in non-human species. We argue for the importance of identifying these mechanisms and delineating their functions and developmental course, as well as suggesting effective means of studying them in human and non-human animals. It is virtually impossible to underpin the evolutionary role of musicality as a whole, but a multicomponent perspective on musicality that emphasizes its constituent capacities, development and neural cognitive specificity is an excellent starting point for a research programme aimed at illuminating the origins and evolution of musical behaviour as an autonomous trait.},
	number = {1664},
	urldate = {2019-12-22},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Honing, Henkjan and ten Cate, Carel and Peretz, Isabelle and Trehub, Sandra E.},
	month = mar,
	year = {2015},
	pmid = {25646511},
	note = {tex.pmcid: PMC4321129},
}

@article{krumhansl_perceived_1982,
	title = {Perceived harmonic structure of chords in three related musical keys.},
	volume = {8},
	issn = {1939-1277, 0096-1523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.8.1.24},
	doi = {10.1037/0096-1523.8.1.24},
	number = {1},
	urldate = {2019-10-08},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Krumhansl, Carol L. and Bharucha, Jamshed J. and Kessler, Edward J.},
	year = {1982},
	pages = {24--36},
}

@article{warren_separating_2003,
	title = {Separating pitch chroma and pitch height in the human brain},
	volume = {100},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1730682100},
	doi = {10.1073/pnas.1730682100},
	number = {17},
	urldate = {2019-10-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Warren, J. D. and Uppenkamp, S. and Patterson, R. D. and Griffiths, T. D.},
	month = aug,
	year = {2003},
	pages = {10038--10042},
}

@article{rusconi_spatial_2006,
	title = {Spatial representation of pitch height: the {SMARC} effect},
	volume = {99},
	issn = {00100277},
	shorttitle = {Spatial representation of pitch height},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027705000260},
	doi = {10.1016/j.cognition.2005.01.004},
	abstract = {Through the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The ﬁnding that our cognitive system maps pitch height onto an internal representation of space, which in turn affects motor performance even when this perceptual attribute is irrelevant to the task, extends previous studies on auditory perception and suggests an interesting analogy between music perception and mathematical cognition. Both the basic elements of mathematical cognition (i.e. numbers) and the basic elements of musical cognition (i.e. pitches), appear to be mapped onto a mental spatial representation in a way that affects motor performance.},
	number = {2},
	urldate = {2019-10-08},
	journal = {Cognition},
	author = {Rusconi, E and Kwan, B and Giordano, B and Umilta, C and Butterworth, B},
	month = mar,
	year = {2006},
	pages = {113--129},
}

@article{edworthy_interval_1985,
	title = {Interval and {Contour} in {Melody} {Processing}},
	volume = {2},
	issn = {07307829, 15338312},
	url = {http://mp.ucpress.edu/cgi/doi/10.2307/40285305},
	doi = {10.2307/40285305},
	number = {3},
	urldate = {2019-10-08},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Edworthy, Judy},
	month = apr,
	year = {1985},
	keywords = {read},
	pages = {375--388},
}

@article{gjerdingen_scanning_2008,
	title = {Scanning the {Dial}: {The} {Rapid} {Recognition} of {Music} {Genres}},
	volume = {37},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Scanning the {Dial}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09298210802479268},
	doi = {10.1080/09298210802479268},
	abstract = {Given brief excerpts of commercially recorded music in one of ten broad genres of music, participants in this study were asked to evaluate each excerpt and to assign it to one of the ten genre labels. It was expected that participants would be good at this task, since they were the very consumers for whom much of this music had been created. But the speed at which participants could perform this task, including above-chance categorizations of excerpts as short as 1/4 second, was quite unexpected.},
	number = {2},
	urldate = {2019-10-14},
	journal = {Journal of New Music Research},
	author = {Gjerdingen, Robert O. and Perrott, David},
	month = jun,
	year = {2008},
	pages = {93--100},
}

@article{schellenberg_name_1999,
	title = {Name that tune: {Identifying} popular recordings from brief excerpts},
	volume = {6},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Name that tune},
	url = {http://www.springerlink.com/index/10.3758/BF03212973},
	doi = {10.3758/BF03212973},
	number = {4},
	urldate = {2019-10-14},
	journal = {Psychonomic Bulletin \& Review},
	author = {Schellenberg, E. Glenn and Iverson, Paul and Mckinnon, Margaret C.},
	month = dec,
	year = {1999},
	pages = {641--646},
}

@article{honing_rhesus_2012,
	title = {Rhesus monkeys ({Macaca} mulatta) detect rhythmic groups in music, but not the beat},
	volume = {7},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0051369},
	abstract = {It was recently shown that rhythmic entrainment, long considered a human-specific mechanism, can be demonstrated in a selected group of bird species, and, somewhat surprisingly, not in more closely related species such as nonhuman primates. This observation supports the vocal learning hypothesis that suggests rhythmic entrainment to be a by-product of the vocal learning mechanisms that are shared by several bird and mammal species, including humans, but that are only weakly developed, or missing entirely, in nonhuman primates. To test this hypothesis we measured auditory event-related potentials (ERPs) in two rhesus monkeys (Macaca mulatta), probing a well-documented component in humans, the mismatch negativity (MMN) to study rhythmic expectation. We demonstrate for the first time in rhesus monkeys that, in response to infrequent deviants in pitch that were presented in a continuous sound stream using an oddball paradigm, a comparable ERP component can be detected with negative deflections in early latencies (Experiment 1). Subsequently we tested whether rhesus monkeys can detect gaps (omissions at random positions in the sound stream; Experiment 2) and, using more complex stimuli, also the beat (omissions at the first position of a musical unit, i.e. the 'downbeat'; Experiment 3). In contrast to what has been shown in human adults and newborns (using identical stimuli and experimental paradigm), the results suggest that rhesus monkeys are not able to detect the beat in music. These findings are in support of the hypothesis that beat induction (the cognitive mechanism that supports the perception of a regular pulse from a varying rhythm) is species-specific and absent in nonhuman primates. In addition, the findings support the auditory timing dissociation hypothesis, with rhesus monkeys being sensitive to rhythmic grouping (detecting the start of a rhythmic group), but not to the induced beat (detecting a regularity from a varying rhythm).},
	number = {12},
	journal = {PloS One},
	author = {Honing, Henkjan and Merchant, Hugo and Háden, Gábor P. and Prado, Luis and Bartolo, Ramón},
	year = {2012},
	pmid = {23251509},
	note = {tex.pmcid: PMC3520841},
	keywords = {Animals, Evoked Potentials, Macaca mulatta, Music},
	pages = {e51369},
}

@article{schellenberg_music_nodate,
	title = {Music and {Cognitive} {Abilities}},
	volume = {14},
	abstract = {Does music make you smarter? Music listening and music lessons have been claimed to confer intellectual advantages. Any association between music and intellectual functioning would be notable only if the beneﬁts apply reliably to nonmusical abilities and if music is unique in producing the effects. The available evidence indicates that music listening leads to enhanced performance on a variety of cognitive tests, but that such effects are shortterm and stem from the impact of music on arousal level and mood, which, in turn, affect cognitive performance; experiences other than music listening have similar effects. Music lessons in childhood tell a different story. They are associated with small but general and long-lasting intellectual beneﬁts that cannot be attributed to obvious confounding variables such as family income and parents’ education. The mechanisms underlying this association have yet to be determined.},
	number = {6},
	author = {Schellenberg, E Glenn},
	pages = {5},
}

@article{hepper_examination_1991,
	title = {An {Examination} of {Fetal} {Learning} {Before} and {After} {Birth}},
	volume = {12},
	issn = {0303-3910, 2158-0812},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03033910.1991.10557830},
	doi = {10.1080/03033910.1991.10557830},
	number = {2},
	urldate = {2019-12-29},
	journal = {The Irish Journal of Psychology},
	author = {Hepper, Peter G.},
	month = jan,
	year = {1991},
	pages = {95--107},
}

@article{thompson_arousal_2001,
	title = {Arousal, {Mood}, and {The} {Mozart} {Effect}},
	volume = {12},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/1467-9280.00345},
	doi = {10.1111/1467-9280.00345},
	abstract = {The “Mozart effect” refers to claims that people perform better on tests of spatial abilities after listening to music composed by Mozart. We examined whether the Mozart effect is a consequence of between-condition differences in arousal and mood. Participants completed a test of spatial abilities after listening to music or sitting in silence. The music was a Mozart sonata (a pleasant and energetic piece) for some participants and an Albinoni adagio (a slow, sad piece) for others. We also measured enjoyment, arousal, and mood. Performance on the spatial task was better following the music than the silence condition, but only for participants who heard Mozart. The two music selections also induced differential responding on the enjoyment, arousal, and mood measures. Moreover, when such differences were held constant by statistical means, the Mozart effect disappeared. These ﬁndings provide compelling evidence that the Mozart effect is an artifact of arousal and mood.},
	number = {3},
	urldate = {2019-12-29},
	journal = {Psychological Science},
	author = {Thompson, William Forde and Schellenberg, E. Glenn and Husain, Gabriela},
	month = may,
	year = {2001},
	pages = {248--251},
}

@article{winkler_newborn_2009,
	title = {Newborn infants detect the beat in music},
	volume = {106},
	issn = {1091-6490},
	doi = {10.1073/pnas.0809035106},
	abstract = {To shed light on how humans can learn to understand music, we need to discover what the perceptual capabilities with which infants are born. Beat induction, the detection of a regular pulse in an auditory signal, is considered a fundamental human trait that, arguably, played a decisive role in the origin of music. Theorists are divided on the issue whether this ability is innate or learned. We show that newborn infants develop expectation for the onset of rhythmic cycles (the downbeat), even when it is not marked by stress or other distinguishing spectral features. Omitting the downbeat elicits brain activity associated with violating sensory expectations. Thus, our results strongly support the view that beat perception is innate.},
	number = {7},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Winkler, István and Háden, Gábor P. and Ladinig, Olivia and Sziller, István and Honing, Henkjan},
	month = feb,
	year = {2009},
	pmid = {19171894},
	note = {tex.pmcid: PMC2631079},
	keywords = {Acoustic Stimulation, Adolescent, Adult, Auditory Perception, Brain, Electroencephalography, Electrophysiology, Evoked Potentials, Female, Humans, Infant, Male, Music, Newborn, Periodicity, Time Factors, Time Perception},
	pages = {2468--2471},
}

@article{patel_experimental_2009,
	title = {Experimental evidence for synchronization to a musical beat in a nonhuman animal},
	volume = {19},
	issn = {1879-0445},
	doi = {10.1016/j.cub.2009.03.038},
	abstract = {The tendency to move in rhythmic synchrony with a musical beat (e.g., via head bobbing, foot tapping, or dance) is a human universal [1] yet is not commonly observed in other species [2]. Does this ability reflect a brain specialization for music cognition, or does it build on neural circuitry that ordinarily serves other functions? According to the "vocal learning and rhythmic synchronization" hypothesis [3], entrainment to a musical beat relies on the neural circuitry for complex vocal learning, an ability that requires a tight link between auditory and motor circuits in the brain [4, 5]. This hypothesis predicts that only vocal learning species (such as humans and some birds, cetaceans, and pinnipeds, but not nonhuman primates) are capable of synchronizing movements to a musical beat. Here we report experimental evidence for synchronization to a beat in a sulphur-crested cockatoo (Cacatua galerita eleonora). By manipulating the tempo of a musical excerpt across a wide range, we show that the animal spontaneously adjusts the tempo of its rhythmic movements to stay synchronized with the beat. These findings indicate that synchronization to a musical beat is not uniquely human and suggest that animal models can provide insights into the neurobiology and evolution of human music [6].},
	number = {10},
	journal = {Current biology: CB},
	author = {Patel, Aniruddh D. and Iversen, John R. and Bregman, Micah R. and Schulz, Irena},
	month = may,
	year = {2009},
	pmid = {19409790},
	keywords = {Adult, Animal, Animals, Auditory Perception, Behavior, Birds, Child, Dancing, Humans, Male, Music, Periodicity, Time Perception},
	pages = {827--830},
}

@article{demorest_lost_2008,
	title = {Lost in translation: {An} enculturation effect in music memory performance},
	volume = {25},
	issn = {07307829, 15338312},
	shorttitle = {Lost in {Translation}},
	url = {http://mp.ucpress.edu/cgi/doi/10.1525/mp.2008.25.3.213},
	doi = {10.1525/mp.2008.25.3.213},
	number = {3},
	urldate = {2019-12-29},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Demorest, Steven M. and Morrison, Steven J. and Jungbluth, Denise and Beken, Münir N.},
	month = feb,
	year = {2008},
	pages = {213--223},
}

@article{salame_effects_1989,
	title = {Effects of {Background} {Music} on {Phonological} {Short}-{Term} {Memory}},
	volume = {41},
	issn = {0272-4987, 1464-0740},
	url = {http://journals.sagepub.com/doi/10.1080/14640748908402355},
	doi = {10.1080/14640748908402355},
	number = {1},
	urldate = {2019-12-29},
	journal = {The Quarterly Journal of Experimental Psychology Section A},
	author = {Salamé, Pierre and Baddeley, Alan},
	month = feb,
	year = {1989},
	pages = {107--122},
}

@article{schellenberg_music_2004,
	title = {Music {Lessons} {Enhance} {IQ}},
	volume = {15},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/j.0956-7976.2004.00711.x},
	doi = {10.1111/j.0956-7976.2004.00711.x},
	abstract = {The idea that music makes you smarter has received considerable attention from scholars and the media. The present report is the first to test this hypothesis directly with random assignment of a large sample of children (N = 144) t o two different types of music lessons (keyboard or voice) or t o control groups that received drama lessons or no lessons. IQ was measured before and after the lessons with the WISC-III (Wechsler, 1991). Compared to children in the control groups, children in the music groups exhibited greater increases i n full-scale IQ from pre- to post-lessons. The effect was relatively small but it generalized across IQ subtests, index scores, and a standardized measure of academic achievement. Nevertheless, children in the drama group exhibited substantial pre- to post-test improvements in adaptive social behavior that were not evident in the music groups.},
	number = {8},
	urldate = {2019-12-29},
	journal = {Psychological Science},
	author = {Schellenberg, E. Glenn},
	month = aug,
	year = {2004},
	pages = {511--514},
}

@article{bigand_multidimensional_2005,
	title = {Multidimensional scaling of emotional responses to music: {The} effect of musical expertise and of the duration of the excerpts},
	volume = {19},
	issn = {0269-9931, 1464-0600},
	shorttitle = {Multidimensional scaling of emotional responses to music},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02699930500204250},
	doi = {10.1080/02699930500204250},
	number = {8},
	urldate = {2020-01-03},
	journal = {Cognition \& Emotion},
	author = {Bigand, E. and Vieillard, S. and Madurell, F. and Marozeau, J. and Dacquet, A.},
	month = dec,
	year = {2005},
	pages = {1113--1139},
}

@article{egermann_music_2015,
	title = {Music induces universal emotion-related psychophysiological responses: comparing {Canadian} listeners to {Congolese} {Pygmies}},
	volume = {5},
	issn = {1664-1078},
	shorttitle = {Music induces universal emotion-related psychophysiological responses},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4286616/},
	doi = {10.3389/fpsyg.2014.01341},
	abstract = {Subjective and psychophysiological emotional responses to music from two different cultures were compared within these two cultures. Two identical experiments were conducted: the first in the Congolese rainforest with an isolated population of Mebenzélé Pygmies without any exposure to Western music and culture, the second with a group of Western music listeners, with no experience with Congolese music. Forty Pygmies and 40 Canadians listened in pairs to 19 music excerpts of 29–99 s in duration in random order (eight from the Pygmy population and 11 Western instrumental excerpts). For both groups, emotion components were continuously measured: subjective feeling (using a two- dimensional valence and arousal rating interface), peripheral physiological activation, and facial expression. While Pygmy music was rated as positive and arousing by Pygmies, ratings of Western music by Westerners covered the range from arousing to calming and from positive to negative. Comparing psychophysiological responses to emotional qualities of Pygmy music across participant groups showed no similarities. However, Western stimuli, rated as high and low arousing by Canadians, created similar responses in both participant groups (with high arousal associated with increases in subjective and physiological activation). Several low-level acoustical features of the music presented (tempo, pitch, and timbre) were shown to affect subjective and physiological arousal similarly in both cultures. Results suggest that while the subjective dimension of emotional valence might be mediated by cultural learning, changes in arousal might involve a more basic, universal response to low-level acoustical characteristics of music.},
	urldate = {2020-01-03},
	journal = {Frontiers in Psychology},
	author = {Egermann, Hauke and Fernando, Nathalie and Chuen, Lorraine and McAdams, Stephen},
	month = jan,
	year = {2015},
	pmid = {25620935},
	note = {tex.pmcid: PMC4286616},
}

@article{juslin_cue_2000,
	title = {Cue utilization in communication of emotion in music performance: {Relating} performance to perception.},
	volume = {26},
	issn = {1939-1277, 0096-1523},
	shorttitle = {Cue utilization in communication of emotion in music performance},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.26.6.1797},
	doi = {10.1037/0096-1523.26.6.1797},
	number = {6},
	urldate = {2020-01-03},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Juslin, Patrik N.},
	year = {2000},
	pages = {1797--1812},
}

@inproceedings{wiggins_expectation_2006,
	title = {Expectation in {Melody}: {The} {Influence} of {Context} and {Learning}},
	shorttitle = {Expectation in {Melody}},
	doi = {10.1525/mp.2006.23.5.377},
	abstract = {The Implication-Realization (IR) theory (Narmour, 1990) posits two cognitive systems involved in the generation of melodic expectations: The first consists of a limited number of symbolic rules that are held to be innate and universal; the second reflects the top-down influences of acquired stylistic knowledge. Aspects of both systems have been implemented as quantitative models in research which has yielded empirical support for both components of the theory (Cuddy \& Lunny, 1995; Krumhansl, 1995a, 1995b; Schellenberg, 1996, 1997). However, there is also evidence that the implemented bottom-up rules constitute too inflexible a model to account for the influence of the musical experience of the listener and the melodic context in which expectations are elicited. A theory is presented, according to which both bottom-up and top-down descriptions of observed patterns of melodic expectation may be accounted for in terms of the induction of statistical regularities in existing musical repertoires. A computational model that embodies this theory is developed and used to reanalyze existing experimental data on melodic expectancy. The results of three experiments with increasingly complex melodic stimuli demonstrate that this model is capable of accounting for listeners’ expectations as well as or better than the two-factor model of Schellenberg (1997).},
	author = {Wiggins, Geraint A. and Pearce, Marcus T.},
	year = {2006},
	keywords = {Archive, Artificial intelligence, Bottom-up proteomics, Cognition Disorders, Computational model, Continuation, Description, Experiment, Gestalt psychology, Mathematical model, Multi-factor authentication, Partial, Rule (guideline), Top-down and bottom-up design},
}

@article{krumhansl_petroushka_1986,
	title = {The {Petroushka} {Chord}: {A} {Perceptual} {Investigation}},
	volume = {4},
	issn = {07307829, 15338312},
	shorttitle = {The {Petroushka} {Chord}},
	url = {http://mp.ucpress.edu/cgi/doi/10.2307/40285359},
	doi = {10.2307/40285359},
	number = {2},
	urldate = {2020-01-06},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Krumhansl, Carol L. and Schmuckler, Mark A.},
	month = dec,
	year = {1986},
	pages = {153--184},
}

@article{slevc_making_2009,
	title = {Making psycholinguistics musical: {Self}-paced reading time evidence for shared processing of linguistic and musical syntax},
	volume = {16},
	issn = {1531-5320},
	shorttitle = {Making psycholinguistics musical},
	url = {https://doi.org/10.3758/16.2.374},
	doi = {10.3758/16.2.374},
	abstract = {Linguistic processing, especially syntactic processing, is often considered a hallmark of human cognition; thus, the domain specificity or domain generality of syntactic processing has attracted considerable debate. The present experiments address this issue by simultaneously manipulating syntactic processing demands in language and music. Participants performed self-paced reading of garden path sentences, in which structurally unexpected words cause temporary syntactic processing difficulty. A musical chord accompanied each sentence segment, with the resulting sequence forming a coherent chord progression. When structurally unexpected words were paired with harmonically unexpected chords, participants showed substantially enhanced garden path effects. No such interaction was observed when the critical words violated semantic expectancy or when the critical chords violated timbral expectancy. These results support a prediction of the shared syntactic integration resource hypothesis (Patel, 2003), which suggests that music and language draw on a common pool of limited processing resources for integrating incoming elements into syntactic structures. Notations of the stimuli from this study may be downloaded from pbr.psychonomic-journals.org/content/supplemental.},
	number = {2},
	urldate = {2020-01-06},
	journal = {Psychonomic Bulletin \& Review},
	author = {Slevc, L. Robert and Rosenberg, Jason C. and Patel, Aniruddh D.},
	month = apr,
	year = {2009},
	pages = {374--381},
}

@article{steinbeis_role_2006,
	title = {The role of harmonic expectancy violations in musical emotions: {Evidence} from subjective, physiological, and neural responses},
	volume = {18},
	issn = {0898-929X, 1530-8898},
	shorttitle = {The {Role} of {Harmonic} {Expectancy} {Violations} in {Musical} {Emotions}},
	url = {http://www.mitpressjournals.org/doi/10.1162/jocn.2006.18.8.1380},
	doi = {10.1162/jocn.2006.18.8.1380},
	number = {8},
	urldate = {2020-01-06},
	journal = {Journal of Cognitive Neuroscience},
	author = {Steinbeis, Nikolaus and Koelsch, Stefan and Sloboda, John A.},
	month = aug,
	year = {2006},
	pages = {1380--1393},
}

@article{steinbeis_shared_2008,
	title = {Shared neural resources between music and language indicate semantic processing of musical tension-{Resolution} patterns},
	volume = {18},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhm149},
	doi = {10.1093/cercor/bhm149},
	abstract = {Harmonic tension-resolution patterns have long been hypothesized to be meaningful to listeners familiar with Western music. Even though it has been shown that speciﬁcally chosen musical pieces can prime meaningful concepts, the empirical evidence in favor of such a highly speciﬁc semantic pathway has been lacking. Here we show that 2 event-related potentials in response to harmonic expectancy violations, the early right anterior negativity (ERAN) and the N500, could be systematically modulated by simultaneously presented language material containing either a syntactic or a semantic violation. Whereas the ERAN was reduced only when presented concurrently with a syntactic language violation and not with a semantic language violation, this pattern was reversed for the N500. This is the ﬁrst piece of evidence showing that tensionresolution patterns represent a route to meaning in music.},
	number = {5},
	urldate = {2020-01-06},
	journal = {Cerebral Cortex},
	author = {Steinbeis, N. and Koelsch, S.},
	month = may,
	year = {2008},
	pages = {1169--1178},
}

@article{fritz_universal_2009,
	title = {Universal {Recognition} of {Three} {Basic} {Emotions} in {Music}},
	volume = {19},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982209008136},
	doi = {10.1016/j.cub.2009.02.058},
	abstract = {It has long been debated which aspects of music perception are universal and which are developed only after exposure to a specific musical culture 1, 2, 3, 4, 5. Here, we report a crosscultural study with participants from a native African population (Mafa) and Western participants, with both groups being naive to the music of the other respective culture. Experiment 1 investigated the ability to recognize three basic emotions (happy, sad, scared/fearful) expressed in Western music. Results show that the Mafas recognized happy, sad, and scared/fearful Western music excerpts above chance, indicating that the expression of these basic emotions in Western music can be recognized universally. Experiment 2 examined how a spectral manipulation of original, naturalistic music affects the perceived pleasantness of music in Western as well as in Mafa listeners. The spectral manipulation modified, among other factors, the sensory dissonance of the music. The data show that both groups preferred original Western music and also original Mafa music over their spectrally manipulated versions. It is likely that the sensory dissonance produced by the spectral manipulation was at least partly responsible for this effect, suggesting that consonance and permanent sensory dissonance universally influence the perceived pleasantness of music.},
	number = {7},
	urldate = {2020-01-03},
	journal = {Current Biology},
	author = {Fritz, Thomas and Jentschke, Sebastian and Gosselin, Nathalie and Sammler, Daniela and Peretz, Isabelle and Turner, Robert and Friederici, Angela D. and Koelsch, Stefan},
	month = apr,
	year = {2009},
	keywords = {SYSNEURO},
	pages = {573--576},
}

@article{koelsch_investigating_2006,
	title = {Investigating emotion with music: an {fMRI} study},
	volume = {27},
	issn = {1065-9471},
	shorttitle = {Investigating emotion with music},
	doi = {10.1002/hbm.20180},
	abstract = {The present study used pleasant and unpleasant music to evoke emotion and functional magnetic resonance imaging (fMRI) to determine neural correlates of emotion processing. Unpleasant (permanently dissonant) music contrasted with pleasant (consonant) music showed activations of amygdala, hippocampus, parahippocampal gyrus, and temporal poles. These structures have previously been implicated in the emotional processing of stimuli with (negative) emotional valence; the present data show that a cerebral network comprising these structures can be activated during the perception of auditory (musical) information. Pleasant (contrasted to unpleasant) music showed activations of the inferior frontal gyrus (IFG, inferior Brodmann's area (BA) 44, BA 45, and BA 46), the anterior superior insula, the ventral striatum, Heschl's gyrus, and the Rolandic operculum. IFG activations appear to reflect processes of music-syntactic analysis and working memory operations. Activations of Rolandic opercular areas possibly reflect the activation of mirror-function mechanisms during the perception of the pleasant tunes. Rolandic operculum, anterior superior insula, and ventral striatum may form a motor-related circuitry that serves the formation of (premotor) representations for vocal sound production during the perception of pleasant auditory information. In all of the mentioned structures, except the hippocampus, activations increased over time during the presentation of the musical stimuli, indicating that the effects of emotion processing have temporal dynamics; the temporal dynamics of emotion have so far mainly been neglected in the functional imaging literature.},
	number = {3},
	journal = {Human Brain Mapping},
	author = {Koelsch, Stefan and Fritz, Thomas and V Cramon, D. Yves and Müller, Karsten and Friederici, Angela D.},
	month = mar,
	year = {2006},
	pmid = {16078183},
	keywords = {Acoustic Stimulation, Adult, Auditory Perception, Basal Ganglia, Brain, Cerebral Cortex, Emotions, Female, Functional Laterality, Humans, Larynx, Limbic System, Magnetic Resonance Imaging, Male, Music, Nerve Net, Neural Pathways},
	pages = {239--250},
}

@article{vieillard_happy_2008,
	title = {Happy, sad, scary and peaceful musical excerpts for research on emotions},
	volume = {22},
	issn = {0269-9931, 1464-0600},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02699930701503567},
	doi = {10.1080/02699930701503567},
	number = {4},
	urldate = {2020-01-03},
	journal = {Cognition \& Emotion},
	author = {Vieillard, Sandrine and Peretz, Isabelle and Gosselin, Nathalie and Khalfa, Stéphanie and Gagnon, Lise and Bouchard, Bernard},
	month = jun,
	year = {2008},
	pages = {720--752},
}

@article{prince_pitch_2009,
	title = {Pitch and time, tonality and meter: how do musical dimensions combine?},
	volume = {35},
	issn = {1939-1277},
	shorttitle = {Pitch and time, tonality and meter},
	doi = {10.1037/a0016456},
	abstract = {The authors examined how the structural attributes of tonality and meter influence musical pitch-time relations. Listeners heard a musical context followed by probe events that varied in pitch class and temporal position. Tonal and metric hierarchies contributed additively to the goodness-of-fit of probes, with pitch class exerting a stronger influence than temporal position (Experiment 1), even when listeners attempted to ignore pitch (Experiment 2). Speeded classification tasks confirmed this asymmetry. Temporal classification was biased by tonal stability (Experiment 3), but pitch classification was unaffected by temporal position (Experiment 4). Experiments 5 and 6 ruled out explanations based on the presence of pitch classes and temporal positions in the context, unequal stimulus quantity, and discriminability. The authors discuss how typical Western music biases attention toward pitch and distinguish between dimensional discriminability and salience.},
	number = {5},
	journal = {Journal of Experimental Psychology. Human Perception and Performance},
	author = {Prince, Jon B. and Thompson, William F. and Schmuckler, Mark A.},
	month = oct,
	year = {2009},
	pmid = {19803659},
	keywords = {Adult, Analysis of Variance, Attention, Comprehension, Concept Formation, Discrimination, Female, Humans, Male, Music, Observer Variation, Pitch Perception, Psychological, Reference Values, Time Perception, Young Adult},
	pages = {1598--1617},
}

@article{blood_intensely_2001,
	title = {Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion},
	volume = {98},
	issn = {0027-8424},
	doi = {10.1073/pnas.191355898},
	abstract = {We used positron emission tomography to study neural mechanisms underlying intensely pleasant emotional responses to music. Cerebral blood flow changes were measured in response to subject-selected music that elicited the highly pleasurable experience of "shivers-down-the-spine" or "chills." Subjective reports of chills were accompanied by changes in heart rate, electromyogram, and respiration. As intensity of these chills increased, cerebral blood flow increases and decreases were observed in brain regions thought to be involved in reward/motivation, emotion, and arousal, including ventral striatum, midbrain, amygdala, orbitofrontal cortex, and ventral medial prefrontal cortex. These brain structures are known to be active in response to other euphoria-inducing stimuli, such as food, sex, and drugs of abuse. This finding links music with biologically relevant, survival-related stimuli via their common recruitment of brain circuitry involved in pleasure and reward.},
	number = {20},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Blood, A. J. and Zatorre, R. J.},
	month = sep,
	year = {2001},
	pmid = {11573015},
	note = {tex.pmcid: PMC58814},
	keywords = {Adult, Amygdala, Brain, Cerebrovascular Circulation, Emission-Computed, Emotions, Female, Functional Laterality, Heart Rate, Hippocampus, Humans, Male, Music, Organ Specificity, Prefrontal Cortex, Regression Analysis, Respiration, Reward, Tomography},
	pages = {11818--11823},
}

@article{dowling_scale_1978,
	title = {Scale and contour: {Two} components of a theory of memory for melodies},
	volume = {85},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	shorttitle = {Scale and contour},
	doi = {10.1037/0033-295X.85.4.341},
	abstract = {Develops a 2-component model of how melodies are stored in long- and short-term memory. The 1st component is the overlearned perceptual-motor schema of the musical scale. Evidence is presented supporting the lifetime stability of scales and the fact that they seem to have a basically logarithmic form cross-culturally. The 2nd component, melodic contour, is shown to function independently of pitch interval sequence in memory. 21 college students were studied using a recognition memory paradigm in which tonal standard stimuli were confused with same-contour comparisons, whether they were exact transpositions or tonal answers, but not with atonal comparison stimuli. This result is contrasted with earlier work using atonal melodies and shows the interdependence of the 2 components, scale and contour. (32 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Psychological Review},
	author = {Dowling, W. Jay},
	year = {1978},
	keywords = {Human Information Storage, Memory, Music, Pitch (Frequency)},
	pages = {341--354},
}

@article{dowling_tonal_1991,
	title = {Tonal strength and melody recognition after long and short delays},
	volume = {50},
	issn = {1532-5962},
	url = {https://doi.org/10.3758/BF03212222},
	doi = {10.3758/BF03212222},
	abstract = {In a continuous-running-memory task, subjects heard novel seven-note melodies that were tested after delays of 11 sec (empty) or 39 sec (filled). Test items were transposed to new pitch levels (to moderately distant keys in the musical sense)and included exact transpositions (targets), same-contour lures with altered pitch intervals, and new-contour lures. Melodies differed in tonal strength (degree of conformity to a musical key) and were tonally strong, tonally weak, or atonal. False alarms to same-contour lures decreased over the longer delay period, but only for tonal stimuli. In agreement with previous studies, discrimination of detailed changes in pitch intervals improved with increased delay, whereas discrimination of more global contour information declined, again only for tonal stimuli. These results suggest that poor short-delay performance in rejecting same-contour lures arises from confusion that is based on the similarity of tonality between standard stimuli and lures. If a test item has the same contour and a similar tonality to a just-presented item, subjects tend to accept it. After a delay filled with melodies in other tonalities, the salience of key information recedes, and subjects base their judgments on more detailed pattern information (namely, exact pitch intervals). The fact that tonality affects judgments of melodic contour indicates that contour is not an entirely separable feature of melodies but rather that a melody with its contour constitutes an integrated perceptual whole.},
	number = {4},
	urldate = {2020-01-08},
	journal = {Perception \& Psychophysics},
	author = {Dowling, W. Jay},
	month = jul,
	year = {1991},
	pages = {305--313},
}

@article{mcauley_play_2004,
	title = {Play it again: did this melody occur more frequently or was it heard more recently? {The} role of stimulus familiarity in episodic recognition of music},
	volume = {116},
	issn = {0001-6918},
	shorttitle = {Play it again},
	doi = {10.1016/j.actpsy.2004.02.001},
	abstract = {Episodic recognition of novel and familiar melodies was examined by asking participants to make judgments about the recency and frequency of presentation of melodies over the course of two days of testing. For novel melodies, recency judgments were poor and participants often confused the number of presentations of a melody with its day of presentation; melodies heard frequently were judged as have been heard more recently than they actually were. For familiar melodies, recency judgments were much more accurate and the number of presentations of a melody helped rather than hindered performance. Frequency judgments were generally more accurate than recency judgments and did not demonstrate the same interaction with musical familiarity. Overall, these findings suggest that (1) episodic recognition of novel melodies is based more on a generalized "feeling of familiarity" than on a specific episodic memory, (2) frequency information contributes more strongly to this generalized memory than recency information, and (3) the formation of an episodic memory for a melody depends either on the overall familiarity of the stimulus or the availability of a verbal label.},
	number = {1},
	journal = {Acta Psychologica},
	author = {McAuley, J. Devin and Stevens, Catherine and Humphreys, Michael S.},
	month = may,
	year = {2004},
	pmid = {15111232},
	keywords = {Adolescent, Adult, Analysis of Variance, Humans, Music, Psychology, Recognition},
	pages = {93--108},
}

@article{halpern_aging_1995,
	title = {Aging and experience in the recognition of musical transpositions},
	volume = {10},
	issn = {0882-7974},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1996-00911-001&site=ehost-live},
	doi = {10.1037/0882-7974.10.3.325},
	abstract = {The authors examined the effects of age, musical experience, and characteristics of musical stimuli on a melodic short-term memory task in which participants had to recognize whether a tune was an exact transposition of another tune recently presented. Participants were musicians and nonmusicians between ages 18 and 30 or 60 and 80. In 4 experiments, the authors found that age and experience affected different aspects of the task, with experience becoming more influential when interference was provided during the task. Age and experience interacted only weakly, and neither age nor experience influenced the superiority of tonal over atonal materials. Recognition memory for the sequences did not reflect the same pattern of results as the transposition task. The implications of these results for theories of aging, experience, and music cognition are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	urldate = {2020-01-08},
	journal = {Psychology and Aging},
	author = {Halpern, Andrea R. and Bartlett, James C. and Dowling, W. Jay},
	month = sep,
	year = {1995},
	keywords = {18–30 vs 60–80 yr olds, Adolescent, Adult, Age Differences, Aged, Aging, Auditory Perception, Experience Level, Female, Humans, Male, Mental Recall, Middle Aged, Music, Recognition (Learning), Short Term Memory, melodic short term memory \& recognition of transpositions, musical experience \& characteristics of music stimuli},
	pages = {325--342},
}

@article{bigand_sensory_2003,
	title = {Sensory versus cognitive components in harmonic priming},
	volume = {29},
	issn = {0096-1523},
	doi = {10.1037//0096-1523.29.1.159},
	abstract = {This study investigated the strength of sensory and cognitive components involved in musical priming. In Experiment 1, the harmonic function of the target chord and the number of pitch classes shared by the prime sequence and the target chord were manipulated. In Experiment 2, the temporal course of sensory and cognitive priming was investigated. For both musician and nonmusician listeners, cognitive priming systematically overruled sensory priming even at fast and very fast tempi (300 ms and 150 ms per chord). Cognitive priming continued to challenge sensory priming processes at extremely fast tempo (75 ms per chord) but only for participants who began the experimental session with slower tempi. This outcome suggests that the cognitive component is a fast-acting component that competes with sensory priming.},
	number = {1},
	journal = {Journal of Experimental Psychology. Human Perception and Performance},
	author = {Bigand, Emmanuel and Poulin, Bénédicte and Tillmann, Barbara and Madurell, François and D'Adamo, Daniel A.},
	month = feb,
	year = {2003},
	pmid = {12669755},
	keywords = {Analysis of Variance, Auditory Perception, Cognition, Humans, Music, Pitch Perception, Psychoacoustics, Psychology, Reaction Time, Set},
	pages = {159--171},
}

@article{dowling_perception_1973,
	title = {The perception of interleaved melodies},
	volume = {5},
	issn = {0010-0285(Print)},
	doi = {10.1016/0010-0285(73)90040-6},
	abstract = {Investigated perceptual grouping of successively-presented tones from 2 melodies and the effect of overlapping pitch ranges in 4 experiments. 6 undergraduates served as Ss in both Exp I and II, 12 in Exp III, and 21 in Exp IV. Exp I showed that identification of interleaved pairs of familiar melodies was possible if pitch ranges did not overlap, but difficult otherwise. A short-term recognition memory paradigm (Exp II) showed that interleaving a background melody with an unfamiliar melody interfered with same-different judgments regardless of the separation of pitch ranges, but that range separation attenuated the interference effect. When pitch ranges overlapped, Ss overcame the interference effect and recognized a familiar melody if it was prespecified (Exp III). Familiarity or prespecification of the interleaved background melody did not reduce interfering effects on same-different judgments of unfamiliar target melodies (Exp IV). (19 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Cognitive Psychology},
	author = {Dowling, W. J.},
	year = {1973},
	keywords = {Music, Pitch (Frequency), Pitch Perception},
	pages = {322--337},
}

@article{deutsch_twochannel_1975,
	title = {Two‐channel listening to musical scales},
	volume = {57},
	issn = {0001-4966},
	url = {https://asa-scitation-org.ezproxy.library.qmul.ac.uk/doi/10.1121/1.380573},
	doi = {10.1121/1.380573},
	number = {5},
	urldate = {2020-01-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Deutsch, Diana},
	month = may,
	year = {1975},
	pages = {1156--1160},
}

@article{miller_trill_1950,
	title = {The {Trill} {Threshold}},
	volume = {22},
	issn = {0001-4966},
	url = {https://asa-scitation-org.ezproxy.library.qmul.ac.uk/doi/10.1121/1.1906663},
	doi = {10.1121/1.1906663},
	number = {5},
	urldate = {2020-01-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Miller, George A. and Heise, George A.},
	month = sep,
	year = {1950},
	pages = {637--638},
}

@article{frankland_parsing_2004,
	title = {Parsing of melody: {Quantification} and testing of the local grouping rules of lerdahl and jackendoff's a generative theory of tonal music},
	volume = {21},
	issn = {0730-7829},
	shorttitle = {Parsing of {Melody}},
	url = {https://www.jstor.org/stable/10.1525/mp.2004.21.4.499},
	doi = {10.1525/mp.2004.21.4.499},
	abstract = {In two experiments, the empirical parsing of melodies was compared with predictions derived from four grouping preference rules of A Generative Theory of Tonal Music (F. Lerdahl \& R. Jackendoff, 1983). In Experiment 1 (n = 123), listeners representing a wide range of musical training heard two familiar nursery-rhyme melodies and one unfamiliar tonal melody, each presented three times. During each repetition, listeners indicated the location of boundaries between units by pressing a key. Experiment 2 (n = 33) repeated Experiment 1 with different stimuli: one familiar and one unfamiliar nursery-rhyme melody, and one unfamiliar, tonal melody from the classical repertoire. In all melodies of both experiments, there was good within-subject consistency of boundary placement across the three repetitions (mean r = .54). Consistencies between Repetitions 2 and 3 were even higher (mean r = .63). Hence, Repetitions 2 and 3 were collapsed. After collapsing, there was high between-subjects similarity in boundary placement for each melody (mean r = .62), implying that all participants parsed the melodies in essentially the same (though not identical) manner. A role for musical training in parsing appeared only for the unfamiliar, classical melody of Experiment 2. The empirical parsing profiles were compared with the quantified predictions of Grouping Preference Rules 2a (the Rest aspect of Slur/Rest), 2b (Attack-point), 3a (Register change), and 3d (Length change). Based on correlational analyses, only Attack-point (mean r = .80) and Rest (mean r = .54) were necessary to explain the parsings of participants. Little role was seen for Register change (mean r = .14) or Length change (mean r = –.09). Solutions based on multiple regression further reduced the role for Register and Length change. Generally, results provided some support for aspects of A Generative Theory of Tonal Music, while implying that some alterations might be useful.},
	number = {4},
	urldate = {2020-01-10},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Frankland, Bradley W. and McAdams, Stephen and Cohen, Annabel J.},
	year = {2004},
	pages = {499--543},
}

@article{peretz_clustering_1989,
	title = {Clustering in {Music}: {An} {Appraisal} of {Task} {Factors}},
	volume = {24},
	issn = {00207594},
	shorttitle = {Clustering in {Music}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=5776040&site=ehost-live},
	doi = {10.1080/00207594.1989.10600040},
	abstract = {Studies the pitch and temporal intervals of tune fragments in terms of gestalt similarity principles. Segmentation of the tunes; Part structure representation that is being built during music listening.},
	number = {2},
	urldate = {2020-01-10},
	journal = {International Journal of Psychology},
	author = {Peretz, Isabelle},
	month = apr,
	year = {1989},
	keywords = {Gestalt psychology, Music},
	pages = {157},
}

@article{cook_california_2013,
	title = {A {California} sea lion ({Zalophus} californianus) can keep the beat: motor entrainment to rhythmic auditory stimuli in a non vocal mimic},
	volume = {127},
	issn = {1939-2087},
	shorttitle = {A {California} sea lion ({Zalophus} californianus) can keep the beat},
	doi = {10.1037/a0032345},
	abstract = {Is the ability to entrain motor activity to a rhythmic auditory stimulus, that is "keep a beat," dependent on neural adaptations supporting vocal mimicry? That is the premise of the vocal learning and synchronization hypothesis, recently advanced to explain the basis of this behavior (A. Patel, 2006, Musical Rhythm, Linguistic Rhythm, and Human Evolution, Music Perception, 24, 99-104). Prior to the current study, only vocal mimics, including humans, cockatoos, and budgerigars, have been shown to be capable of motoric entrainment. Here we demonstrate that a less vocally flexible animal, a California sea lion (Zalophus californianus), can learn to entrain head bobbing to an auditory rhythm meeting three criteria: a behavioral response that does not reproduce the stimulus; performance transfer to a range of novel tempos; and entrainment to complex, musical stimuli. These findings show that the capacity for entrainment of movement to rhythmic sounds does not depend on a capacity for vocal mimicry, and may be more widespread in the animal kingdom than previously hypothesized.},
	number = {4},
	journal = {Journal of Comparative Psychology (Washington, D.C.: 1983)},
	author = {Cook, Peter and Rouse, Andrew and Wilson, Margaret and Reichmuth, Colleen},
	month = nov,
	year = {2013},
	pmid = {23544769},
	keywords = {Animal, Animals, Auditory Perception, Behavior, Family cacatuidae, Female, Hearing problem, Learning Disorders, Linguistics, Melopsittacus, Movement, Music, Psychomotor Performance, Sea Lion, Sea Lions, Time Factors, Zalophus califonianus},
	pages = {412--427},
}

@article{hyde_cortical_2007,
	title = {Cortical thickness in congenital amusia: {When} less is better than more},
	volume = {27},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Cortical {Thickness} in {Congenital} {Amusia}},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3039-07.2007},
	doi = {10.1523/JNEUROSCI.3039-07.2007},
	number = {47},
	urldate = {2019-10-08},
	journal = {Journal of Neuroscience},
	author = {Hyde, K. L. and Lerch, J. P. and Zatorre, R. J. and Griffiths, T. D. and Evans, A. C. and Peretz, I.},
	month = nov,
	year = {2007},
	keywords = {Acoustic Stimulation, Adult, Aged, Central, Cerebral Cortex, Female, Hearing Loss, Humans, Male, Middle Aged, Music},
	pages = {13028--13032},
}

@article{saffran_statistical_1999,
	title = {Statistical learning of tone sequences by human infants and adults},
	volume = {70},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027798000754},
	doi = {10.1016/S0010-0277(98)00075-4},
	abstract = {Previous research suggests that language learners can detect and use the statistical properties of syllable sequences to discover words in continuous speech (e.g. Aslin, R.N., Saffran, J.R., Newport, E.L., 1998. Computation of conditional probability statistics by 8-month-old infants. Psychological Science 9, 321–324; Saffran, J.R., Aslin, R.N., Newport, E.L., 1996. Statistical learning by 8-month-old infants. Science 274, 1926–1928; Saffran, J., R., Newport, E.L., Aslin, R.N., (1996). Word segmentation: the role of distributional cues. Journal of Memory and Language 35, 606–621; Saffran, J.R., Newport, E.L., Aslin, R.N., Tunick, R.A., Barrueco, S., 1997. Incidental language learning: Listening (and learning) out of the corner of your ear. Psychological Science 8, 101–195). In the present research, we asked whether this statistical learning ability is uniquely tied to linguistic materials. Subjects were exposed to continuous non-linguistic auditory sequences whose elements were organized into `tone words'. As in our previous studies, statistical information was the only word boundary cue available to learners. Both adults and 8-month-old infants succeeded at segmenting the tone stream, with performance indistinguishable from that obtained with syllable streams. These results suggest that a learning mechanism previously shown to be involved in word segmentation can also be used to segment sequences of non-linguistic stimuli.},
	number = {1},
	urldate = {2020-01-10},
	journal = {Cognition},
	author = {Saffran, Jenny R and Johnson, Elizabeth K and Aslin, Richard N and Newport, Elissa L},
	month = feb,
	year = {1999},
	keywords = {Continuous speech, Statistical learning ability, Word segmentation},
	pages = {27--52},
}

@article{tillmann_implicit_2004,
	title = {Implicit learning of musical timbre sequences: {Statistical} regularities confronted with acoustical ({Dis}){Similarities}},
	volume = {30},
	issn = {0278-7393},
	shorttitle = {Implicit {Learning} of {Musical} {Timbre} {Sequences}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2004-17949-014&site=ehost-live},
	doi = {10.1037/0278-7393.30.5.1131},
	abstract = {The present study investigated the influence of acoustical characteristics on the implicit learning of statistical regularities (transition probabilities) in sequences of musical timbres. The sequences were constructed in such a way that the acoustical dissimilarities between timbres potentially created segmentations that either supported (S1) or contradicted (S2) the statistical regularities or were neutral (S3). In the learning group, participants first listened to the continuous timbre sequence and then had to distinguish statistical units from new units. In comparison to a control group without the exposition phase, no interaction between sequence type and amount of learning was observed: Performance increased by the same amount for the three sequences. In addition, performance reflected an overall preference for acoustically similar timbre units. The present outcome extends previous data from the domain of implicit learning to complex nonverbal auditory material. It further suggests that listeners become sensitive to statistical regularities despite acoustical characteristics in the material that potentially affect grouping. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	urldate = {2020-01-10},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Tillmann, Barbara and McAdams, Stephen},
	month = sep,
	year = {2004},
	keywords = {Acoustics, Auditory Stimulation, Humans, Implicit Learning, Incidental Learning, Learning, Music, Music Perception, Pitch Discrimination, Pitch Perception, Stimulus Similarity, acoustical characteristic similarity, auditory material, implicit learning, statistical regularities, transition probabilities},
	pages = {1131--1142},
}

@article{holtz_analyzing_2012,
	title = {Analyzing {Internet} {Forums}: {A} {Practical} {Guide}},
	volume = {24},
	issn = {1864-1105, 2151-2388},
	shorttitle = {Analyzing {Internet} {Forums}},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-1105/a000062},
	doi = {10.1027/1864-1105/a000062},
	number = {2},
	urldate = {2020-01-27},
	journal = {Journal of Media Psychology},
	author = {Holtz, Peter and Kronberger, Nicole and Wagner, Wolfgang},
	month = jan,
	year = {2012},
	pages = {55--66},
}

@article{dalla_bella_developmental_2001,
	title = {A developmental study of the affective value of tempo and mode in music},
	volume = {80},
	issn = {0010-0277},
	doi = {10.1016/s0010-0277(00)00136-0},
	abstract = {Do children use the same properties as adults in determining whether music sounds happy or sad? We addressed this question with a set of 32 excerpts (16 happy and 16 sad) taken from pre-existing music. The tempo (i.e. the number of beats per minute) and the mode (i.e. the specific subset of pitches used to write a given musical excerpt) of these excerpts were modified independently and jointly in order to measure their effects on happy-sad judgments. Adults and children from 3 to 8 years old were required to judge whether the excerpts were happy or sad. The results show that as adults, 6–8-year-old children are affected by mode and tempo manipulations. In contrast, 5-year-olds' responses are only affected by a change of tempo. The youngest children (3–4-year-olds) failed to distinguish the happy from the sad tone of the music above chance. The results indicate that tempo is mastered earlier than mode to infer the emotional tone conveyed by music.},
	number = {3},
	journal = {Cognition},
	author = {Dalla Bella, S. and Peretz, I. and Rousseau, L. and Gosselin, N.},
	month = jul,
	year = {2001},
	pmid = {11274986},
	keywords = {Adult, Affect, Auditory Perception, Child, Child Development, Female, Humans, Male, Music, Preschool},
	pages = {B1--10},
}

@article{darwin_auditory_1972,
	title = {An auditory analogue of the {Sperling} partial report procedure: {Evidence} for brief auditory storage},
	volume = {3},
	issn = {0010-0285(Print)},
	shorttitle = {An auditory analogue of the {Sperling} partial report procedure},
	doi = {10.1016/0010-0285(72)90007-2},
	abstract = {Discusses 3 experiments on the partial report of material presented auditorily over 3 spatially different channels. When partial report was required by spatial location, it was superior to whole report if the cue came less than 4 sec. after the end of the stimuli (Exp. I). When partial report was required by semantic category (letters/digits), the relation between it and whole report depended on whether the S was also asked to attribute each item to its correct spatial location. When location was required, partial report was lower than whole report and showed no significant decay with delay of the partial report indicator (Exp. II), but when location was not required partial report was superior to whole report for indicator delays of less than 2 sec. (Exp. III). However, this superiority was much less than that found in Exp. I when partial report was required by spatial location. These results are compatible with a store which has a useful life of around 2 sec. and from which material may be retrieved more easily by spatial location than by semantic category. (26 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Cognitive Psychology},
	author = {Darwin, Christopher J. and Turvey, Michael T. and Crowder, Robert C.},
	year = {1972},
	keywords = {Auditory Perception, Information, Memory, Semantics, Spatial Perception},
	pages = {255--267},
}

@article{juslin_experience_2008,
	title = {An experience sampling study of emotional reactions to music: listener, music, and situation},
	volume = {8},
	issn = {1528-3542},
	shorttitle = {An experience sampling study of emotional reactions to music},
	doi = {10.1037/a0013505},
	abstract = {The Experience Sampling Method was used to explore emotions to music as they naturally occurred in everyday life, with a focus on the prevalence of different musical emotions and how such emotions are related to various factors in the listener, the music, and the situation. Thirty-two college students, 20 to 31 years old, carried a palmtop that emitted a sound signal seven times per day at random intervals for 2 weeks. When signaled, participants were required to complete a questionnaire on the palmtop. Results showed that music occurred in 37\% of the episodes, and in 64\% of the music episodes, the participants reported that the music affected how they felt. Comparisons showed that happiness-elation and nostalgia-longing were more frequent in episodes with musical emotions, whereas anger-irritation, boredom-indifference, and anxiety-fear were more frequent in episodes with nonmusical emotions. The prevalence of specific musical emotions correlated with personality measures and also varied depending on the situation (e.g., current activity, other people present), thus highlighting the need to use representative samples of situations to obtain valid estimates of prevalence.},
	number = {5},
	journal = {Emotion (Washington, D.C.)},
	author = {Juslin, Patrik N. and Liljeström, Simon and Västfjäll, Daniel and Barradas, Gonçalo and Silva, Ana},
	month = oct,
	year = {2008},
	pmid = {18837617},
	keywords = {Activities of Daily Living, Adult, Computers, Emotions, Female, Handheld, Humans, Male, Music, Psychology, Set, Social Environment, Surveys and Questionnaires, Young Adult},
	pages = {668--683},
}

@article{iverson_auditory_1995,
	title = {Auditory stream segregation by musical timbre: {Effects} of static and dynamic acoustic attributes},
	volume = {21},
	issn = {1939-1277(Electronic),0096-1523(Print)},
	shorttitle = {Auditory stream segregation by musical timbre},
	doi = {10.1037/0096-1523.21.4.751},
	abstract = {Two experiments examined the influence of timbre on auditory stream segregation. In Experiment 1, listeners heard sequences of orchestral tones equated for pitch and loudness, and they rated how strongly the instruments segregated. Multidimensional scaling analyses of these ratings revealed that segregation was based on the static and dynamic acoustic attributes that influenced similarity judgments in a previous experiment (P. Iverson \& C. L. Krumhansl, 1993). In Experiment 2, listeners heard interleaved melodies and tried to recognize the melodies played by a target timbre. The results extended the findings of Experiment 1 to tones varying in pitch. Auditory stream segregation appears to be influenced by gross differences in static spectra and by dynamic attributes, including attack duration and spectral flux. These findings support a Gestalt explanation of stream segregation and provide evidence against a peripheral channel model. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Iverson, Paul},
	year = {1995},
	keywords = {Acoustics, Auditory Perception, Auditory Stimulation, Humans, Loudness, Music, Pitch (Frequency), Pitch Perception, Time Factors, listeners, musical stream segregation by timbre, static vs dynamic acoustic spectral flux vs amplitude vs frequency vs perceptual attack time},
	pages = {751--763},
}

@article{thompson_decoding_2004,
	title = {Decoding speech prosody: do music lessons help?},
	volume = {4},
	issn = {1528-3542},
	shorttitle = {Decoding speech prosody},
	doi = {10.1037/1528-3542.4.1.46},
	abstract = {Three experiments revealed that music lessons promote sensitivity to emotions conveyed by speech prosody. After hearing semantically neutral utterances spoken with emotional (i.e., happy, sad, fearful, or angry) prosody, or tone sequences that mimicked the utterances' prosody, participants identified the emotion conveyed. In Experiment 1 (n=20), musically trained adults performed better than untrained adults. In Experiment 2 (n=56), musically trained adults outperformed untrained adults at identifying sadness, fear, or neutral emotion. In Experiment 3 (n=43), 6-year-olds were tested after being randomly assigned to 1 year of keyboard, vocal, drama, or no lessons. The keyboard group performed equivalently to the drama group and better than the no-lessons group at identifying anger or fear.},
	number = {1},
	journal = {Emotion (Washington, D.C.)},
	author = {Thompson, William Forde and Schellenberg, E. Glenn and Husain, Gabriela},
	month = mar,
	year = {2004},
	pmid = {15053726},
	keywords = {Adult, Emotions, Female, Humans, Male, Music, Semantics, Speech Perception},
	pages = {46--64},
}

@article{schellenberg_expectancy_1996,
	title = {Expectancy in melody: {Tests} of the implication-realization model},
	volume = {58},
	issn = {1873-7838(Electronic),0010-0277(Print)},
	shorttitle = {Expectancy in melody},
	doi = {10.1016/0010-0277(95)00665-6},
	abstract = {Examined the implication-realization model's description of tone-to-tone expectancies for continuations of melodies in 3 experiments with a total of 62 Ss. The model's predictions for expectancies are described by principles specified in terms of interval size and direction of pitch, used to predict data in which Ss judged how well individual test tones continued melodic fragments. The model successfully predicted Ss'judgments across different musical styles, regardless of the extent of musical training or whether they were raised in China or the US. A revised and simplified model did not reduce predictive power for any condition. Findings indicate that the implication-realization model is over-specified. The consistency found across experimental tasks, musical styles, and Ss suggests that the revised version of the model may withstand the original model's claims of universality. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Cognition},
	author = {Schellenberg, E. Glenn},
	year = {1996},
	keywords = {Adult, Auditory Discrimination, China, Cross Cultural Differences, Cross-Cultural Comparison, Expectations, Humans, Judgment, Models, Music, Musicians, Pattern Discrimination, Pitch (Frequency), Pitch Discrimination, Practice, Psychoacoustics, Psychological, Psychology, Set, Sound Spectrography, United States},
	pages = {75--125},
}

@article{deliege_grouping_1987,
	title = {Grouping conditions in listening to music: {An} approach to lerdahl \& jackendoff's grouping preference rules},
	volume = {4},
	issn = {0730-7829},
	shorttitle = {Grouping {Conditions} in {Listening} to {Music}},
	url = {https://www.jstor.org/stable/40285378},
	doi = {10.2307/40285378},
	abstract = {Lerdahl and Jackendoff propose a grouping theory that can apply to both the global and the local structures of the process of listening to music. It is a set of rules expressing the intuitive organization of groups in music perception. The "proximity rules" describe the length differences and the "change rules" describe the modifications in the acoustic or temporal state of sound structures, in relation to Gestalt Theory. As such, they propose a testable hypothesis on certain aspects of music perception. Two experiments are reported, which do not go beyond segmentation into two levels of grouping. They compare the grouping behavior of two categories of subjects, nonmusicians and musicians. Four questions are raised: (1) Do the segmentations reported by subjects answer in all respects the predictions of the rules? (2) Are they available to both categories of subjects? (3) Do they cover all grouping situations in music? (4) Are they of equal perceptual salience? The first experiment used material taken from compositions in the Western art music repertoire (Bach to Stravinsky). The second one put the rules in conflict in simple melodic sequences, where a combination of all possible conflicts between pairs of rules was designed. The results show the validity of the rules. Nonmusicians had poorer performances with repertoire music sequences. Yet the two categories of subjects do not show a radically different grouping behavior. New rules are suggested by the segmentations that were not in accordance with the theory. They also show some difficulties for the length rules deriving from the Gestalt Similarity law.},
	number = {4},
	urldate = {2020-01-10},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Deliege, Irene},
	year = {1987},
	pages = {325--359},
}

@article{iversen_perception_2008,
	title = {Perception of rhythmic grouping depends on auditory experience},
	volume = {124},
	issn = {1520-8524},
	doi = {10.1121/1.2973189},
	abstract = {Many aspects of perception are known to be shaped by experience, but others are thought to be innate universal properties of the brain. A specific example comes from rhythm perception, where one of the fundamental perceptual operations is the grouping of successive events into higher-level patterns, an operation critical to the perception of language and music. Grouping has long been thought to be governed by innate perceptual principles established a century ago. The current work demonstrates instead that grouping can be strongly dependent on culture. Native English and Japanese speakers were tested for their perception of grouping of simple rhythmic sequences of tones. Members of the two cultures showed different patterns of perceptual grouping, demonstrating that these basic auditory processes are not universal but are shaped by experience. It is suggested that the observed perceptual differences reflect the rhythms of the two languages, and that native language can exert an influence on general auditory perception at a basic level.},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Iversen, John R. and Patel, Aniruddh D. and Ohgushi, Kengo},
	month = oct,
	year = {2008},
	pmid = {19062864},
	keywords = {Acoustic Stimulation, Adolescent, Adult, California, Cross-Cultural Comparison, Cues, Humans, Japan, Language, Multilingualism, Periodicity, Pitch Discrimination, Psychological, Signal Detection, Young Adult},
	pages = {2263--2271},
}

@article{mosing_practice_2014,
	title = {Practice does not make perfect: no causal effect of music practice on music ability},
	volume = {25},
	issn = {1467-9280},
	shorttitle = {Practice does not make perfect},
	doi = {10.1177/0956797614541990},
	abstract = {The relative importance of nature and nurture for various forms of expertise has been intensely debated. Music proficiency is viewed as a general model for expertise, and associations between deliberate practice and music proficiency have been interpreted as supporting the prevailing idea that long-term deliberate practice inevitably results in increased music ability. Here, we examined the associations (rs = .18-.36) between music practice and music ability (rhythm, melody, and pitch discrimination) in 10,500 Swedish twins. We found that music practice was substantially heritable (40\%-70\%). Associations between music practice and music ability were predominantly genetic, and, contrary to the causal hypothesis, nonshared environmental influences did not contribute. There was no difference in ability within monozygotic twin pairs differing in their amount of practice, so that when genetic predisposition was controlled for, more practice was no longer associated with better music skills. These findings suggest that music practice may not causally influence music ability and that genetic variation among individuals affects both ability and inclination to practice.},
	number = {9},
	journal = {Psychological Science},
	author = {Mosing, Miriam A. and Madison, Guy and Pedersen, Nancy L. and Kuja-Halkola, Ralf and Ullén, Fredrik},
	month = sep,
	year = {2014},
	pmid = {25079217},
	keywords = {Adult, Aptitude, Basal Ganglia Diseases, Causality, Dizygotic, Female, Genetic Predisposition to Disease, Humans, Male, Mental association, Middle Aged, Monozygotic, Music, Pitch Discrimination, Practice, Psychological, Twins, Variation (Genetics), causality, expertise, heritability, music ability, practice, training, twin},
	pages = {1795--1803},
}

@article{egermann_probabilistic_2013,
	title = {Probabilistic models of expectation violation predict psychophysiological emotional responses to live concert music},
	volume = {13},
	issn = {1531-135X},
	doi = {10.3758/s13415-013-0161-y},
	abstract = {We present the results of a study testing the often-theorized role of musical expectations in inducing listeners' emotions in a live flute concert experiment with 50 participants. Using an audience response system developed for this purpose, we measured subjective experience and peripheral psychophysiological changes continuously. To confirm the existence of the link between expectation and emotion, we used a threefold approach. (1) On the basis of an information-theoretic cognitive model, melodic pitch expectations were predicted by analyzing the musical stimuli used (six pieces of solo flute music). (2) A continuous rating scale was used by half of the audience to measure their experience of unexpectedness toward the music heard. (3) Emotional reactions were measured using a multicomponent approach: subjective feeling (valence and arousal rated continuously by the other half of the audience members), expressive behavior (facial EMG), and peripheral arousal (the latter two being measured in all 50 participants). Results confirmed the predicted relationship between high-information-content musical events, the violation of musical expectations (in corresponding ratings), and emotional reactions (psychologically and physiologically). Musical structures leading to expectation reactions were manifested in emotional reactions at different emotion component levels (increases in subjective arousal and autonomic nervous system activations). These results emphasize the role of musical structure in emotion induction, leading to a further understanding of the frequently experienced emotional effects of music.},
	number = {3},
	journal = {Cognitive, Affective \& Behavioral Neuroscience},
	author = {Egermann, Hauke and Pearce, Marcus T. and Wiggins, Geraint A. and McAdams, Stephen},
	month = sep,
	year = {2013},
	pmid = {23605956},
	keywords = {Acoustic Stimulation, Adolescent, Adult, Arousal, Auditory Perception, Autonomic Nervous System, Emotions, Female, Humans, Male, Models, Music, Psychophysiology, Statistical, Young Adult},
	pages = {533--553},
}

@article{blood_emotional_1999,
	title = {Emotional responses to pleasant and unpleasant music correlate with activity in paralimbic brain regions},
	volume = {2},
	issn = {1097-6256},
	doi = {10.1038/7299},
	abstract = {Neural correlates of the often-powerful emotional responses to music are poorly understood. Here we used positron emission tomography to examine cerebral blood flow (CBF) changes related to affective responses to music. Ten volunteers were scanned while listening to six versions of a novel musical passage varying systematically in degree of dissonance. Reciprocal CBF covariations were observed in several distinct paralimbic and neocortical regions as a function of dissonance and of perceived pleasantness/unpleasantness. The findings suggest that music may recruit neural mechanisms similar to those previously associated with pleasant/unpleasant emotional states, but different from those underlying other components of music perception, and other emotions such as fear.},
	number = {4},
	journal = {Nature Neuroscience},
	author = {Blood, A. J. and Zatorre, R. J. and Bermudez, P. and Evans, A. C.},
	month = apr,
	year = {1999},
	pmid = {10204547},
	keywords = {Adult, Brain Mapping, Cerebrovascular Circulation, Emission-Computed, Emotions, Female, Frontal Lobe, Gyrus Cinguli, Humans, Limbic System, Male, Music, Neocortex, Pilot Projects, Temporal Lobe, Tomography},
	pages = {382--387},
}

@article{sloboda_music_1991,
	title = {Music structure and emotional response: {Some} empirical findings},
	volume = {19},
	issn = {1741-3087(Electronic),0305-7356(Print)},
	shorttitle = {Music structure and emotional response},
	doi = {10.1177/0305735691192002},
	abstract = {83 music listeners completed a questionnaire in which they provided information about the occurrence of a range of physical reactions while listening to music. Shivers down the spine, laughter, tears, and lump in the throat were reported by over 80\% of Ss. Ss were asked to locate specific musical passages that reliably evoked such responses. Structural analysis of these passages shows that tears were most reliably evoked by melodic appogiaturas and, to a lesser extent, by sequences and harmonic movements through the cycle of 5ths to the tonic. Shivers were most reliably provoked by relatively sudden changes in harmony. Racing heart was provoked by acceleration and syncopation. Data generally support theoretical approaches to emotion based on confirmations and violations of expectancy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychology of Music},
	author = {Sloboda, John A.},
	year = {1991},
	keywords = {Emotional Responses, Music, Physiological Correlates},
	pages = {110--120},
}

@article{balkwill_recognition_2004,
	title = {Recognition of emotion in {Japanese}, {Western}, and {Hindustani} music by {Japanese} listeners},
	volume = {46},
	issn = {1468-5884(Electronic),0021-5368(Print)},
	doi = {10.1111/j.1468-5584.2004.00265.x},
	abstract = {Japanese listeners rated the expression of joy, anger and sadness in Japanese, Western, and Hindustani music. Excerpts were also rated for tempo, loudness, and complexity. Listeners were sensitive to the intended emotion in music from all three cultures, and judgments of emotion were related to judgments of acoustic cues. High ratings of joy were associated with music judged to be fast in tempo and melodically simple. High ratings of sadness were associated with music judged to be slow in tempo and melodically complex. High ratings of anger were associated with music judged to be louder and more complex. The findings suggest that listeners are sensitive to emotion in familiar and unfamiliar music, and this sensitivity is associated with the perception of acoustic cues that transcend cultural boundaries. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Japanese Psychological Research},
	author = {Balkwill, Laura-Lee and Thompson, William Forde and Matsunaga, Rie},
	year = {2004},
	keywords = {Cross Cultural Differences, Emotional Content, Emotions, Music, Music Perception, cross-culture, music, recognition of emotion},
	pages = {337--349},
}

@article{bigand_repetition_2018,
	title = {Repetition priming: {Is} music special?:},
	shorttitle = {Repetition priming},
	url = {https://journals.sagepub.com/doi/10.1080/02724980443000601},
	doi = {10.1080/02724980443000601},
	abstract = {Using short and long contexts, the present study investigated musical priming effects that are based on chord repetition and harmonic relatedness. A musical tar...},
	urldate = {2020-01-06},
	journal = {The Quarterly Journal of Experimental Psychology Section A},
	author = {Bigand, E. and Tillmann, B. and Poulin-Charronnat, B. and Manderlier, D.},
	month = jan,
	year = {2018},
}

@inproceedings{bock_madmom_2016,
	address = {Amsterdam, The Netherlands},
	title = {madmom: {A} new python audio and music signal processing library},
	isbn = {978-1-4503-3603-1},
	shorttitle = {madmom},
	url = {http://dl.acm.org/citation.cfm?doid=2964284.2973795},
	doi = {10.1145/2964284.2973795},
	abstract = {In this paper, we present madmom, an open-source audio processing and music information retrieval (MIR) library written in Python. madmom features a concise, NumPycompatible, object oriented design with simple calling conventions and sensible default values for all parameters, which facilitates fast prototyping of MIR applications. Prototypes can be seamlessly converted into callable processing pipelines through madmom’s concept of Processors, callable objects that run transparently on multiple cores. Processors can also be serialised, saved, and re-run to allow results to be easily reproduced anywhere.},
	urldate = {2020-03-08},
	booktitle = {Proceedings of the 2016 {ACM} on {Multimedia} {Conference} - {MM} '16},
	publisher = {ACM Press},
	author = {Böck, Sebastian and Korzeniowski, Filip and Schlüter, Jan and Krebs, Florian and Widmer, Gerhard},
	year = {2016},
	pages = {1174--1178},
}

@inproceedings{deville_2018_2018,
	address = {Cham},
	title = {The 2018 {Signal} {Separation} {Evaluation} {Campaign}},
	volume = {10891},
	isbn = {978-3-319-93763-2 978-3-319-93764-9},
	url = {http://link.springer.com/10.1007/978-3-319-93764-9%5F28},
	doi = {10.1007/978-3-319-93764-9_28},
	abstract = {This paper reports the organization and results for the 2018 community-based Signal Separation Evaluation Campaign (SiSEC 2018). This year’s edition was focused on audio and pursued the eﬀort towards scaling up and making it easier to prototype audio separation software in an era of machine-learning based systems. For this purpose, we prepared a new music separation database: MUSDB18, featuring close to 10 h of audio. Additionally, open-source software was released to automatically load, process and report performance on MUSDB18. Furthermore, a new oﬃcial Python version for the BSS Eval toolbox was released, along with reference implementations for three oracle separation methods: ideal binary mask, ideal ratio mask, and multichannel Wiener ﬁlter. We ﬁnally report the results obtained by the participants.},
	urldate = {2020-04-23},
	booktitle = {Latent {Variable} {Analysis} and {Signal} {Separation}},
	publisher = {Springer International Publishing},
	author = {Stöter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka},
	editor = {Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic},
	year = {2018},
	pages = {293--305},
}

@article{stoter_open-unmix_2019,
	title = {Open-unmix - a reference implementation for music source separation},
	volume = {4},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.01667},
	doi = {10.21105/joss.01667},
	abstract = {Music source separation is the task of decomposing music into its constitutive components, e.g., yielding separated stems for the vocals, bass, and drums. Such a separation has many applications ranging from rearranging/repurposing the stems (remixing, repanning, upmixing) to full extraction (karaoke, sample creation, audio restoration). Music separation has a long history of scientific activity as it is known to be a very challenging problem. In recent years, deep learning-based systems - for the first time - yielded high-quality separations that also lead to increased commercial interest. However, until now, no open-source implementation that achieves state-of-the-art results is available. Open-Unmix closes this gap by providing a reference implementation based on deep neural networks. It serves two main purposes. Firstly, to accelerate academic research as Open-Unmix provides implementations for the most popular deep learning frameworks, giving researchers a flexible way to reproduce results. Secondly, we provide a pre-trained model for end users and even artists to try and use source separation. Furthermore, we designed Open-Unmix to be one core component in an open ecosystem on music separation, where we already provide open datasets, software utilities, and open evaluation to foster reproducible research as the basis of future development.},
	number = {41},
	urldate = {2020-04-23},
	journal = {Journal of Open Source Software},
	author = {Stöter, Fabian-Robert and Uhlich, Stefan and Liutkus, Antoine and Mitsufuji, Yuki},
	month = sep,
	year = {2019},
	pages = {1667},
}

@article{mcfee_resampy_2016,
	title = {resampy: efficient sample rate conversion in {Python}},
	volume = {1},
	issn = {2475-9066},
	shorttitle = {resampy},
	url = {https://joss.theoj.org/papers/10.21105/joss.00125},
	doi = {10.21105/joss.00125},
	abstract = {McFee, (2016), resampy: efficient sample rate conversion in Python, Journal of Open Source Software, 1(8), 125, doi:10.21105/joss.00125},
	number = {8},
	urldate = {2020-04-23},
	journal = {Journal of Open Source Software},
	author = {McFee, Brian},
	month = dec,
	year = {2016},
	pages = {125},
}

@article{schellenberg_simplifying_1997,
	title = {Simplifying the implication-{Realization} model of melodic expectancy},
	volume = {14},
	issn = {0730-7829},
	url = {https://www.jstor.org/stable/40285723},
	doi = {10.2307/40285723},
	abstract = {Results from previous investigations indicate that the implication-realization (I-R) model (Narmour, 1990) of expectancy in melody may be overspecified and more complex than necessary. Indeed, Schellenberg's (1996) revised model, with two fewer predictor variables, improved predictive accuracy compared with the original model. A reanalysis of data reported by Cuddy and Lunney (1995) provided similar results. When the principles of the I-R model were submitted to a principal- components analysis, a solution containing three orthogonal (uncorrelated) factors retained the accuracy of the model but was inferior to the revised model. A separate principal-components analysis of the predictors of the revised model yielded a two-factor solution that did not compromise the revised model's predictive power. Consequently, an even simpler model of melodic expectancy was derived. These results provide further evidence that redundancy in the I-R model can be eliminated without loss of predictive accuracy.},
	number = {3},
	urldate = {2020-01-06},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Schellenberg, E. Glenn},
	year = {1997},
	pages = {295--318},
}

@incollection{zumbahlen_linear_nodate,
	title = {Linear {Circuit} {Design} {Handbook}},
	url = {http://www.analog.com/library/analogdialogue/archives/43-09/edch%208%20filter.pdf},
	author = {Zumbahlen, Hank},
	pages = {ch 8},
}

@inproceedings{davies_temporal_2019,
	address = {A Coruna, Spain},
	title = {Temporal convolutional networks for musical audio beat tracking},
	isbn = {978-90-827970-3-9},
	url = {https://ieeexplore.ieee.org/document/8902578/},
	doi = {10.23919/EUSIPCO.2019.8902578},
	abstract = {We propose the use of Temporal Convolutional Networks for audio-based beat tracking. By contrasting our convolutional approach with the current state-of-the-art recurrent approach using Bidirectional Long Short-Term Memory, we demonstrate three highly promising attributes of TCNs for music analysis, namely: i) they achieve state-of-the-art performance on a wide range of existing beat tracking datasets, ii) they are well suited to parallelisation and thus can be trained efﬁciently even on very large training data; and iii) they require a small number of weights.},
	urldate = {2020-02-17},
	booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Davies, Matthew E. P. and Bock, Sebastian},
	month = sep,
	year = {2019},
	pages = {1--5},
}

@misc{antoine_liutkus_sigsepnorbert_2019,
	title = {sigsep/norbert: {First} official {Norbert} release},
	shorttitle = {sigsep/norbert},
	url = {https://zenodo.org/record/3269749},
	abstract = {We are happy to finally release the first official pypi release of norbert.},
	urldate = {2020-04-23},
	publisher = {Zenodo},
	author = {Liutkus, Antoine and Stöter, Fabian-Robert},
	month = jul,
	year = {2019},
	doi = {10.5281/zenodo.3269749},
}

@article{woods_headphone_2017,
	title = {Headphone screening to facilitate web-based auditory experiments},
	volume = {79},
	issn = {1943-3921, 1943-393X},
	url = {http://link.springer.com/10.3758/s13414-017-1361-2},
	doi = {10.3758/s13414-017-1361-2},
	abstract = {Psychophysical experiments conducted remotely over the internet permit data collection from large numbers of participants but sacrifice control over sound presentation and therefore are not widely employed in hearing research. To help standardize online sound presentation, we introduce a brief psychophysical test for determining whether online experiment participants are wearing headphones. Listeners judge which of three pure tones is quietest, with one of the tones presented 180° out of phase across the stereo channels. This task is intended to be easy over headphones but difficult over loudspeakers due to phase-cancellation. We validated the test in the lab by testing listeners known to be wearing headphones or listening over loudspeakers. The screening test was effective and efficient, discriminating between the two modes of listening with a small number of trials. When run online, a bimodal distribution of scores was obtained, suggesting that some participants performed the task over loudspeakers despite instructions to use headphones. The ability to detect and screen out these participants mitigates concerns over sound quality for online experiments, a first step toward opening auditory perceptual research to the possibilities afforded by crowdsourcing.},
	number = {7},
	urldate = {2020-05-01},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Woods, Kevin J. P. and Siegel, Max H. and Traer, James and McDermott, Josh H.},
	month = oct,
	year = {2017},
	pages = {2064--2072},
}

@book{oppenheim_discrete-time_1999,
	title = {Discrete-time {Signal} {Processing}},
	isbn = {978-0-13-754920-7},
	abstract = {THE definitive, authoritative book on DSP – ideal for those with an introductory-level knowledge of signals and systems. Written by prominent, DSP pioneers, it provides thorough treatment of the fundamental theorems and properties of discrete-time linear systems, filtering, sampling, and discrete-time Fourier Analysis. By focusing on the general and universal concepts in discrete-time signal processing, it remains vital and relevant to the new challenges arising in the field – "without" limiting itself to specific technologies with relatively short life spans. FEATURES NEW–Provides a new chapter organization. NEW–Material on: Multi-rate filtering banks. The discrete cosine transform. Noise-shaping sampling strategies. NEW–Includes several dozen new problem-solving examples that not only illustrate key points, but demonstrate approaches to typical problems related to the material. NEW–Contains a wealth of "combat tested" problems which are the best produced over decades of undergraduate and graduate signal processing classes at MIT and Georgia Tech. NEW–Problems are completely reorganized by level of difficulty into separate categories: Basic Problems with Answers to allow the user to check their results, but not solutions (20 per chapter). Basic Problems – without answers. Advanced Problems. Extension Problems – start from the discussion in the book and lead the reader beyond to glimpse some advanced areas of signal processing. Covers the history of discrete-time signal processing as well as contemporary developments in the field. Discusses the wide range of present and future applications of the technology. Focuses on the general and universal concepts in discrete-time signal processing. Offers a wealth of problems and examples.},
	publisher = {Prentice Hall},
	author = {Oppenheim, Alan V. and Schafer, Ronald W. and Buck, John R.},
	year = {1999},
	keywords = {Technology \& Engineering / Electrical},
}

@inproceedings{morise_fast_2009,
	title = {Fast and reliable f0 estimation method based on the period extraction of vocal fold vibration of singing voice and speech},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=15165},
	abstract = {A fast and reliable fundamental frequency (F0) extraction method is proposed for real-time interactive applications using a singing voice. It is based on period detection of the vocal fold vibration, so it does not require expensive computation such as STFT or autocorrelation. Parallel processing architecture and a new cost function make this simple idea competitive with state of the art F0 estimation methods. A series of tests using publicly accessible F0 reference databases revealed that the...},
	urldate = {2020-04-23},
	publisher = {Audio Engineering Society},
	author = {Morise, Masanori and Kawahara, Hideki and Katayose, Haruhiro},
	month = feb,
	year = {2009},
}

@incollection{bock_maximum_2013,
	title = {Maximum {Filter} {Vibrato} {Suppression} for {Onset} {Detection}},
	abstract = {We present SuperFlux - a new onset detection algorithm with vibrato suppression. It is an enhanced version of the universal spectral ﬂux onset detection algorithm, and reduces the number of false positive detections considerably by tracking spectral trajectories with a maximum ﬁlter. Especially for music with heavy use of vibrato (e.g., sung operas or string performances), the number of false positive detections can be reduced by up to 60\% without missing any additional events. Algorithm performance was evaluated and compared to state-of-the-art methods on the basis of three different datasets comprising mixed audio material (25,927 onsets), violin recordings (7,677 onsets) and operatic solo voice recordings (1,448 onsets). Due to its causal nature, the algorithm is applicable in both ofﬂine and online real-time scenarios.},
	booktitle = {Proceedings of the 16th international conference on digital audio effects ({DAFx})},
	author = {Böck, Sebastian and Widmer, Gerhard},
	year = {2013},
	pages = {55--61},
}

@inproceedings{bogdanov_essentia_2013,
	address = {Barcelona, Spain},
	title = {{ESSENTIA}: an open-source library for sound and music analysis},
	isbn = {978-1-4503-2404-5},
	shorttitle = {{ESSENTIA}},
	url = {http://dl.acm.org/citation.cfm?doid=2502081.2502229},
	doi = {10.1145/2502081.2502229},
	abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Aﬀero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predeﬁned executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, speciﬁcally the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
	urldate = {2020-05-04},
	booktitle = {Proceedings of the 21st {ACM} international conference on {Multimedia} - {MM} '13},
	publisher = {ACM Press},
	author = {Bogdanov, Dmitry and Serra, Xavier and Wack, Nicolas and Gómez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, José},
	year = {2013},
	pages = {855--858},
}

@book{kouzis-loukas_learning_2016,
	title = {Learning {Scrapy}},
	publisher = {Packt Publishing Ltd},
	author = {Kouzis-Loukas, Dimitrios},
	year = {2016},
}

@book{bird_natural_2009,
	title = {Natural {Language} {Processing} with {Python}},
	publisher = {O’Reilly Media Inc.},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	year = {2009},
}

@article{moffat_perceptual_2018,
	title = {Perceptual {Evaluation} of {Synthesized} {Sound} {Effects}},
	volume = {15},
	issn = {15443558},
	url = {http://dl.acm.org/citation.cfm?doid=3190502.3165287},
	doi = {10.1145/3165287},
	number = {2},
	urldate = {2019-11-16},
	journal = {ACM Transactions on Applied Perception},
	author = {Moffat, David and Reiss, Joshua D.},
	month = apr,
	year = {2018},
	keywords = {unread},
	pages = {1--19},
}

@article{trosset_out--sample_2008,
	title = {The out-of-sample problem for classical multidimensional scaling},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947308001515},
	doi = {10.1016/j.csda.2008.02.031},
	abstract = {Out-of-sample embedding techniques insert additional points into previously constructed configurations. An out-of-sample extension of classical multidimensional scaling is presented. The out-of-sample extension is formulated as an unconstrained nonlinear least-squares problem. The objective function is a fourth-order polynomial, easily minimized by standard gradient-based methods for numerical optimization. Two examples are presented.},
	number = {10},
	urldate = {2019-12-02},
	journal = {Computational Statistics \& Data Analysis},
	author = {Trosset, Michael W. and Priebe, Carey E.},
	month = jun,
	year = {2008},
	keywords = {unread},
	pages = {4635--4642},
}

@inproceedings{smith_nonnegative_2018,
	address = {Calgary, AB},
	title = {Nonnegative tensor factorization for source separation of loops in audio},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8461876/},
	doi = {10.1109/ICASSP.2018.8461876},
	abstract = {The prevalence of exact repetition in loop-based music makes it an opportune target for source separation. Nonnegative factorization approaches have been used to model the repetition of looped content, and kernel additive modeling has leveraged periodicity within a piece to separate looped background elements. We propose a novel method of leveraging periodicity in a factorization model: we treat the two-dimensional spectrogram as a three-dimensional tensor, and use nonnegative tensor factorization to estimate the component spectral templates, rhythms and loop recurrences in a single step. Testing our method on synthesized loop-based examples, we ﬁnd that our algorithm mostly exceeds the performance of competing methods, with a reduction in execution cost. We discuss limitations of the algorithm as we demonstrate its potential to analyze larger and more complex songs.},
	urldate = {2019-12-17},
	booktitle = {2018 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Smith, Jordan B. L. and Goto, Masataka},
	month = apr,
	year = {2018},
	keywords = {source separation, tensor factorisation, unread},
	pages = {171--175},
}

@incollection{reuter_colourful_2017,
	address = {New York, NY, US},
	series = {{SEMPRE} studies in the psychology of music},
	title = {The colourful life of timbre spaces: {Timbre} concepts from early ideas to meta-timbre space and beyond},
	isbn = {978-1-4724-8540-3 978-1-315-56962-8},
	shorttitle = {The colourful life of timbre spaces},
	abstract = {In orchestration treatises, musical features like pitch, instrument registers and timbre are often described with an idea of spatiality. They are perceived in spatial dimensions such as high versus low, thin versus wide, full versus empty. Timbre similarity has also been mostly described with a near-versus-distance-paradigm. So it stands to reason that for musicians the idea of timbre is commonly located in an imaginary two- or three-dimensional space. In orchestration treatises, musical features like pitch, instrument registers and timbre are often described with an idea of spatiality. They are perceived in spatial dimensions such as high versus low, thin versus wide, full versus empty. Timbre similarity has also been mostly described with a near-versus-distance-paradigm. So it stands to reason that for musicians the idea of timbre is commonly located in an imaginary two- or three-dimensional space. As early as the end of the 19th century, the multidimensional aspects of timbre came under the attention of psychologists and musicologists with the works of Carl Stumpf . In 1939 a first three-dimensional attempt at timbre description was published by Gerhard Albersheim and the empirically ascertained, so-called 'timbre spaces' followed in the 1970s. Before dealing with the pro and cons of actual known timbre spaces it is helpful to take a look at the history of timbre concepts, starting with the well-known simple questions about timbre: What is timbre? How is it quantifiable? Where does the concept of musical timbre come from? 'Why is musical timbre so hard to understand?; and which paradigms move(d) ideas in the discussion about the phenomenon of musical timbre? (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
	booktitle = {Body, sound and space in music and beyond: {Multimodal} explorations},
	publisher = {Routledge/Taylor \& Francis Group},
	author = {Reuter, Christoph and Siddiq, Saleh},
	year = {2017},
	keywords = {Concepts, Music, Spatial Imagery, unread},
	pages = {150--167},
}

@article{koren_matrix_2009,
	title = {Matrix {Factorization} {Techniques} for {Recommender} {Systems}},
	volume = {42},
	issn = {1558-0814},
	doi = {10.1109/MC.2009.263},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
	number = {8},
	journal = {Computer},
	author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	month = aug,
	year = {2009},
	keywords = {Bioinformatics, Collaboration, Computational intelligence, Filtering, Genomics, Matrix factorization, Motion pictures, Nearest neighbor searches, Netflix Prize, Netflix Prize competition, Predictive models, Recommender systems, Sea measurements, information filtering, matrix decomposition, matrix factorization technique, nearest neighbor technique, product recommendation system, recommender system, retail data processing, unread},
	pages = {30--37},
}

@article{bhattacharjee_statistical_2016,
	title = {A statistical analysis on the impact of noise on {MFCC} features for speech recognition},
	doi = {10.1109/ICRAIE.2016.7939548},
	abstract = {Noise is omnipresent in almost all acoustical environments. The investigation presents here seeks to quantify the impact of noise on mel-frequency cepstral coefficients (MFCC) of speech signal. MFCC is one of the most commonly used features for speech recognition systems. However, it has been observed that performance of MFCC based system degrades drastically with changing noise levels and noise types. In the present study, different noise types at different levels have been added to the clean speech signal and the changes in statistical distribution pattern of the signal has been investigated. Further, performance of two commonly used noise normalization techniques Cepstral Mean and Variance Normalization (CMVN) and Spectral Subtraction (SS) have also been evaluated.},
	journal = {2016 International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
	author = {Bhattacharjee, Utpal and Gogoi, Swapnanil and Sharma, Rakesh},
	year = {2016},
	keywords = {Cepstral Mean and Variance Normalization, Coefficient, Database normalization, Hearing Loss, High-Frequency, Mel-frequency cepstrum, Speech recognition, Subtraction Technique, unread},
	pages = {1--5},
}

@inproceedings{pearce_timbral_2017,
	title = {Timbral {Attributes} for {Sound} {Effect} {Library} {Searching}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=18754},
	abstract = {To improve the search functionality of online sound effect libraries, timbral information could be extracted using perceptual models, and added as metadata, allowing users to filter results by timbral characteristics. This paper identifies the timbral attributes that end-users commonly search for, to indicate the attributes that might usefully be modelled for automatic metadata generation. A literature review revealed 1187 descriptors that were subsequently reduced to a hierarchy of 145...},
	urldate = {2020-04-17},
	publisher = {Audio Engineering Society},
	author = {Pearce, Andy and Brookes, Tim and Mason, Russell},
	month = jun,
	year = {2017},
	keywords = {unread},
}

@article{eronen_comparison_2001,
	title = {Comparison of features for musical instrument recognition},
	doi = {10.1109/ASPAA.2001.969532},
	abstract = {Several features were compared with regard to recognition performance in a musical instrument recognition system. Both mel-frequency and linear prediction cepstral and delta cepstral coefficients were calculated. Linear prediction analysis was carried out both on a uniform and a warped frequency scale, and reflection coefficients were also used as features. The performance of earlier described features relating to the temporal development, modulation properties, brightness, and spectral synchronity of sounds was also analysed. The data base consisted of 5286 acoustic and synthetic solo tones from 29 different Western orchestral instruments, out of which 16 instruments were included in the test set. The best performance for solo tone recognition, 35\% for individual instruments and 77\% for families, was obtained with a feature set consisting of two sets of mel-frequency cepstral coefficients and a subset of the other analysed features. The confusions made by the system were analysed and compared to results reported in a human perception experiment.},
	journal = {Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575)},
	author = {Eronen, Antti J.},
	year = {2001},
	keywords = {Acoustic cryptanalysis, Algorithm, Coefficient, Computational complexity theory, Database, Mel-frequency cepstrum, Modulation, Structure of observed learning outcome, Synchronicity, Synthetic intelligence, Test set, Warped linear predictive coding, unread},
	pages = {19--22},
}

@inproceedings{bammer_invariance_2017,
	title = {Invariance and stability of {Gabor} scattering for music signals},
	doi = {10.1109/SAMPTA.2017.8024444},
	abstract = {A feature extractor based on Gabor frames and Mallat's scattering transform is introduced. The resulting Gabor scattering is applied to a simple model for audio signals in order to study invariance properties and deformation stability. In particular, it is shown that different layers create invariance to certain signal features. The decoupling technique previously used to investigate deformation stability of scattering transforms for Cartoon functions is applied to investigate to which extent the feature extractor is robust to changes in spectral shape and frequency modulation. The results are illustrated by numerical examples.},
	booktitle = {2017 international conference on sampling theory and applications ({SampTA})},
	author = {Bammer, Roswitha and Dörfler, Monika},
	month = jul,
	year = {2017},
	keywords = {Atomic layer deposition, Cartoon functions, Feature extraction, Gabor filters, Gabor frames, Gabor scattering invariance, Gabor scattering stability, Harmonic analysis, Mallat scattering transform, Numerical stability, Scattering, Stability analysis, Transforms, decoupling technique, deformation stability, feature extraction, feature extractor, frequency modulation, music, music signals, spectral analysis, spectral shape, transforms, unread},
	pages = {299--302},
}

@article{pearce_modelling_2019,
	title = {Modelling {Timbral} {Hardness}},
	volume = {9},
	url = {https://www.mdpi.com/2076-3417/9/3/466},
	doi = {10.3390/app9030466},
	abstract = {Hardness is the most commonly searched timbral attribute within freesound.org, a commonly used online sound effect repository. A perceptual model of hardness was developed to enable the automatic generation of metadata to facilitate hardness-based filtering or sorting of search results. A training dataset was collected of 202 stimuli with 32 sound source types, and perceived hardness was assessed by a panel of listeners. A multilinear regression model was developed on six features: maximum bandwidth, attack centroid, midband level, percussive-to-harmonic ratio, onset strength, and log attack time. This model predicted the hardness of the training data with R 2 = 0.76. It predicted hardness within a new dataset with R 2 = 0.57, and predicted the rank order of individual sources perfectly, after accounting for the subjective variance of the ratings. Its performance exceeded that of human listeners.},
	number = {3},
	urldate = {2020-04-17},
	journal = {Applied Sciences},
	author = {Pearce, Andy and Brookes, Tim and Mason, Russell},
	month = jan,
	year = {2019},
	keywords = {artificial intelligence, audio coding, modelling, music information retrieval, perception, psychoacoustics, sound quality, sound recording, timbre, unread},
	pages = {466},
}

@article{siedenburg_comparison_2016,
	title = {A comparison of approaches to timbre descriptors in music information retrieval and music psychology},
	volume = {45},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1132737},
	doi = {10.1080/09298215.2015.1132737},
	abstract = {A curious divide characterizes the usage of audio descriptors for timbre research in music information research (MIR) and music psychology. While MIR uses a multitude of audio descriptors for tasks such as automatic instrument classiﬁcation, only a highly constrained set is used to describe the physical correlates of timbre perception in parts of music psychology. We argue that this gap is not coincidental and results from the differences in the two ﬁelds’ methodologies, their epistemic groundwork, and research goals. This paper lays out perspectives on the emergence of the divide and reviews studies in both ﬁelds with regards to divergences in research methods and goals. We discuss new representations for spectro-temporal modulations in MIR and psychology, and compare approaches to spectral envelope description in depth. Finally, we will propose that the interdisciplinary discourse on the computational modelling of music requires negotiations about the roles of scientiﬁc evaluation criteria.},
	number = {1},
	urldate = {2020-06-08},
	journal = {Journal of New Music Research},
	author = {Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
	month = jan,
	year = {2016},
	keywords = {unread},
	pages = {27--41},
}

@article{siedenburg_modeling_2017,
	title = {Modeling {Timbre} {Similarity} of {Short} {Music} {Clips}},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00639/full},
	doi = {10.3389/fpsyg.2017.00639},
	abstract = {There is evidence from a number of recent studies that most listeners are able to extract information related to song identity, emotion, or genre from music excerpts with durations in the range of tenths of seconds. Because of these very short durations, timbre as a multifaceted auditory attribute appears as a plausible candidate for the type of features that listeners make use of when processing short music excerpts. However, the importance of timbre in listening tasks that involve short excerpts has not yet been demonstrated empirically. Hence, the goal of this study was to develop a method that allows to explore to what degree similarity judgements of short music clips can be modelled with low-level acoustic features related to timbre. We utilized the similarity data from two large samples of participants: Sample I was obtained via an online survey, used 16 clips of 400 ms length, and contained responses of 137,339 participants. Sample II was collected in a lab environment, used 16 clips of 800 ms length, and contained responses from 648 participants. Our model used two sets of audio features which included commonly used timbre descriptors and the well-known Mel-frequency cepstral coefficients as well as their temporal derivates. In order to predict pairwise similarities, the resulting distances between clips in terms of their audio features were used as predictor variables with partial least-squares regression. We found that a sparse selection of three to seven features from both descriptor sets—mainly encoding the coarse shape of the spectrum as well as spectrotemporal variability—best predicted similarities across the two sets of sounds. Notably, the inclusion of non-acoustic predictors of musical genre and record release date allowed much better generalization performance and explained up to 50\% of shared variance (Rˆ2) between observations and model predictions. Overall, the results of this study empirically demonstrate that both acoustic features related to timbre as well as higher level categorical features such as musical genre play a major role in the perception of short music clips.},
	urldate = {2020-06-08},
	journal = {Frontiers in Psychology},
	author = {Siedenburg, Kai and Müllensiefen, Daniel},
	year = {2017},
	keywords = {Audio features, Music similarity, genre, short audio clips, timbre, unread},
}

@article{malhotra_robustness_1988,
	title = {The robustness of {MDS} configurations in the case of incomplete data},
	volume = {25},
	issn = {0022-2437},
	url = {www.jstor.org/stable/3172929},
	doi = {10.2307/3172929},
	abstract = {The authors examine the robustness of MDS configurations when incomplete rather than complete input data are used. Using two empirical studies, they show that robustness varies as the amount of incomplete data increases and that random methods of data deletion perform as well as cyclic designs. These findings provide empirical support for earlier Monté Carlo literature on the topic. The authors also show that individual characteristics of respondents, namely cognitive integration and imagery, influence the quality of configurations obtained with incomplete data.},
	number = {1},
	urldate = {2019-12-02},
	journal = {Journal of Marketing Research},
	author = {Malhotra, Naresh K. and Jain, Arun K. and Pinson, Christian},
	year = {1988},
	keywords = {unread},
	pages = {95--102},
}

@article{peeters_sound_2010,
	title = {Sound {Indexing} {Using} {Morphological} {Description}},
	volume = {18},
	issn = {1558-7924},
	doi = {10.1109/TASL.2009.2038809},
	abstract = {Sound sample indexing usually deals with the recognition of the source/cause that has produced the sound. For abstract sounds, sound effects, unnatural, or synthetic sounds, this cause is usually unknown or unrecognizable. An efficient description of these sounds has been proposed by Schaeffer under the name morphological description. Part of this description consists in describing a sound by identifying the temporal evolution of its acoustic properties to a set of profiles. In this paper, we consider three morphological descriptions: dynamic profiles (ascending, descending, ascending/descending, stable, impulsive), melodic profiles (up, down, stable, up/down, down/up) and complex-iterative sound description (non-iterative, iterative, grain, repetition). We study the automatic indexing of a sound into these profiles. Because this automatic indexing is difficult using standard audio features, we propose new audio features to perform this task. The dynamic profiles are estimated by modeling the loudness over-time of a sound by a second-order B-spline model and derive features from this model. The melodic profiles are estimated by tracking over time the perceptual filter which has the maximum excitation. A function is derived from this track which is then modeled using a second-order B-spline model. The features are again derived from the B-spline model. The description of complex-iterative sounds is obtained by estimating the amount of repetition and the period of the repetition. These are obtained by computing an audio similarity function derived from an Mel frequency cepstral coefficients (MFCC) similarity matrix. The proposed audio features are then tested for automatic classification. We consider three classification tasks corresponding to the three profiles. In each case, the results are compared with the ones obtained using standard audio features.},
	number = {3},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Peeters, Geoffroy and Deruty, Emmanuel},
	month = mar,
	year = {2010},
	keywords = {Audio features, Automatic testing, Filters, Instruments, Machine assisted indexing, Mel frequency cepstral coefficient, Search engines, Spline, Timbre, Trademarks, Usability, audio signal processing, audio similarity, automatic indexing, complex-iterative sound description, dynamic profiles, frequency cepstral coefficients, loudness, melodic profiles, morphological description, second-order B-spline model, sound description, sound indexing, splines (mathematics)},
	pages = {675--687},
}

@article{caclin_acoustic_2005,
	title = {Acoustic correlates of timbre space dimensions: a confirmatory study using synthetic tones.},
	volume = {118},
	shorttitle = {Acoustic correlates of timbre space dimensions},
	doi = {10.1121/1.1929229},
	abstract = {Timbre spaces represent the organization of perceptual distances, as measured with dissimilarity ratings, among tones equated for pitch, loudness, and perceived duration. A number of potential acoustic correlates of timbre-space dimensions have been proposed in the psychoacoustic literature, including attack time, spectral centroid, spectral flux, and spectrum fine structure. The experiments reported here were designed as direct tests of the perceptual relevance of these acoustical parameters for timbre dissimilarity judgments. Listeners presented with carefully controlled synthetic tones use attack time, spectral centroid, and spectrum fine structure in dissimilarity rating experiments. These parameters thus appear as major determinants of timbre. However, spectral flux appears as a less salient timbre parameter, its salience depending on the number of other dimensions varying concurrently in the stimulus set. Dissimilarity ratings were analyzed with two different multidimensional scaling models (CLASCAL and CONSCAL), the latter providing psychophysical functions constrained by the physical parameters. Their complementarity is discussed.},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Caclin, Anne and McAdams, Stephen and Smith, Bennett K. and Winsberg, Suzanne},
	year = {2005},
	keywords = {Dimensions, Judgment, Population Parameter, Psychoacoustics, to read this week},
	pages = {471--482},
}

@article{leeuw_multidimensional_2009,
	title = {Multidimensional scaling using majorization: {SMACOF} in \textit{{R}}},
	volume = {31},
	issn = {1548-7660},
	shorttitle = {Multidimensional {Scaling} {Using} {Majorization}},
	url = {http://www.jstatsoft.org/v31/i03/},
	doi = {10.18637/jss.v031.i03},
	abstract = {In this paper we present the methodology of multidimensional scaling problems (MDS) solved by means of the majorization algorithm. The objective function to be minimized is known as stress and functions which majorize stress are elaborated. This strategy to solve MDS problems is called SMACOF and it is implemented in an R package of the same name which is presented in this article. We extend the basic SMACOF theory in terms of conﬁguration constraints, three-way data, unfolding models, and projection of the resulting conﬁgurations onto spheres and other quadratic surfaces. Various examples are presented to show the possibilities of the SMACOF approach oﬀered by the corresponding package.},
	number = {3},
	urldate = {2020-06-11},
	journal = {Journal of Statistical Software},
	author = {Leeuw, Jan de and Mair, Patrick},
	year = {2009},
	keywords = {unread},
}

@phdthesis{seago_new_nodate,
	type = {phd},
	title = {A new user interface for musical timbre design},
	abstract = {This thesis characterises and addresses problems and issues associated with the design of intuitive user interfaces for timbral control. The usability of a range of synthesis methods and representative implementations of these methods is assessed, and three interface architectures - fixed architecture, architecture specification and direct specification - are identified. The characteristics of each of these architectures, as well as problems of usability inherent to each of them are discussed; it is argued that none of them provide intuitive tools for the manipulation and control of timbre.},
	author = {Seago, Allan},
}

@article{peeters_timbre_2011,
	title = {The {Timbre} {Toolbox}: {Extracting} audio descriptors from musical signals},
	volume = {130},
	issn = {0001-4966},
	shorttitle = {The {Timbre} {Toolbox}},
	url = {http://asa.scitation.org/doi/10.1121/1.3642604},
	doi = {10.1121/1.3642604},
	abstract = {The analysis of musical signals to extract audio descriptors that can potentially characterize their timbre has been disparate and often too focused on a particular small set of sounds. The Timbre Toolbox provides a comprehensive set of descriptors that can be useful in perceptual research, as well as in music information retrieval and machine-learning approaches to content-based retrieval in large sound databases. Sound events are first analyzed in terms of various input representations (short-term Fourier transform, harmonic sinusoidal components, an auditory model based on the equivalent rectangular bandwidth concept, the energy envelope). A large number of audio descriptors are then derived from each of these representations to capture temporal, spectral, spectrotemporal, and energetic properties of the sound events. Some descriptors are global, providing a single value for the whole sound event, whereas others are time-varying. Robust descriptive statistics are used to characterize the time-varying descriptors. To examine the information redundancy across audio descriptors, correlational analysis followed by hierarchical clustering is performed. This analysis suggests ten classes of relatively independent audio descriptors, showing that the Timbre Toolbox is a multidimensional instrument for the measurement of the acoustical structure of complex sound signals.},
	number = {5},
	urldate = {2020-06-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
	month = nov,
	year = {2011},
	pages = {2902--2916},
}

@article{zhang_single_2019,
	title = {A single item measure for identifying musician and nonmusician categories based on measures of musical sophistication},
	volume = {36},
	issn = {0730-7829, 1533-8312},
	url = {http://mp.ucpress.edu/lookup/doi/10.1525/mp.2019.36.5.457},
	doi = {10.1525/mp.2019.36.5.457},
	number = {5},
	urldate = {2020-06-26},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Zhang, J. Diana and Schubert, Emery},
	month = jun,
	year = {2019},
	pages = {457--467},
}

@incollection{jori_discourse_2020,
	address = {Cham},
	series = {Music {Business} {Research}},
	title = {The discourse community of electronic dance music through the example of the {TB}-303 owners club},
	isbn = {978-3-030-39002-0},
	url = {https://doi.org/10.1007/978-3-030-39002-0%5F10},
	abstract = {The aim of this chapter is to introduce the model of the discourse community of electronic dance music using a case study of the Facebook group TB-303 Owners Club. This model is based on the discourse community theories of John M. Swales and the analytical system of computer-mediated discourse analysis put forward by Susan C. Herring. The methodology presented in this study is new in research on electronic dance music cultures. It can help us to understand the fine details about online community building and social identity construction via language use. Here, discourse is seen as language in use and as a tool for expressing identity. Therefore, different examples are examined of how identity construction works through discourse and how discourse influences these mechanisms. Furthermore, the results of this study add up-to-date information to the research on computer-mediated communication, with a special focus on the communication processes of social media.},
	urldate = {2020-06-27},
	booktitle = {The {New} {Age} of {Electronic} {Dance} {Music} and {Club} {Culture}},
	publisher = {Springer International Publishing},
	author = {Jóri, Anita},
	editor = {Jóri, Anita and Lücke, Martin},
	year = {2020},
	doi = {10.1007/978-3-030-39002-0_10},
	keywords = {Computer-mediated discourse analysis, Electronic dance music, Online communication, Roland TB-303, Virtual community},
	pages = {117--131},
}

@article{mallat_understanding_2016,
	title = {Understanding deep convolutional networks},
	volume = {374},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0203},
	doi = {10.1098/rsta.2015.0203},
	number = {2065},
	urldate = {2020-08-03},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Mallat, Stéphane},
	month = apr,
	year = {2016},
	pages = {20150203},
}

@incollection{holland_new_2013,
	address = {London},
	title = {A {New} {Interaction} {Strategy} for {Musical} {Timbre} {Design}},
	isbn = {978-1-4471-2989-9 978-1-4471-2990-5},
	url = {http://link.springer.com/10.1007/978-1-4471-2990-5%5F9},
	abstract = {Sound creation and editing in hardware and software synthesizers presents usability problems and a challenge for HCI research. Synthesis parameters vary considerably in their degree of usability, and musical timbre itself is a complex and multidimensional attribute of sound. This chapter presents a user-driven searchbased interaction style where the user engages directly with sound rather than with a mediating interface layer. Where the parameters of a given sound synthesis method do not readily map to perceptible sonic attributes, the search algorithm offers an alternative means of timbre speciﬁcation and control. However, it is argued here that the method has wider relevance for interaction design in search domains which are generally well-ordered and understood, but whose parameters do not afford a useful or intuitive means of search.},
	urldate = {2020-06-17},
	booktitle = {Music and {Human}-{Computer} {Interaction}},
	publisher = {Springer London},
	author = {Seago, Allan},
	editor = {Holland, Simon and Wilkie, Katie and Mulholland, Paul and Seago, Allan},
	year = {2013},
	doi = {10.1007/978-1-4471-2990-5_9},
	pages = {153--169},
}

@inproceedings{humphrey_non-linear_2011,
	title = {Non-linear semantic embedding for organizing large instrument sample libraries},
	volume = {2},
	doi = {10.1109/ICMLA.2011.105},
	abstract = {Though tags and metadata may provide rich indicators of relationships between high-level concepts like songs, artists or even genres, verbal descriptors lack the fine-grained detail necessary to capture acoustic nuances necessary for efficient retrieval of sounds in extremely large sample libraries. To these ends, we present a flexible approach titled Non-linear Semantic Embedding (NLSE), capable of projecting high-dimensional time-frequency representations of musical instrument samples into a low-dimensional, semantically-organized metric space. As opposed to other dimensionality reduction techniques, NLSE incorporates extrinsic semantic information in learning a projection, automatically learns salient acoustic features, and generates an intuitively meaningful output space.},
	booktitle = {2011 10th international conference on machine learning and applications and workshops},
	author = {Humphrey, Eric J. and Glennon, Aron P. and Bello, Juan Pablo},
	month = dec,
	year = {2011},
	keywords = {Aerospace electronics, Convolution, Instruments, NLSE, Organizations, Principal component analysis, Semantics, Training, acoustic nuances, acoustic signal processing, digital libraries, dimensionality reduction techniques, extrinsic semantic information, fine-grained detail, high-dimensional time-frequency representations, high-level concepts, information retrieval, large instrument sample library, large sample library, low-dimensional metric space, meta data, metadata, musical instrument samples, musical instruments, nonlinear semantic embedding, output space, salient acoustic features, semantically-organized metric space, sound retrieval, time-frequency analysis, verbal descriptors},
	pages = {142--147},
}

@article{singh_modulation_2003,
	title = {Modulation spectra of natural sounds and ethological theories of auditory processing},
	volume = {114},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1624067},
	doi = {10.1121/1.1624067},
	number = {6},
	urldate = {2020-08-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Singh, Nandini C. and Theunissen, Frédéric E.},
	month = dec,
	year = {2003},
	pages = {3394--3411},
}

@techreport{itu-r_bs1770-4_algorithms_2015,
	title = {Algorithms to measure audio programme loudness and true-peak audio level},
	url = {https://www.itu.int/dms%5Fpubrec/itu-r/rec/bs/R-REC-BS.1770-4-201510-I!!PDF-E.pdf},
	institution = {ITU-R},
	author = {{ITU-R BS.1770-4}},
	year = {2015},
	pages = {25},
}

@article{lorenzo-seva_tuckers_2006,
	title = {Tucker's congruence coefficient as a meaningful index of factor similarity},
	volume = {2},
	issn = {1614-2241(Electronic),1614-1881(Print)},
	doi = {10.1027/1614-2241.2.2.57},
	abstract = {When Tucker's congruence coefficient is used to assess the similarity of factor interpretations, it is desirable to have a critical congruence level less than unity that can be regarded as indicative of identity of the factors. The literature only reports rules of thumb. The present article repeats and broadens the approach used in the study by Haven and ten Berge (1977). It aims to find a critical congruence level on the basis of judgments of factor similarity by practitioners of factor analysis. Our results suggest that a value in the range .85-.94 corresponds to a fair similarity, while a value higher than .95 implies that the two factors or components compared can be considered equal. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
	author = {Lorenzo-Seva, Urbano and ten Berge, Jos M. F.},
	year = {2006},
	keywords = {Exploratory Factor Analysis, Factor Analysis, Factor Structure, Principal Component Analysis, Statistical Correlation},
	pages = {57--64},
}

@article{smilde_matrix_2009,
	title = {Matrix correlations for high-dimensional data: the modified {RV}-coefficient},
	volume = {25},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Matrix correlations for high-dimensional data},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btn634},
	doi = {10.1093/bioinformatics/btn634},
	abstract = {Motivation: Modern functional genomics generates high-dimensional data sets. It is often convenient to have a single simple number characterizing the relationship between pairs of such high-dimensional data sets in a comprehensive way. Matrix correlations are such numbers and are appealing since they can be interpreted in the same way as Pearson’s correlations familiar to biologists. The high-dimensionality of functional genomics data is, however, problematic for existing matrix correlations. The motivation of this paper is twofold: i) we introduce the idea of matrix correlations to the bioinformatics community and ii) we give an improvement of the most promising matrix correlation coefﬁcient (the RV-coefﬁcient) circumventing the problems of high-dimensional data.},
	number = {3},
	urldate = {2020-08-27},
	journal = {Bioinformatics (Oxford, England)},
	author = {Smilde, A. K. and Kiers, H. A. L. and Bijlsma, S. and Rubingh, C. M. and van Erk, M. J.},
	month = feb,
	year = {2009},
	pages = {401--405},
}

@incollection{plomp_timbre_1970,
	title = {Timbre as a {Multidimensional} {Attribute} of {Complex} {Tones}},
	booktitle = {Frequency {Analysis} and {Periodicity} {Detection} in {Hearing}},
	author = {Plomp, Reinier},
	year = {1970},
	keywords = {unread},
	pages = {397--410},
}

@article{stevens_direct_1956,
	title = {The {Direct} {Estimation} of {Sensory} {Magnitudes}: {Loudness}},
	volume = {69},
	issn = {0002-9556},
	shorttitle = {The {Direct} {Estimation} of {Sensory} {Magnitudes}},
	url = {https://www.jstor.org/stable/1418112},
	doi = {10.2307/1418112},
	number = {1},
	urldate = {2020-08-20},
	journal = {The American Journal of Psychology},
	author = {Stevens, S. S.},
	year = {1956},
	pages = {1--25},
}

@article{thoret_perceptually_2017,
	title = {Perceptually salient regions of the modulation power spectrum for musical instrument identification},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5390014/},
	doi = {10.3389/fpsyg.2017.00587},
	abstract = {The ability of a listener to recognize sound sources, and in particular musical instruments from the sounds they produce, raises the question of determining the acoustical information used to achieve such a task. It is now well known that the shapes of the temporal and spectral envelopes are crucial to the recognition of a musical instrument. More recently, Modulation Power Spectra (MPS) have been shown to be a representation that potentially explains the perception of musical instrument sounds. Nevertheless, the question of which specific regions of this representation characterize a musical instrument is still open. An identification task was applied to two subsets of musical instruments: tuba, trombone, cello, saxophone, and clarinet on the one hand, and marimba, vibraphone, guitar, harp, and viola pizzicato on the other. The sounds were processed with filtered spectrotemporal modulations with 2D Gaussian windows. The most relevant regions of this representation for instrument identification were determined for each instrument and reveal the regions essential for their identification. The method used here is based on a “molecular approach,” the so-called bubbles method. Globally, the instruments were correctly identified and the lower values of spectrotemporal modulations are the most important regions of the MPS for recognizing instruments. Interestingly, instruments that were confused with each other led to non-overlapping regions and were confused when they were filtered in the most salient region of the other instrument. These results suggest that musical instrument timbres are characterized by specific spectrotemporal modulations, information which could contribute to music information retrieval tasks such as automatic source recognition.},
	urldate = {2020-08-22},
	journal = {Frontiers in Psychology},
	author = {Thoret, Etienne and Depalle, Philippe and McAdams, Stephen},
	month = apr,
	year = {2017},
	pmid = {28450846},
	note = {tex.pmcid: PMC5390014},
}

@mastersthesis{brosnahan_bridging_2020,
	title = {Bridging the gap: {Generating} an intuitive synthesizer control space},
	school = {Queen Mary University of London},
	author = {Brosnahan, Luke},
	year = {2020},
}

@inproceedings{ananthabhotla_towards_2019,
	address = {Nice France},
	title = {Towards a perceptual loss: {Using} a neural network codec approximation as a loss for generative audio models},
	isbn = {978-1-4503-6889-6},
	shorttitle = {Towards a {Perceptual} {Loss}},
	url = {https://dl.acm.org/doi/10.1145/3343031.3351148},
	doi = {10.1145/3343031.3351148},
	abstract = {Generative audio models based on neural networks have led to considerable improvements across fields including speech enhancement, source separation, and text-to-speech synthesis. These systems are typically trained in a supervised fashion using simple element-wise ℓ1 or ℓ2 losses. However, because they do not capture properties of the human auditory system, such losses encourage modelling perceptually meaningless aspects of the output, wasting capacity and limiting performance. Additionally, while adversarial models have been employed to encourage outputs that are statistically indistinguishable from ground truth and have resulted in improvements in this regard, such losses do not need to explicitly model perception as their task; furthermore, training adversarial networks remains an unstable and slow process. In this work, we investigate an idea fundamentally rooted in psychoacoustics. We train a neural network to emulate an MP3 codec as a differentiable function. Feeding the output of a generative model through this MP3 function, we remove signal components that are perceptually irrelevant before computing a loss. To further stabilize gradient propagation, we employ intermediate layer outputs to define our loss, as found useful in image domain methods. Our experiments using an autoencoding task show an improvement over standard losses in listening tests, indicating the potential of psychoacoustically motivated models for audio generation.},
	urldate = {2020-09-30},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Ananthabhotla, Ishwarya and Ewert, Sebastian and Paradiso, Joseph A.},
	month = oct,
	year = {2019},
	pages = {1518--1525},
}

@article{noauthor_conditional_2016,
	title = {Conditional {Similarity} {Networks}},
	url = {https://www.arxiv-vanity.com/papers/1603.07810/},
	abstract = {We propose Multi-Query Networks to answer questions like “Find a shoe precisely like this, but with higher heel“. To respond to a question like this, one needs an image representation that captures all the different notions of similarities that shoes can be compared to. However, when learning such similarity based embeddings with siamese or triplet networks the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. A main reason for this is that contradicting notions of similarities cannot be captured in a single space. To address this shortcoming, we propose Multi-Query Networks (MQNs) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. In addition, MQNs make the representation interpretable by encoding different similarities in separate dimensions. We show that our approach learns visually relevant semantic subspaces. Further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.},
	urldate = {2020-10-13},
	month = mar,
	year = {2016},
}

@book{weidenaar_magic_1995,
	title = {Magic {Music} from the {Telharmonium}},
	isbn = {978-0-8108-2692-2},
	abstract = {It was 1906. "Get Music on Tap Like Gas or Water" promised the headlines, and soon the public was enchanted with inventor Thaddeus Cahill's (1867-1934) electrical music by wire. The Telharmonium was a 200-ton behemoth that created numerous musical timbres and could flood rooms with sound. It was installed at Telharmonic Hall in New York, across from the Metropolitan Opera. People thronged to attend four daily concerts there, listening as the music poured from huge horns attached to telephone receivers. Beginning with the first wire transmission of music in 1851, covering the three instruments that were constructed, and ending with efforts to find a home for the only surviving Telharmonium in 1951, this definitive account of the first music synthesizer will interest anyone concerned with the roots of electronic and computer music. The extensive bibliography is a valuable source for historians of musical instruments and cable broadcasting, as well as for technologists and students of American culture. With 47 photographs and illustrations.},
	publisher = {Reynold Weidenaar},
	author = {Weidenaar, Reynold},
	year = {1995},
	keywords = {Music / Musical Instruments / Woodwinds},
}

@techreport{itu-t_recommendation_1988,
	title = {Recommendation {G}. 711},
	institution = {ITU-T},
	author = {{ITU-T}},
	year = {1988},
}

@inproceedings{gonog_review_2019,
	title = {A {Review}: {Generative} {Adversarial} {Networks}},
	shorttitle = {A {Review}},
	doi = {10.1109/ICIEA.2019.8833686},
	abstract = {Deep learning has achieved great success in the field of artificial intelligence, and many deep learning models have been developed. Generative Adversarial Networks (GAN) is one of the deep learning model, which was proposed based on zero-sum game theory and has become a new research hotspot. The significance of the model variation is to obtain the data distribution through unsupervised learning and to generate more realistic/actual data. Currently, GANs have been widely studied due to the enormous application prospect, including image and vision computing, video and language processing, etc. In this paper, the background of the GAN, theoretic models and extensional variants of GANs are introduced, where the variants can further optimize the original GAN or change the basic structures. Then the typical applications of GANs are explained. Finally the existing problems of GANs are summarized and the future work of GANs models are given.},
	booktitle = {2019 14th {IEEE} conference on industrial electronics and applications ({ICIEA})},
	author = {Gonog, L. and Zhou, Y.},
	month = jun,
	year = {2019},
	keywords = {Application, Brain modeling, Data models, Deep learning, GANs models, Gallium nitride, Game theory, Generative Adversarial Networks (GANs), Generative adversarial networks, Generative models, Generators, Training, artificial intelligence, computer vision, data distribution, deep learning model, game theory, generative adversarial networks, language processing, unsupervised learning, zero-sum game theory},
	pages = {505--510},
}

@incollection{farnell_procedural_2014,
	address = {Oxford},
	title = {Procedural {Audio} {Theory} and {Practice}},
	isbn = {978-0-19-979722-6},
	url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199797226.001.0001/oxfordhb-9780199797226-e-031},
	abstract = {"Procedural Audio Theory and Practice" published on by Oxford University Press.},
	urldate = {2020-11-01},
	booktitle = {The {Oxford} {Handbook} of {Interactive} {Audio}},
	publisher = {Oxford University Press},
	author = {Farnell, Andy},
	month = may,
	year = {2014},
	doi = {10.1093/oxfordhb/9780199797226.013.031},
}

@inproceedings{lee_disentangled_2020,
	address = {Barcelona, Spain},
	title = {Disentangled multidimensional metric learning for music similarity},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053442/},
	doi = {10.1109/ICASSP40776.2020.9053442},
	abstract = {Music similarity search is useful for a variety of creative tasks such as replacing one music recording with another recording with a similar “feel”, a common task in video editing. For this task, it is typically necessary to deﬁne a similarity metric to compare one recording to another. Music similarity, however, is hard to deﬁne and depends on multiple simultaneous notions of similarity (i.e. genre, mood, instrument, tempo). While prior work ignore this issue, we embrace this idea and introduce the concept of multidimensional similarity and unify both global and specialized similarity metrics into a single, semantically disentangled multidimensional similarity metric. To do so, we adapt a variant of deep metric learning called conditional similarity networks to the audio domain and extend it using track-based information to control the speciﬁcity of our model. We evaluate our method and show that our single, multidimensional model outperforms both specialized similarity spaces and alternative baselines. We also run a user-study and show that our approach is favored by human annotators as well.},
	urldate = {2020-10-13},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Lee, Jongpil and Bryan, Nicholas J. and Salamon, Justin and Jin, Zeyu and Nam, Juhan},
	month = may,
	year = {2020},
	pages = {6--10},
}

@inproceedings{nistal_comparing_2020,
	address = {Amsterdam, The Netherlands},
	title = {Comparing representations for audio synthesis using generative adversarial networks},
	url = {http://arxiv.org/abs/2006.09266},
	abstract = {In this paper, we compare different audio signal representations, including the raw audio waveform and a variety of time-frequency representations, for the task of audio synthesis with Generative Adversarial Networks (GANs). We conduct the experiments on a subset of the NSynth dataset. The architecture follows the benchmark Progressive Growing Wasserstein GAN. We perform experiments both in a fully non-conditional manner as well as conditioning the network on the pitch information. We quantitatively evaluate the generated material utilizing standard metrics for assessing generative models, and compare training and sampling times. We show that complex-valued as well as the magnitude and Instantaneous Frequency of the Short-Time Fourier Transform achieve the best results, and yield fast generation and inversion times. The code for feature extraction, training and evaluating the model is available online.},
	urldate = {2020-10-12},
	booktitle = {28th {European} {Signal} {Processing} {Conference}},
	author = {Nistal, Javier and Lattner, Stefan and Richard, Gaël},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{qian_autovc_2019,
	title = {{AUTOVC}: {Zero}-{Shot} voice style transfer with only autoencoder loss},
	shorttitle = {{AUTOVC}},
	url = {http://arxiv.org/abs/1905.05879},
	abstract = {Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 36th international conference on machine learning},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	pages = {5210--5219},
}

@inproceedings{manocha_differentiable_2020,
	title = {A differentiable perceptual audio metric learned from just noticeable differences},
	url = {http://arxiv.org/abs/2001.04460},
	abstract = {Many audio processing tasks require perceptual assessment. The “gold standard“ of obtaining human judgments is time-consuming, expensive, and cannot be used as an optimization criterion. On the other hand, automated metrics are efficient to compute but often correlate poorly with human judgment, particularly for audio differences at the threshold of human detection. In this work, we construct a metric by fitting a deep neural network to a new large dataset of crowdsourced human judgments. Subjects are prompted to answer a straightforward, objective question: are two recordings identical or not? These pairs are algorithmically generated under a variety of perturbations, including noise, reverb, and compression artifacts; the perturbation space is probed with the goal of efficiently identifying the just-noticeable difference (JND) level of the subject. We show that the resulting learned metric is well-calibrated with human judgments, outperforming baseline methods. Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks. Thus, simply replacing an existing loss (e.g., deep feature loss) with our metric yields significant improvement in a denoising network, as measured by subjective pairwise comparison.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the annual conference of the international speech communication association},
	author = {Manocha, Pranay and Finkelstein, Adam and Zhang, Richard and Bryan, Nicholas J. and Mysore, Gautham J. and Jin, Zeyu},
	month = may,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, perceptual loss},
}

@inproceedings{ramires_neural_2020,
	address = {Barcelona, Spain},
	title = {Neural percussive synthesis parameterised by high-{Level} timbral features},
	url = {http://arxiv.org/abs/1911.11853},
	abstract = {We present a deep neural network-based methodology for synthesising percussive sounds with control over high-level timbral characteristics of the sounds. This approach allows for intuitive control of a synthesizer, enabling the user to shape sounds without extensive knowledge of signal processing. We use a feedforward convolutional neural network-based architecture, which is able to map input parameters to the corresponding waveform. We propose two datasets to evaluate our approach on both a restrictive context, and in one covering a broader spectrum of sounds. The timbral features used as parameters are taken from recent literature in signal processing. We also use these features for evaluation and validation of the presented model, to ensure that changing the input parameters produces a congruent waveform with the desired characteristics. Finally, we evaluate the quality of the output sound using a subjective listening test. We provide sound examples and the system's source code for reproducibility.},
	urldate = {2019-12-10},
	booktitle = {2020 \{{IEEE}\} international conference on acoustics, speech and signal processing},
	publisher = {IEEE},
	author = {Ramires, António and Chandna, Pritish and Favory, Xavier and Gómez, Emilia and Serra, Xavier},
	month = may,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, unread},
	pages = {786--790},
}

@inproceedings{luo_learning_2019,
	address = {Delft, The Netherlands},
	title = {Learning disentangled representations of timbre and pitch for musical instrument sounds using gaussian mixture variational autoencoders},
	url = {http://arxiv.org/abs/1906.08152},
	abstract = {In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model efficacy by latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high accuracy when tested on our synthesized sounds, which verifies the model performance of controllable realistic timbre and pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single autoencoder architecture, which is evaluated by measuring the shift in posterior of instrument classification. Our in depth evaluation confirms the model ability to successfully disentangle timbre and pitch.},
	urldate = {2020-05-13},
	booktitle = {Proceedings of the 20th international society for music information retrieval conference},
	author = {Luo, Yin-Jyun and Agres, Kat and Herremans, Dorien},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, unread},
}

@inproceedings{tan_generative_2020,
	title = {Generative modelling for controllable audio synthesis of expressive piano performance},
	url = {http://arxiv.org/abs/2006.09833},
	abstract = {We present a controllable neural audio synthesizer based on Gaussian Mixture Variational Autoencoders (GM-VAE), which can generate realistic piano performances in the audio domain that closely follows temporal conditions of two essential style features for piano performances: articulation and dynamics. We demonstrate how the model is able to apply fine-grained style morphing over the course of synthesizing the audio. This is based on conditions which are latent variables that can be sampled from the prior or inferred from other pieces. One of the envisioned use cases is to inspire creative and brand new interpretations for existing pieces of piano music.},
	urldate = {2020-06-23},
	booktitle = {{ICML} {Workshop} on {Machine} {Learning} for {Media} {Discovery}},
	author = {Tan, Hao Hao and Luo, Yin-Jyun and Herremans, Dorien},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@incollection{bilbao_chapter_2009,
	title = {Chapter 1, {Sound} {Synthesis} and {Physical} {Modeling}},
	url = {https://doi.org/10.1002/9780470749012.ch1},
	urldate = {2020-07-14},
	booktitle = {Numerical {Sound} {Synthesis}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Bilbao, Stefan},
	year = {2009},
	pages = {1--23},
}

@article{thakur_deep_2019,
	title = {Deep metric learning for bioacoustic classification: {Overcoming} training data scarcity using dynamic triplet loss},
	volume = {146},
	issn = {0001-4966},
	shorttitle = {Deep metric learning for bioacoustic classification},
	url = {https://asa.scitation.org/doi/10.1121/1.5118245},
	doi = {10.1121/1.5118245},
	number = {1},
	urldate = {2020-11-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Thakur, Anshul and Thapar, Daksh and Rajan, Padmanabhan and Nigam, Aditya},
	month = jul,
	year = {2019},
	pages = {534--547},
}

@article{carroll_analysis_1970,
	title = {Analysis of individual differences in multidimensional scaling via an n-way generalization of “{Eckart}-{Young}” decomposition},
	volume = {35},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02310791},
	doi = {10.1007/BF02310791},
	number = {3},
	urldate = {2021-01-19},
	journal = {Psychometrika},
	author = {Carroll, J. Douglas and Chang, Jih-Jie},
	month = sep,
	year = {1970},
	pages = {283--319},
}

@article{thoret_learning_2020,
	title = {Learning metrics on spectrotemporal modulations reveals the perception of musical instrument timbre},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-020-00987-5},
	doi = {10.1038/s41562-020-00987-5},
	urldate = {2021-01-27},
	journal = {Nature Human Behaviour},
	author = {Thoret, Etienne and Caramiaux, Baptiste and Depalle, Philippe and McAdams, Stephen},
	month = nov,
	year = {2020},
}

@inproceedings{xie_backpropagation_2009,
	address = {Nanchang, China},
	series = {{IITA}'09},
	title = {Backpropagation modification in {Monte}-{Carlo} game tree search},
	isbn = {978-1-4244-5212-5},
	abstract = {The Algorithm UCT, proposed by Kocsys et al. [3], which apply multi-armed bandit problem into the tree-structured search space, achieves some remarkable success in some challenging fields [2]. For UCT algorithm, Monte-Carlo simulations are performed with the guidance of UCB1 formula, which are averaged to evaluate a specified action. We observe that, as more simulations are performed, later ones usually lead to more accurate results, partly because the level of the search used in the later simulation is deeper and partly because more results are available to direct subsequent simulations. This paper presents a new method to improve the performance of UCT algorithm by increasing the feedback value of the later simulations. And the experimental results in the classical game Go show that our approach increases the performance of Monte-Carlo simulations significantly when exponential models are used.},
	urldate = {2021-01-02},
	booktitle = {Proceedings of the 3rd international conference on {Intelligent} information technology application},
	publisher = {IEEE Press},
	author = {Xie, Fan and Liu, Zhiqing},
	month = nov,
	year = {2009},
	keywords = {Monte-Carlo tree search, machine learning, weight factor},
	pages = {125--128},
}

@article{patil_music_2012,
	title = {Music in our ears: {The} biological bases of musical timbre perception},
	volume = {8},
	issn = {1553-7358},
	shorttitle = {Music in {Our} {Ears}},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1002759},
	doi = {10.1371/journal.pcbi.1002759},
	number = {11},
	urldate = {2021-01-13},
	journal = {PLoS Computational Biology},
	author = {Patil, Kailash and Pressnitzer, Daniel and Shamma, Shihab and Elhilali, Mounya},
	editor = {Theunissen, Frederic E.},
	month = nov,
	year = {2012},
	pages = {e1002759},
}

@article{dutta_unsupervised_2020,
	title = {Unsupervised deep metric learning via orthogonality based probabilistic loss},
	url = {http://arxiv.org/abs/2008.09880},
	doi = {10.1109/TAI.2020.3026982},
	abstract = {Metric learning is an important problem in machine learning. It aims to group similar examples together. Existing state-of-the-art metric learning approaches require class labels to learn a metric. As obtaining class labels in all applications is not feasible, we propose an unsupervised approach that learns a metric without making use of class labels. The lack of class labels is compensated by obtaining pseudo-labels of data using a graph-based clustering approach. The pseudo-labels are used to form triplets of examples, which guide the metric learning. We propose a probabilistic loss that minimizes the chances of each triplet violating an angular constraint. A weight function, and an orthogonality constraint in the objective speeds up the convergence and avoids a model collapse. We also provide a stochastic formulation of our method to scale up to large-scale datasets. Our studies demonstrate the competitiveness of our approach against state-of-the-art methods. We also thoroughly study the effect of the different components of our method.},
	urldate = {2021-01-17},
	journal = {arXiv:2008.09880 [cs]},
	author = {Dutta, Ujjal Kr and Harandi, Mehrtash and Sekhar, Chellu Chandra},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jackson_protest_1995,
	title = {{PROTEST}: {A} {PROcrustean} randomization {TEST} of community environment concordance},
	volume = {2},
	issn = {1195-6860, 2376-7626},
	shorttitle = {{PROTEST}},
	url = {https://www.tandfonline.com/doi/full/10.1080/11956860.1995.11682297},
	doi = {10.1080/11956860.1995.11682297},
	number = {3},
	urldate = {2021-01-19},
	journal = {Écoscience},
	author = {Jackson, Donald A.},
	month = jan,
	year = {1995},
	pages = {297--303},
}

@incollection{helmholtz_differences_2009,
	address = {Cambridge},
	edition = {3},
	series = {Cambridge {Library} {Collection} - {Music}},
	title = {On the {Differences} in the {Quality} of {Musical} {Tones}},
	isbn = {978-1-108-00177-9},
	booktitle = {On the sensations of tone as a physiological basis for the theory of music},
	publisher = {Cambridge University Press},
	author = {Helmholtz, Hermann L. F.},
	translator = {Ellis, Alexander J.},
	year = {2009},
	doi = {10.1017/CBO9780511701801.010},
	pages = {106--173},
}

@inproceedings{jain_att_2020,
	address = {Glasgow, United Kingdom},
	title = {{ATT}: {Attention}-based {Timbre} {Transfer}},
	isbn = {978-1-72816-926-2},
	shorttitle = {{ATT}},
	url = {https://ieeexplore.ieee.org/document/9207146/},
	doi = {10.1109/IJCNN48605.2020.9207146},
	urldate = {2021-02-19},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Jain, Deepak Kumar and Kumar, Akshi and Cai, Linqin and Singhal, Siddharth and Kumar, Vaibhav},
	month = jul,
	year = {2020},
	pages = {1--6},
}

@article{osgood_nature_1952,
	title = {The nature and measurement of meaning.},
	volume = {49},
	issn = {1939-1455, 0033-2909},
	doi = {10.1037/h0055737},
	number = {3},
	urldate = {2021-01-13},
	journal = {Psychological Bulletin},
	author = {Osgood, Charles E.},
	year = {1952},
	pages = {197--237},
}

@misc{henninger_labjs_2020,
	title = {lab.js: {A} free, open, online experiment builder},
	shorttitle = {lab.js},
	abstract = {lab.js makes it easy to build experiments for both online and in-laboratory data collection.. It provides a visual interface for constructing studies, and a powerful Javascript library for stimulus presentation and response collection.},
	urldate = {2020-05-07},
	publisher = {Zenodo},
	author = {Henninger, Felix and Shevchenko, Yury and Mertens, Ulf and Kieslich, Pascal J. and Hilbig, Benjamin E.},
	month = apr,
	year = {2020},
	doi = {10.5281/zenodo.3767907},
}

@article{pantev_timbre-specific_2001,
	title = {Timbre-specific enhancement of auditory cortical representations in musicians},
	volume = {12},
	doi = {10.1097/00001756-200101220-00041},
	number = {1},
	journal = {Cognitive Neuroscience and Neuropsychology},
	author = {Pantev, Christo and Roberts, Larry E and Schulz, Matthias and Engelien, Almut and Ross, Bernhard},
	year = {2001},
	pages = {6},
}

@article{grey_multidimensional_1977,
	title = {Multidimensional perceptual scaling of musical timbres},
	volume = {61},
	issn = {0001-4966},
	doi = {10.1121/1.381428},
	number = {5},
	urldate = {2019-12-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Grey, John M.},
	year = {1977},
	keywords = {unread},
	pages = {1270--1277},
}

@article{huzaifah_bin_md_shahrin_applying_2020,
	title = {Applying visual domain style transfer and texture synthesis techniques to audio: insights and challenges},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Applying visual domain style transfer and texture synthesis techniques to audio},
	url = {http://link.springer.com/10.1007/s00521-019-04053-8},
	doi = {10.1007/s00521-019-04053-8},
	number = {4},
	urldate = {2021-01-30},
	journal = {Neural Computing and Applications},
	author = {Huzaifah bin Md Shahrin, Muhammad and Wyse, Lonce},
	month = feb,
	year = {2020},
	pages = {1051--1065},
}

@inproceedings{zeghidour_deep_2016,
	address = {Shanghai},
	title = {A deep scattering spectrum — {Deep} {Siamese} network pipeline for unsupervised acoustic modeling},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7472622/},
	doi = {10.1109/ICASSP.2016.7472622},
	urldate = {2021-02-08},
	booktitle = {2016 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Zeghidour, Neil and Synnaeve, Gabriel and Versteegh, Maarten and Dupoux, Emmanuel},
	month = mar,
	year = {2016},
	pages = {4965--4969},
}

@article{zacharakis_interlanguage_2015,
	title = {An interlanguage unification of musical timbre: {Bridging} semantic, perceptual, and acoustic dimensions},
	volume = {32},
	issn = {07307829, 15338312},
	shorttitle = {An {Interlanguage} {Unification} of {Musical} {Timbre}},
	doi = {10.1525/mp.2015.32.4.394},
	number = {4},
	urldate = {2020-05-20},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
	month = apr,
	year = {2015},
	keywords = {unread},
	pages = {394--412},
}

@article{solomon_semantic_1958,
	title = {Semantic {Approach} to the {Perception} of {Complex} {Sounds}},
	volume = {30},
	issn = {0001-4966},
	doi = {10.1121/1.1909632},
	number = {5},
	urldate = {2021-01-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Solomon, Lawrence N.},
	year = {1958},
	pages = {421--425},
}

@article{wallmark_corpus_2019,
	title = {A corpus analysis of timbre semantics in orchestration treatises},
	volume = {47},
	issn = {0305-7356},
	doi = {10.1177/0305735618768102},
	abstract = {What does the common descriptive lexicon for instrumental sound tell us about how we conceptualize musical timbre? Perceptual studies have revealed a number of verbal attributes that reliably map onto timbral qualities, but the conventions of timbre description in spoken and written discourse remain poorly understood. Books on orchestration provide a valuable source of natural language about instrumental timbre. This article uses methods from corpus linguistics to explore the semantic features of timbre through a quantitative analysis of 11 orchestration treatises and manuals. Findings reveal a relatively constrained vocabulary for timbre: about 50 adjectives account for half of all descriptions in the corpus. The timbre lexicon can be categorized according to affect, matter, metaphor, mimesis, action, acoustics, and onomatopoeia, and further reduced to three latent conceptual dimensions, which are labeled and discussed. Descriptive patterns vary systematically by instrument and instrument family, suggesting certain regularities and consistencies to timbre description in the orchestral tradition. This study helps test the long-held assumption that conventions of timbre description are vague and unsystematic, and offers a cognitive linguistic account of the timbre-language connection.},
	number = {4},
	urldate = {2020-08-06},
	journal = {Psychology of Music},
	author = {Wallmark, Zachary},
	year = {2019},
	pages = {585--605},
}

@article{wallmark_semantic_2019,
	title = {Semantic {Crosstalk} in {Timbre} {Perception}},
	volume = {2},
	issn = {2059-2043},
	url = {https://doi.org/10.1177/2059204319846617},
	doi = {10.1177/2059204319846617},
	abstract = {Many adjectives for musical timbre reflect cross-modal correspondence, particularly with vision and touch (e.g., “dark–bright,” “smooth–rough”). Although multisensory integration between visual/tactile processing and hearing has been demonstrated for pitch and loudness, timbre is not well understood as a locus of cross-modal mappings. Are people consistent in these semantic associations? Do cross-modal terms reflect dimensional interactions in timbre processing? Here I designed two experiments to investigate crosstalk between timbre semantics and perception through the use of Stroop-type speeded classification. Experiment 1 found that incongruent pairings of instrument timbres and written names caused significant Stroop-type interference relative to congruent pairs, indicating bidirectional crosstalk between semantic and auditory modalities. Pre-Experiment 2 asked participants to rate natural and synthesized timbres on semantic differential scales capturing luminance (brightness) and texture (roughness) associations, finding substantial consistency for a number of timbres. Acoustic correlates of these associations were also assessed, indicating an important role for high-frequency energy in the intensity of cross-modal ratings. Experiment 2 used timbre adjectives and sound stimuli validated in the previous experiment in two variants of a semantic-auditory Stroop-type task. Results of linear mixed-effects modeling of reaction time and accuracy showed slight interference in semantic processing when adjectives were paired with cross-modally incongruent instrument timbres (e.g., the word “smooth” with a “rough” timbre). Taken together, I conclude by suggesting that semantic crosstalk in timbre processing may be partially automatic and could reflect weak synesthetic congruency between interconnected sensory domains.},
	urldate = {2020-08-12},
	journal = {Music \& Science},
	author = {Wallmark, Zachary},
	year = {2019},
	pages = {2059204319846617},
}

@misc{romani_picas_good-sounds_2017,
	title = {Good-{Sounds} {Dataset}},
	url = {https://zenodo.org/record/820937},
	abstract = {{\textless}strong{\textgreater}General description:{\textless}/strong{\textgreater} This dataset was created in the context of the Pablo project, partially funded by KORG Inc. It contains monophonic recordings of two kind of exercises: single notes and scales. The dataset was reported in the following article: {\textless}strong{\textgreater}Romaní Picas O., Parra Rodriguez H., Dabiri D., Tokuda H., Hariya W., Oishi K., \&amp; Serra X."A real-time system for measuring sound goodness in instrumental sounds", 138th Audio Engineering Society Convention (2015). {\textless}/strong{\textgreater} The recordings were made in the Universitat Pompeu Fabra / Phonos recording studio by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments were recorded using one or up to 4 different microphones (depending on the recording session). For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate mono .flac audio file of 48kHz and 32 bits. The tonal characteristics are explained both in the the following section and the related publication. The audio files are organised in one directory for each recording session. In addition to the files, one SQLite database file is included. The structure of the database is related in the following section. {\textless}strong{\textgreater}Database description:{\textless}/strong{\textgreater} The database is meant for organizing the sounds in a handy way. It is organised in four different tables: sounds, takes, packs and ratings. Sounds The table containing the sounds annotations. {\textless}strong{\textgreater}id{\textless}/strong{\textgreater} {\textless}strong{\textgreater}instrument {\textless}/strong{\textgreater}: flute, cello, clarinet, trumpet, violin, sax\_alto, sax\_tenor, sax\_baritone, sax\_soprano, oboe, piccolo, bass {\textless}strong{\textgreater}note{\textless}/strong{\textgreater} {\textless}strong{\textgreater}octave{\textless}/strong{\textgreater} {\textless}strong{\textgreater}dynamics {\textless}/strong{\textgreater}: for some sounds, the musical notation of the loudness level (p, mf, f..) {\textless}strong{\textgreater}recorded\_at {\textless}/strong{\textgreater}: recording date and time {\textless}strong{\textgreater}location {\textless}/strong{\textgreater}: recording place {\textless}strong{\textgreater}player {\textless}/strong{\textgreater}: the musician who recorded it. For detailed information about the musicians please contact us. {\textless}strong{\textgreater}bow\_velocity {\textless}/strong{\textgreater}: for some string instruments the velocity of the bow (slow, medieum, fast) {\textless}strong{\textgreater}bridge\_position {\textless}/strong{\textgreater}: for some string instruments the position of the bow (tasto, middle, ponticello) {\textless}strong{\textgreater}string {\textless}/strong{\textgreater}: for some string instruments the number of the string in which the sound it's played (1: lowest in pitch) {\textless}strong{\textgreater}csv\_file {\textless}/strong{\textgreater}: used for creation of the DB {\textless}strong{\textgreater}csv\_id {\textless}/strong{\textgreater}: used for creation of the DB {\textless}strong{\textgreater}pack\_filename {\textless}/strong{\textgreater}: used for creation of the DB {\textless}strong{\textgreater}pack\_id {\textless}/strong{\textgreater}: used for creation of the DB {\textless}strong{\textgreater}attack {\textless}/strong{\textgreater}: for single notes, manual annotation of the onset in samples. {\textless}strong{\textgreater}decay {\textless}/strong{\textgreater}: for single notes, manual annotation of the decay in samples. {\textless}strong{\textgreater}sustain {\textless}/strong{\textgreater}: for single notes, manual annotation of the beginnig of the sustained part in samples. {\textless}strong{\textgreater}release {\textless}/strong{\textgreater}: for single notes, manual annotation of the beginnig of the release part in samples. {\textless}strong{\textgreater}offset {\textless}/strong{\textgreater}: for single notes, manual annotation of the offset in samples {\textless}strong{\textgreater}reference {\textless}/strong{\textgreater}: 1 if sound was used to create the models in the good-sounds project, 0 if not. {\textless}strong{\textgreater}klass {\textless}/strong{\textgreater}: user generated tags of the tonal qualities of the sound. They also contain information about the exercise, that could be single note or scale. "good-sound": good examples of single note "bad": bad example of one of the sound attributes defined in the project (please read the papers for a detailed explanation) "scale-good": good example of a one octave scale going up and down (15 notes). If the scale is minor a tagged "minor" is also available. "scale-bad": bad example scale of one of the sounds defined in the project. (15 notes up and down). {\textless}strong{\textgreater}comments {\textless}/strong{\textgreater}: if any {\textless}strong{\textgreater}semitone {\textless}/strong{\textgreater}: midi note {\textless}strong{\textgreater}pitch\_reference {\textless}/strong{\textgreater}: the reference pitch Takes A sound can have several takes as some of them were recorded using different microphones at the same time. Each take has an associated audio file. {\textless}strong{\textgreater}id{\textless}/strong{\textgreater} {\textless}strong{\textgreater}microphone{\textless}/strong{\textgreater} {\textless}strong{\textgreater}filename {\textless}/strong{\textgreater}: the name of the associated audio file original\_filename : {\textless}strong{\textgreater}freesound\_id {\textless}/strong{\textgreater}: for some sounds uploaded to freesound.org {\textless}strong{\textgreater}sound\_id {\textless}/strong{\textgreater}: the id of the sound in the DB {\textless}strong{\textgreater}goodsound\_id {\textless}/strong{\textgreater}: for some of the sounds available in good-sounds.org Packs A pack is a group of sounds from the same recording session. The audio files are organised in the *sound\_files* directory in subfolders with the pack name to which they belong. {\textless}strong{\textgreater}id{\textless}/strong{\textgreater} {\textless}strong{\textgreater}name{\textless}/strong{\textgreater} {\textless}strong{\textgreater}description{\textless}/strong{\textgreater} Ratings Some musicians rated some sounds in a 0-10 goodness scale for the user evaluatio of the first project prototype. Please read the paper for more detailed information. {\textless}strong{\textgreater}id{\textless}/strong{\textgreater} {\textless}strong{\textgreater}mark{\textless}/strong{\textgreater}: the rate or score. {\textless}strong{\textgreater}type{\textless}/strong{\textgreater}: the klass of the sound. Related to the tags of the sound. {\textless}strong{\textgreater}created\_at{\textless}/strong{\textgreater} {\textless}strong{\textgreater}comments{\textless}/strong{\textgreater} {\textless}strong{\textgreater}sound\_id{\textless}/strong{\textgreater} {\textless}strong{\textgreater}rater{\textless}/strong{\textgreater}: the musician who rated the sound. {\textless}strong{\textgreater}License:{\textless}/strong{\textgreater} This work is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.},
	urldate = {2021-03-02},
	publisher = {Zenodo},
	author = {Romani Picas, Oriol and Parra Rodriguez, Hector and Dabiri, Dara and Serra, Xavier},
	month = jun,
	year = {2017},
	doi = {10.5281/ZENODO.820937},
	note = {tex.copyright: Creative Commons Attribution 4.0, Open Access},
	keywords = {MTG, good-sounds, goodness, instrument, sound, timbre},
}

@inproceedings{ravanelli_speaker_2018,
	address = {Athens, Greece},
	title = {Speaker {Recognition} from {Raw} {Waveform} with {SincNet}},
	isbn = {978-1-5386-4334-1},
	url = {https://ieeexplore.ieee.org/document/8639585/},
	doi = {10.1109/SLT.2018.8639585},
	urldate = {2021-03-11},
	booktitle = {2018 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Ravanelli, Mirco and Bengio, Yoshua},
	month = dec,
	year = {2018},
	pages = {1021--1028},
}

@article{chrysos_deep_2021,
	title = {Deep {Polynomial} {Neural} {Networks}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2006.13026},
	doi = {10.1109/TPAMI.2021.3058891},
	abstract = {Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose \${\textbackslash}Pi\$-Nets, a new class of function approximators based on polynomial expansions. \${\textbackslash}Pi\$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that \${\textbackslash}Pi\$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, \${\textbackslash}Pi\$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning. The source code is available at {\textbackslash}url\{https://github.com/grigorisg9gr/polynomial\_nets\}.},
	urldate = {2021-03-12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chrysos, Grigorios and Moschoglou, Stylianos and Bouritsas, Giorgos and Deng, Jiankang and Panagakis, Yannis and Zafeiriou, Stefanos},
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@article{subramani_vapar_2020,
	title = {{VaPar} synth – a variational parametric model for audio synthesis},
	url = {http://arxiv.org/abs/2004.00001},
	doi = {10.1109/ICASSP40776.2020.9054181},
	abstract = {With the advent of data-driven statistical modeling and abundant computing power, researchers are turning increasingly to deep learning for audio synthesis. These methods try to model audio signals directly in the time or frequency domain. In the interest of more flexible control over the generated sound, it could be more useful to work with a parametric representation of the signal which corresponds more directly to the musical attributes such as pitch, dynamics and timbre. We present VaPar Synth - a Variational Parametric Synthesizer which utilizes a conditional variational autoencoder (CVAE) trained on a suitable parametric representation. We demonstrate our proposed model's capabilities via the reconstruction and generation of instrumental tones with flexible control over their pitch.},
	urldate = {2021-03-02},
	journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Subramani, Krishna and Rao, Preeti and D'Hooge, Alexandre},
	month = may,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, \_tablet},
	pages = {796--800},
}

@inproceedings{caetano_source-filter_2012,
	address = {Kyoto, Japan},
	title = {A source-filter model for musical instrument sound transformation},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6287836/},
	doi = {10.1109/ICASSP.2012.6287836},
	urldate = {2021-03-16},
	booktitle = {2012 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Caetano, Marcelo and Rodet, Xavier},
	month = mar,
	year = {2012},
	pages = {137--140},
}

@article{troumbis_chebyshev_2020,
	title = {A {Chebyshev} polynomial feedforward neural network trained by differential evolution and its application in environmental case studies},
	volume = {126},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815218309538},
	doi = {10.1016/j.envsoft.2020.104663},
	urldate = {2021-03-17},
	journal = {Environmental Modelling \& Software},
	author = {Troumbis, Ioannis A. and Tsekouras, George E. and Tsimikas, John and Kalloniatis, Christos and Haralambopoulos, Dias},
	month = apr,
	year = {2020},
	pages = {104663},
}

@book{schnupp_auditory_2010,
	title = {Auditory {Neuroscience}: {Making} {Sense} of {Sound}},
	isbn = {978-0-262-28975-7},
	shorttitle = {Auditory {Neuroscience}},
	url = {https://direct.mit.edu/books/book/3945/auditory-neurosciencemaking-sense-of-sound},
	urldate = {2021-03-19},
	publisher = {The MIT Press},
	author = {Schnupp, Jan and Nelken, Israel and King, Andrew J.},
	year = {2010},
	doi = {10.7551/mitpress/7942.001.0001},
}

@incollection{schnupp_periodicity_2010-1,
	title = {Periodicity and {Pitch} {Perception}},
	isbn = {978-0-262-28975-7},
	url = {https://direct.mit.edu/books/book/3945/chapter/164938/periodicity-and-pitch-perception},
	urldate = {2021-03-19},
	booktitle = {Auditory {Neuroscience}},
	publisher = {The MIT Press},
	collaborator = {Schnupp, Jan and Nelken, Israel and King, Andrew J.},
	year = {2010},
	doi = {10.7551/mitpress/7942.003.0004},
}

@incollection{schnupp_periodicity_2010,
	title = {Periodicity and pitch perception: {Physics}, psychophysics, and neural mechanisms},
	isbn = {978-0-262-28975-7},
	url = {https://direct.mit.edu/books/book/3945/auditory-neurosciencemaking-sense-of-sound},
	urldate = {2021-03-19},
	booktitle = {Auditory {Neuroscience}: {Making} {Sense} of {Sound}},
	publisher = {The MIT Press},
	author = {Schnupp, Jan and Nelken, Israel and King, Andrew J.},
	year = {2010},
	doi = {10.7551/mitpress/7942.003.0004},
	pages = {93--138},
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice},
	title = {Unpaired image-to-image translation using cycle-{Consistent} adversarial networks},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237506/},
	doi = {10.1109/ICCV.2017.244},
	urldate = {2021-03-19},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{karras_style-based_2019,
	address = {Long Beach, CA, USA},
	title = {A style-{Based} generator architecture for generative adversarial networks},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953766/},
	doi = {10.1109/CVPR.2019.00453},
	urldate = {2021-03-19},
	booktitle = {2019 {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	publisher = {IEEE},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = jun,
	year = {2019},
	pages = {4396--4405},
}

@inproceedings{radford_unsupervised_2016,
	title = {Unsupervised representation learning with deep convolutional generative adversarial networks},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2021-03-19},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{schnupp_pitch_nodate,
	title = {Pitch},
	booktitle = {Auditory {Neuroscience}},
	author = {Schnupp, Jan},
}

@inproceedings{collier_progressively_2018,
	address = {Singapore, Singapore},
	title = {Progressively growing generative adversarial networks for high resolution semantic segmentation of satellite images},
	isbn = {978-1-5386-9288-2},
	url = {https://ieeexplore.ieee.org/document/8637381/},
	doi = {10.1109/ICDMW.2018.00115},
	urldate = {2021-03-19},
	booktitle = {2018 {IEEE} international conference on data mining workshops ({ICDMW})},
	publisher = {IEEE},
	author = {Collier, Edward and Duffy, Kate and Ganguly, Sangram and Madanguit, Geri and Kalia, Subodh and Shreekant, Gayaka and Nemani, Ramakrishna and Michaelis, Andrew and Li, Shaung and Ganguly, Auroop and Mukhopadhyay, Supratik},
	month = nov,
	year = {2018},
	pages = {763--769},
}

@article{el_gharbi_two-level_2021,
	title = {Two-level substructuring and parallel mesh generation for domain decomposition methods},
	volume = {192},
	issn = {0168874X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168874X20301645},
	doi = {10.1016/j.finel.2020.103484},
	urldate = {2021-03-24},
	journal = {Finite Elements in Analysis and Design},
	author = {El Gharbi, Y. and Parret-Fréaud, A. and Bovet, C. and Gosselet, P.},
	month = sep,
	year = {2021},
	pages = {103484},
}

@inproceedings{goto_rwc_2003,
	title = {{RWC} music database: {Music} genre database and musical instrument sound database},
	booktitle = {Proceedings of the 4th international conference on music information retrieval},
	author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
	month = oct,
	year = {2003},
	pages = {229--230},
}

@misc{fant_t_2001,
	title = {T. {Chiba} and {M}. {Kajiyama}, pioneers in speech acoustics(\&lt;feature articles\&gt;sixtieth anniversary of the publication of the vowel, its nature and structure by chiba and kajiyama)},
	url = {https://doi.org/10.24467/onseikenkyu.5.2%5F4},
	urldate = {2021-03-29},
	publisher = {The Phonetic Society of Japan},
	author = {FANT, Gunnar},
	year = {2001},
}

@book{chiba_vowel_1958,
	address = {Tokyo},
	title = {The vowel, its nature and structure},
	publisher = {Phonetic Society of Japan},
	author = {Chiba, Tsutomu and Kajiyama, Masato},
	year = {1958},
}

@inproceedings{cooper_zero-shot_2020,
	address = {Barcelona, Spain},
	title = {Zero-shot multi-{Speaker} text-{To}-{Speech} with state-{Of}-{The}-{Art} neural speaker embeddings},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054535/},
	doi = {10.1109/ICASSP40776.2020.9054535},
	urldate = {2021-04-05},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Cooper, Erica and Lai, Cheng-I and Yasuda, Yusuke and Fang, Fuming and Wang, Xin and Chen, Nanxin and Yamagishi, Junichi},
	month = may,
	year = {2020},
	pages = {6184--6188},
}

@misc{rafii_musdb18_2017,
	title = {{MUSDB18} - a corpus for music separation},
	url = {https://zenodo.org/record/1117372},
	urldate = {2020-04-23},
	publisher = {Zenodo},
	author = {Rafii, Zafar and Liutkus, Antoine and Stöter, Fabian-Robert and Mimilakis, Stylianos Ioannis and Bittner, Rachel},
	month = dec,
	year = {2017},
	doi = {10.5281/zenodo.1117372},
	keywords = {audio, multitrack, music, source separation, stems},
}

@inproceedings{yang_vocgan_2020,
	title = {{VocGAN}: {A} high-{Fidelity} real-{Time} vocoder with a hierarchically-{Nested} adversarial network},
	shorttitle = {{VocGAN}},
	url = {http://www.isca-speech.org/archive/Interspeech%5F2020/abstracts/1238.html},
	doi = {10.21437/Interspeech.2020-1238},
	urldate = {2021-03-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Yang, Jinhyeok and Lee, Junmo and Kim, Youngik and Cho, Hoon-Young and Kim, Injung},
	month = oct,
	year = {2020},
	pages = {200--204},
}

@article{lindsay_convolutional_2020,
	title = {Convolutional neural networks as a model of the visual system: {Past}, present, and future},
	issn = {0898-929X, 1530-8898},
	shorttitle = {Convolutional {Neural} {Networks} as a {Model} of the {Visual} {System}},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/jocn%5Fa%5F01544},
	doi = {10.1162/jocn_a_01544},
	abstract = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.},
	urldate = {2021-03-26},
	journal = {Journal of Cognitive Neuroscience},
	author = {Lindsay, Grace W.},
	month = feb,
	year = {2020},
	pages = {1--15},
}

@misc{droppo_distance_nodate,
	title = {Distance {Metrics} for {Discrete} {Time}-{Frequency} {Representations}},
	author = {Droppo, James and Atlas, Les},
}

@article{rafii_overview_2018,
	title = {An overview of lead and accompaniment separation in music},
	volume = {26},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2018.2825440},
	number = {8},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Rafii, Z. and Liutkus, A. and Stöter, F. R. and Mimilakis, S. I. and FitzGerald, D. and Pardo, B.},
	month = aug,
	year = {2018},
	pages = {1307--1335},
}

@inproceedings{szegedy_rethinking_2016,
	address = {Las Vegas, NV, USA},
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780677/},
	doi = {10.1109/CVPR.2016.308},
	urldate = {2021-04-06},
	booktitle = {2016 {IEEE} conference on computer vision and pattern recognition ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	pages = {2818--2826},
}

@article{depireux_spectro-temporal_2001,
	title = {Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex},
	volume = {85},
	url = {https://doi.org/10.1152/jn.2001.85.3.1220},
	doi = {10.1152/jn.2001.85.3.1220},
	abstract = {To understand the neural representation of broadband, dynamic sounds in primary auditory cortex (AI), we characterize responses using the spectro-temporal response field (STRF). The STRF describes, predicts, and fully characterizes the linear dynamics of neurons in response to sounds with rich spectro-temporal envelopes. It is computed from the responses to elementary “ripples,” a family of sounds with drifting sinusoidal spectral envelopes. The collection of responses to all elementary ripples is the spectro-temporal transfer function. The complex spectro-temporal envelope of any broadband, dynamic sound can expressed as the linear sum of individual ripples. Previous experiments using ripples with downward drifting spectra suggested that the transfer function is separable, i.e., it is reducible into a product of purely temporal and purely spectral functions. Here we measure the responses to upward and downward drifting ripples, assuming reparability within each direction, to determine if the total bidirectional transfer function is fully separable. In general, the combined transfer function for two directions is not symmetric, and hence units in AI are not, in general, fully separable. Consequently, many AI units have complex response properties such as sensitivity to direction of motion, though most inseparable units are not strongly directionally selective. We show that for most neurons, the lack of full separability stems from differences between the upward and downward spectral cross-sections but not from the temporal cross-sections; this places strong constraints on the neural inputs of these AI units.},
	number = {3},
	journal = {Journal of Neurophysiology},
	author = {Depireux, Didier A. and Simon, Jonathan Z. and Klein, David J. and Shamma, Shihab A.},
	year = {2001},
	pages = {1220--1234},
}

@misc{salamon_mdb-stem-synth_2018,
	title = {Mdb-{Stem}-{Synth}},
	url = {https://zenodo.org/record/1481172},
	abstract = {MDB-stem-synth{\textless}br{\textgreater} ============== MDB-stem-synth (c) by Justin Salamon, Rachel Bittner, Jordi Bonada, Juan Jose Bosch, Emilia Gómez and Juan Pablo Bello.{\textless}br{\textgreater} MDB-stem-synth is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0). {\textless}br{\textgreater} You should have received a copy of the license along with this work. If not, see http://creativecommons.org/licenses/by-nc/4.0/ {\textless}br{\textgreater} Created By{\textless}br{\textgreater} ———- Justin Salamon*, Rachel Bittner*, Jordi Bonadaˆ, Juan Jose Boschˆ, Emilia Gómezˆ and Juan Pablo Bello*.{\textless}br{\textgreater} * Music and Audio Research Lab (MARL), New York University, USA{\textless}br{\textgreater} ˆ Music Technology Group, Universitat Pompeu Fabra, Spain{\textless}br{\textgreater} http://synthdatasets.weebly.com/{\textless}br{\textgreater} http://steinhardt.nyu.edu/marl/{\textless}br{\textgreater} https://www.upf.edu/web/mtg Version 1.0.0 {\textless}br{\textgreater} Description{\textless}br{\textgreater} ———– MDB-stem-synth contains 230 solo stems (tracks) from the MedleyDB dataset (http://medleydb.weebly.com/) spanning a {\textless}br{\textgreater} variety of musical instruments and voices, which have been resynthesized to obtain a perfect f0 annotation using the {\textless}br{\textgreater} analysis/synthesis method described in the following publication: J. Salamon, R. M. Bittner, J. Bonada, J. J. Bosch, E. Gómez, and J. P. Bello. "An analysis/synthesis framework for {\textless}br{\textgreater} automatic f0 annotation of multitrack datasets". In 18th Int. Soc. for Music Info. Retrieval Conf., Suzhou, China, {\textless}br{\textgreater} Oct. 2017. This dataset includes:{\textless}br{\textgreater} * 230 mono wav files containing the resynthesized solo stem (track) from a variety of instruments and voices{\textless}br{\textgreater} * 230 csv files containing a perfect f0 annotation of the stem (track) obtained via the analysis/synthesis method {\textless}br{\textgreater} described in the paper The data come in two folders, the contents of which is described below. {\textless}br{\textgreater} audio\_stems{\textless}br{\textgreater} ———–{\textless}br{\textgreater} Contains 230 mono wav files of the resynthesized solo stems (tracks), spanning a variety of musical instruments and {\textless}br{\textgreater} voices. The resynthesized stems are obtained via the analysis/synthesis method described in the paper. Naming convention: {\textless}br{\textgreater} \&lt;artist\&gt;\_\&lt;songtitle\&gt;\_STEM\_\&lt;stemID\&gt;.RESYN.wav Example: {\textless}br{\textgreater} AClassicEducation\_NightOwl\_STEM\_01.RESYN.wav {\textless}br{\textgreater} annotation\_stems{\textless}br{\textgreater} —————-{\textless}br{\textgreater} Contains 230 csv files containing a perfect f0 annotation of the stems (tracks) obtained via the analysis/synthesis {\textless}br{\textgreater} method described in the paper. Format:{\textless}br{\textgreater} Each file contains two comma-separated columns, the first containing timestamps and the second containing the stem {\textless}br{\textgreater} f0 in Hz. The first frame in the annotation is zero-centered. Silence is indicated as 0 Hz. The hop size of the {\textless}br{\textgreater} annotation is 128/44100 seconds ({\textasciitilde}2.9 ms). Naming convention:{\textless}br{\textgreater} \&lt;artist\&gt;\_\&lt;songtitle\&gt;\_STEM\_\&lt;stemID\&gt;.RESYN.csv Example:{\textless}br{\textgreater} AClassicEducation\_NightOwl\_STEM\_01.RESYN.csv {\textless}br{\textgreater} Please Acknowledge MDB-stem-synth in Academic Research{\textless}br{\textgreater} —————————————————— Please cite the following publication when using MDB-stem-synth: J. Salamon, R. M. Bittner, J. Bonada, J. J. Bosch, E. Gómez, and J. P. Bello. "An analysis/synthesis framework for {\textless}br{\textgreater} automatic f0 annotation of multitrack datasets". In 18th Int. Soc. for Music Info. Retrieval Conf., Suzhou, China, {\textless}br{\textgreater} Oct. 2017. For information about the original MedleyDB dataset please see (and cite): R. M. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and J. P. Bello. MedleyDB: A multitrack dataset for {\textless}br{\textgreater} annotation-intensive MIR research. In 15th Int. Soc. for Music Info. Retrieval Conf., pages 155–160, Taipei, Taiwan, {\textless}br{\textgreater} Oct. 2014. {\textless}br{\textgreater} Conditions of Use{\textless}br{\textgreater} —————– Dataset created by Justin Salamon, Rachel Bittner, Jordi Bonada, Juan Jose Bosch, Emilia Gómez and Juan Pablo Bello. {\textless}br{\textgreater} {\textless}br{\textgreater} The MDB-stem-synth dataset is offered free of charge under the terms of the Creative Commons{\textless}br{\textgreater} Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0): http://creativecommons.org/licenses/by-nc/4.0/{\textless}br{\textgreater} {\textless}br{\textgreater} The dataset and its contents are made available on an "as is" basis and without warranties of any kind, including {\textless}br{\textgreater} without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or {\textless}br{\textgreater} completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, NYU is not {\textless}br{\textgreater} liable for, and expressly excludes, all liability for loss or damage however and whenever caused to anyone by any use of {\textless}br{\textgreater} the MDB-stem-synth dataset or any part of it. {\textless}br{\textgreater} Feedback{\textless}br{\textgreater} ——– Please help us improve MDB-stem-synth by sending your feedback to: justin.salamon@gmail.com{\textless}br{\textgreater} In case of a problem report please include as many details as possible.{\textless}br{\textgreater}},
	urldate = {2021-04-07},
	publisher = {Zenodo},
	author = {Salamon, Justin and Bittner, Rachel and Bonada, Jordi and Bosch, Juan Jose and Gómez, Emilia and Bello, Juan Pablo},
	month = nov,
	year = {2018},
	doi = {10.5281/ZENODO.1481172},
	note = {tex.copyright: Creative Commons Attribution-NonCommercial 4.0, Open Access},
}

@misc{carmine-emanuele_orchideasol_2020,
	title = {{OrchideaSOL}: an audio dataset of isolated musical notes, including mutes and extended playing techniques},
	shorttitle = {{OrchideaSOL}},
	url = {https://zenodo.org/record/3686252},
	abstract = {OrchideaSOL{\textless}br{\textgreater} =========={\textless}br{\textgreater} Version 1.0, February 2020.{\textless}br{\textgreater} Created By{\textless}br{\textgreater} ————– Carmine-Emanuele Cella (1), Daniele Ghisi (1), Vincent Lostanlen (2), Fabien Lévy (3), Joshua Fineberg (4), Yan Maresz (5){\textless}br{\textgreater} {\textless}br{\textgreater} (1): UC Berkeley{\textless}br{\textgreater} (2): New York University{\textless}br{\textgreater} (3): Columbia University{\textless}br{\textgreater} (4): Boston University{\textless}br{\textgreater} (5): Conservatoire de Paris Description{\textless}br{\textgreater} ————— {\textless}br{\textgreater} OrchideaSOL is a dataset of 13265 samples, each containing a single musical note from one of 14 different instruments: Bass Tuba French Horn Trombone Trumpet in C Accordion Contrabass Violin Viola Violoncello Bassoon Clarinet in B-flat Flute Oboe Alto Saxophone These sounds were originally recorded at Ircam in Paris (France) between 1996 and 1999, as part of a larger project named Studio On Line (SOL). One asset of OrchideaSOL is that it contains many combinations of mutes and extended playing techniques.{\textless}br{\textgreater} {\textless}br{\textgreater} The OrchideaSOL audio data can be used for creative purposes insofar at the use complies with the Ircam Forum License. Please visit: https://forum.ircam.fr/legal/contrat-de-licence-forum-ircam/ {\textless}br{\textgreater} The OrchideaSOL metadata can be used for creative purposes insofar at the use complies with the Creative Commons Attribution 4.0 International license (see below).{\textless}br{\textgreater} {\textless}br{\textgreater} OrchideaSOL can be used for education and research purposes. In particular, it can be employed as a dataset for training and/or evaluating music information retrieval (MIR) systems, for tasks such as instrument recognition, playing technique recognition, or fundamental frequency estimation. For this purpose, we provide an official 5-fold split of OrchideaSOL. This split has been carefully balanced in terms of instrumentation, pitch range, and dynamics. For the sake of research reproducibility, we encourage users of OrchideaSOL to adopt this split and report their results in terms of average performance across folds. Data Files{\textless}br{\textgreater} ————– OrchideaSOL contains 13265 audio clips as WAV files, sampled at 44.1 kHz, with a single channel (mono), at a bit depth of 16. This is equivalent to the audio quality of a compact disc. Audio clips vary in duration between two and ten seconds. Every audio file has a file path of the form:{\textless}br{\textgreater} \&lt;FAMILY\&gt;/\&lt;INSTRUMENT\&gt;\&lt;+MUTE\&gt;/\&lt;TECHNIQUE\&gt;/\&lt;INSTR\&gt;\&lt;+M\&gt;-\&lt;TECH\&gt;-\&lt;PITCH\&gt;-\&lt;DYN\&gt;-\&lt;INSTANCE\&gt;-\&lt;MISC\&gt;.wav {\textless}br{\textgreater} where: \&lt;FAMILY\&gt; corresponds to the instrument family: "Brass", "Keyboards" (includes accordion), "Strings", and "Winds" (i.e., woodwinds). \&lt;INSTRUMENT\&gt; is the full name of the instrument. \&lt;+MUTE\&gt; is the type of mute being used, such as "wah", "harmon", "piombo", or "sordina". If there is no mute, this field is absent. \&lt;TECHNIQUE\&gt; is the type of playing technique. \&lt;INSTR\&gt; is the abbreviation of the instrument. \&lt;+M\&gt; is the abbreviation of the type of mute, if applicable. \&lt;TECH\&gt; is the abbreviation of playing technique. \&lt;PITCH\&gt; denotes the pitch of the musical note. This pitch is encoded in the American standard pitch notation: pitch class (C means "do") followed by pitch octave. According to this convention, A4 has a fundamental frequency of 440 Hz. \&lt;DYN\&gt; denotes the intensity dynamics, ranked from pp (pianissimo) to ff (fortissimo). \&lt;INSTANCE\&gt; contains additional information, when applicable. For example, for bowed string instruments, the same pitch may sometimes be achieved on different positions and different strings, resulting in small timbre differences. In this case the label "1c", "2c", "3c", or "4c" denotes the string which is being bowed. (The letter c originates from the word "corde", which means string in French.) By convention, the first string is the one with the highest pitch when played as an open string. Furthermore, on some wind instruments, the same note was played multiple times, e.g. at multiple durations. In this case, we use the label "alt1", "alt2", etc. to denote alternative instances of the note. If none of these tags apply, the \&lt;INSTANCE\&gt; field becomes "N", which stands for "Not Applicable". \&lt;MISC\&gt; contains additional information, if applicable. In OrchideaSOL, some pitches were never recorded, and thus missing from the chromatic scale. In this case, the \&lt;MISC\&gt; tag contains a letter "R", to denote the fact that the corresponding WAV file has been obtained by transforming a different audio clip via some digital frequency transposition (similar to Auto-Tune). The letter "R" stands for "resampled". Furthermore, some pitches were slightly out of tune in comparison with the A440 tuning standard. Again, we applied some digital frequency transposition to correct them and put them exactly in tune. The amount of frequency transposition is measured in "cents" of an equal-tempered semitone. The letter "T" stands for "tuned". Because we employed a high-fidelity algorithm for frequency transposition, and because the amount of digital frequency transposition is small, the timbre of pitch-corrected notes remains faithful to the instrument. If none of these tags apply, the \&lt;MISC\&gt; field becomes "N", which stands for "natural"; in this case, the note is distributed exactly as it was recorded in the studio. For example, "Strings/Violin+sordina/tremolo/Vn+S-trem-A4-mf-4c-T13d\_R200d.wav" corresponds to: a violin sound; equipped with a sordina mute; played in the tremolo playing technique; at pitch A4 (440 Hz); with mezzoforte dynamics; on the fourth string (i.e. the lowest); resampled from a B4 by lowering pitch by a semitone, i.e. 100 cents (R100d) lowered by 13 cents (T22d) to match the A440 tuning standard. The audio data for OrchideaSOL is not directly downloadable on Zenodo. Rather, it can be downloaded for free after registering to the Ircam forum. Please visit: https://forum.ircam.fr/ Metadata File{\textless}br{\textgreater} ——————- The OrchideaSOL\_metadata.csv file contains 13265 rows, one for each audio clip. It can be opened by a text editor or by a spreadsheet software application. It contains 13 columns: Path to the WAV file, in UNIX filesystem format. For Windows compatibility, replace the slashes ("/") by backslashes ("{\textbackslash}"). Ex: "Strings/Violin+sordina/tremolo/Vn+S-trem-A4-mf-4c-T13d\_R200d.wav" Fold ID. Either equal to 0, 1, 2, 3, or 4. Family. Ex: "Brass" Instrument abbreviation. Ex: "BTb" Instrument name in full. Ex: "Bass Tuba" Technique abbreviation. Technique name in full. Pitch. Ex: "A\#1" Pitch ID in MIDI format. Ex: 34. Integer in the range 0-127. Dynamics. Ex: "ff". Dynamics ID. Integer. pp maps to 0 and ff maps to 4. The higher, the louder. Instance ID. Integer in the range 0-4 String ID. Equal to 1, 2, 3, 4, or empty if not applicable. "Needed digital retuning". TRUE if the file has been pitch-shifted with digital audio effects; FALSE otherwise. Conditions of Use{\textless}br{\textgreater} ———————— OrchideaSOL was created in 2020 by Carmine-Emanuele Cella, Daniele Ghisi, Vincent Lostanlen, Fabien Lévy, Joshua Fineberg, and Yan Maresz. OrchideaSOL is a derivative of SOL. We wish to thank Hugues Vinet, Greg Beller, and all coordinators of the Ircam Forum for their authorization to upload the metadata of OrchideaSOL to Zenodo. The audio samples in OrchideaSOL are offered free of charge under the Ircam Forum License. Please visit: https://forum.ircam.fr/legal/contrat-de-licence-forum-ircam/ The dataset and its contents are made available on an "as is" basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, the authors are not liable for, and expressly exclude all liability for, loss or damage however and whenever caused to anyone by any use of the OrchideaSOL dataset or any part of it. Versions{\textless}br{\textgreater} ———–{\textless}br{\textgreater} 1.0 was released on February 24th, 2020. Feedback{\textless}br{\textgreater} ————- Please help us improve OrchideaSOL by sending your feedback to:{\textless}br{\textgreater} carmine.cella@berkeley.edu For issues regarding the metadata encoding, the five-fold split, or the OrchideaSOL module in mirdata, please write to:{\textless}br{\textgreater} vincent.lostanlen@nyu.edu In case of a problem, please include as many details as possible.},
	urldate = {2021-04-07},
	publisher = {Zenodo},
	author = {Carmine-Emanuele, Cella and Ghisi, Daniele and Lostanlen, Vincent and Lévy, Fabien and Fineberg, Joshua and Maresz, Yan},
	month = feb,
	year = {2020},
	doi = {10.5281/ZENODO.3686252},
	note = {tex.copyright: Creative Commons Attribution 4.0 International, Open Access},
	keywords = {audio signal processing, computer music, music cognition, music information retrieval},
}

@inproceedings{arjovsky_wasserstein_2017,
	address = {Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Wasserstein generative adversarial networks},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	booktitle = {Proceedings of the 34th international conference on machine learning},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {214--223},
}

@inproceedings{antognini_audio_2019,
	address = {Brighton, United Kingdom},
	title = {Audio texture synthesis with random neural networks: {Improving} diversity and quality},
	isbn = {978-1-4799-8131-1},
	shorttitle = {Audio {Texture} {Synthesis} with {Random} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8682598/},
	doi = {10.1109/ICASSP.2019.8682598},
	urldate = {2021-01-30},
	booktitle = {Proceedings of the {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Antognini, Joseph M. and Hoffman, Matt and Weiss, Ron J.},
	month = may,
	year = {2019},
	pages = {3587--3591},
}

@article{zhao_understanding_2011,
	title = {Understanding auditory spectro-{Temporal} receptive fields and their changes with input statistics by efficient coding principles},
	volume = {7},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1002123},
	doi = {10.1371/journal.pcbi.1002123},
	number = {8},
	urldate = {2021-04-07},
	journal = {PLoS Computational Biology},
	author = {Zhao, Lingyun and Zhaoping, Li},
	editor = {Graham, Lyle J.},
	month = aug,
	year = {2011},
	pages = {e1002123},
}

@article{ethington_seawave_1994,
	title = {{SeaWave}: {A} {System} for {Musical} {Timbre} {Description}},
	volume = {18},
	issn = {01489267},
	shorttitle = {{SeaWave}},
	url = {https://www.jstor.org/stable/3680520?origin=crossref},
	doi = {10.2307/3680520},
	number = {1},
	urldate = {2021-04-07},
	journal = {Computer Music Journal},
	author = {Ethington, Russ and Punch, Bill},
	year = {1994},
	pages = {30},
}

@inproceedings{bond-taylor_gradient_2021,
	title = {Gradient {Origin} {Networks}},
	url = {http://arxiv.org/abs/2007.02798},
	abstract = {This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.},
	urldate = {2021-02-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bond-Taylor, Sam and Willcocks, Chris G.},
	month = jan,
	year = {2021},
	keywords = {68T01 (Primary), 68T07 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, G.3, I.4.0, I.5.0, \_tablet},
}

@inproceedings{dosovitskiy_generating_2016,
	title = {Generating images with perceptual similarity metrics based on deep networks},
	volume = {29},
	url = {http://arxiv.org/abs/1602.02644},
	abstract = {Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.},
	urldate = {2020-10-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{che_your_2020,
	title = {Your {GAN} is secretly an energy-based model and you should use discriminator driven latent sampling},
	volume = {33},
	url = {http://arxiv.org/abs/2003.06060},
	abstract = {We show that the sum of the implicit generator log-density \${\textbackslash}log p\_g\$ of a GAN with the logit score of the discriminator defines an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal, thus making it possible to improve on the typical generator (with implicit density \$p\_g\$). To make that practical, we show that sampling from this modified density can be achieved by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. This can be achieved by running a Langevin MCMC in latent space and then applying the generator function, which we call Discriminator Driven Latent Sampling{\textasciitilde}(DDLS). We show that DDLS is highly efficient compared to previous methods which work in the high-dimensional pixel space and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN{\textasciitilde}{\textbackslash}citep\{sngan\} from \$8.22\$ to \$9.09\$ which is even comparable to the class-conditional BigGAN{\textasciitilde}{\textbackslash}citep\{biggan\} model. This achieves a new state-of-the-art in unconditional image synthesis setting without introducing extra parameters or additional training.},
	urldate = {2021-02-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
	month = mar,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {12275--12287},
}

@inproceedings{de_brabandere_dynamic_2016,
	title = {Dynamic {Filter} {Networks}},
	volume = {29},
	url = {http://arxiv.org/abs/1605.09673},
	abstract = {In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.},
	urldate = {2021-02-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {De Brabandere, Bert and Jia, Xu and Tuytelaars, Tinne and Van Gool, Luc},
	month = jun,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@book{fant_acoustic_1971,
	title = {Acoustic theory of speech production: {With} calculations based on {X}-{Ray} studies of russian articulations},
	isbn = {978-90-279-1600-6},
	shorttitle = {Acoustic {Theory} of {Speech} {Production}},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110873429/html},
	urldate = {2021-03-29},
	publisher = {De Gruyter},
	author = {Fant, Gunnar},
	month = dec,
	year = {1971},
	doi = {10.1515/9783110873429},
}

@inproceedings{grathwohl_your_2020,
	title = {Your classifier is secretly an energy based model and you should treat it like one},
	url = {http://arxiv.org/abs/1912.03263},
	abstract = {We propose to reinterpret a standard discriminative classifier of p(y{\textbar}x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x{\textbar}y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.},
	urldate = {2021-01-17},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Jörn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{rothlauf_synthesising_2006,
	address = {Berlin, Heidelberg},
	title = {Synthesising {Timbres} and {Timbre}-{Changes} from {Adjectives}/{Adverbs}},
	volume = {3907},
	isbn = {978-3-540-33237-4 978-3-540-33238-1},
	url = {http://link.springer.com/10.1007/11732242%5F63},
	urldate = {2021-04-07},
	booktitle = {Applications of {Evolutionary} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	editor = {Rothlauf, Franz and Branke, Jürgen and Cagnoni, Stefano and Costa, Ernesto and Cotta, Carlos and Drechsler, Rolf and Lutton, Evelyne and Machado, Penousal and Moore, Jason H. and Romero, Juan},
	year = {2006},
	doi = {10.1007/11732242_63},
	pages = {664--675},
}

@inproceedings{ha_hypernetworks_2016,
	title = {{HyperNetworks}},
	url = {http://arxiv.org/abs/1609.09106},
	abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
	urldate = {2021-02-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ha, David and Dai, Andrew and Le, Quoc V.},
	month = dec,
	year = {2016},
	keywords = {Computer Science - Machine Learning, \_tablet},
}

@article{haque_high-fidelity_2020,
	title = {High-fidelity audio generation and representation learning with guided adversarial autoencoder},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9272282/},
	doi = {10.1109/ACCESS.2020.3040797},
	urldate = {2021-04-14},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Haque, Kazi Nazmul and Rana, Rajib and Schuller, Bjorn W.},
	year = {2020},
	pages = {223509--223528},
}

@inproceedings{jansen_unsupervised_2018,
	title = {Unsupervised {Learning} of {Semantic} {Audio} {Representations}},
	url = {http://arxiv.org/abs/1711.02209},
	doi = {10.1109/ICASSP.2018.8461684},
	abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41\% and 84\% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.},
	urldate = {2020-10-13},
	booktitle = {2018 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
	year = {2018},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	pages = {126--130},
}

@inproceedings{heusel_gans_2017,
	address = {Long Beach, California, USA},
	series = {{NIPS}'17},
	title = {{GANs} trained by a two time-scale update rule converge to a local nash equilibrium},
	volume = {31},
	isbn = {978-1-5108-6096-4},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fréchet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year = {2017},
	pages = {6629--6640},
}

@incollection{lecun_tutorial_2006,
	title = {A {Tutorial} on {Energy}-{Based} {Learning}},
	isbn = {978-0-262-52804-7},
	abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each conﬁguration of the variables. Inference consists in clamping the value of observed variables and ﬁnding conﬁgurations of the remaining variables that minimize the energy. Learning consists in ﬁnding an energy function in which observed conﬁgurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random ﬁelds, maximum margin Markov networks, and several manifold learning methods.},
	booktitle = {Predicting {Structured} {Data}},
	publisher = {MIT Press},
	author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc’Aurelio and Huang, Fu Jie},
	year = {2006},
	pages = {59},
}

@inproceedings{kilgour_frechet_2019,
	title = {Fréchet audio distance: {A} reference-{Free} metric for evaluating music enhancement algorithms},
	shorttitle = {Fréchet {Audio} {Distance}},
	url = {http://www.isca-speech.org/archive/Interspeech%5F2019/abstracts/2219.html},
	doi = {10.21437/Interspeech.2019-2219},
	urldate = {2021-04-29},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Kilgour, Kevin and Zuluaga, Mauricio and Roblek, Dominik and Sharifi, Matthew},
	month = sep,
	year = {2019},
	pages = {2350--2354},
}

@inproceedings{hershey_cnn_2017,
	address = {New Orleans, LA},
	title = {{CNN} architectures for large-scale audio classification},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7952132/},
	doi = {10.1109/ICASSP.2017.7952132},
	urldate = {2021-04-29},
	booktitle = {2017 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
	month = mar,
	year = {2017},
	pages = {131--135},
}

@inproceedings{de_man_adaptive_2014,
	title = {Adaptive control of amplitude distortion effects},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=17118},
	booktitle = {Audio engineering society conference: 53rd international conference: {Semantic} audio},
	author = {De Man, Brecht and Reiss, Joshua D.},
	month = jan,
	year = {2014},
}

@inproceedings{zhao_transferring_2020,
	address = {Barcelona, Spain},
	title = {Transferring neural speech waveform synthesizers to musical instrument sounds generation},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053047/},
	doi = {10.1109/ICASSP40776.2020.9053047},
	urldate = {2021-05-03},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Zhao, Yi and Wang, Xin and Juvela, Lauri and Yamagishi, Junichi},
	month = may,
	year = {2020},
	pages = {6269--6273},
}

@inproceedings{song_efficient_2020,
	title = {Efficient {WaveGlow}: {An} improved {WaveGlow} vocoder with enhanced speed},
	shorttitle = {Efficient {WaveGlow}},
	url = {http://www.isca-speech.org/archive/Interspeech%5F2020/abstracts/2172.html},
	doi = {10.21437/Interspeech.2020-2172},
	urldate = {2021-05-03},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Song, Wei and Xu, Guanghui and Zhang, Zhengchen and Zhang, Chao and He, Xiaodong and Zhou, Bowen},
	month = oct,
	year = {2020},
	pages = {225--229},
}

@techreport{itu-r_bs1534-3_method_2015,
	title = {Method for the subjective assessment of intermediate quality level of audio systems},
	institution = {ITU-R},
	author = {BS.1534-3, ITU-R},
	year = {2015},
}

@article{schoeffler_webmushra_2018,
	title = {{webMUSHRA} — {A} comprehensive framework for web-based listening tests},
	volume = {6},
	issn = {2049-9647},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.187/},
	doi = {10.5334/jors.187},
	urldate = {2021-05-03},
	journal = {Journal of Open Research Software},
	author = {Schoeffler, Michael and Bartoschek, Sarah and Stöter, Fabian-Robert and Roess, Marlene and Westphal, Susanne and Edler, Bernd and Herre, Jürgen},
	month = feb,
	year = {2018},
	pages = {8},
}

@article{milne_online_2020,
	title = {An online headphone screening test based on dichotic pitch},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-020-01514-0},
	doi = {10.3758/s13428-020-01514-0},
	abstract = {Abstract Online experimental platforms can be used as an alternative to, or complement, lab-based research. However, when conducting auditory experiments via online methods, the researcher has limited control over the participants’ listening environment. We offer a new method to probe one aspect of that environment, headphone use. Headphones not only provide better control of sound presentation but can also “shield” the listener from background noise. Here we present a rapid ({\textless} 3 min) headphone screening test based on Huggins Pitch (HP), a perceptual phenomenon that can only be detected when stimuli are presented dichotically. We validate this test using a cohort of “Trusted” online participants who completed the test using both headphones and loudspeakers. The same participants were also used to test an existing headphone test (AP test; Woods et al., 2017, Attention Perception Psychophysics ). We demonstrate that compared to the AP test, the HP test has a higher selectivity for headphone users, rendering it as a compelling alternative to existing methods. Overall, the new HP test correctly detects 80\% of headphone users and has a false-positive rate of 20\%. Moreover, we demonstrate that combining the HP test with an additional test–either the AP test or an alternative based on a beat test (BT)–can lower the false-positive rate to {\textasciitilde} 7\%. This should be useful in situations where headphone use is particularly critical (e.g., dichotic or spatial manipulations). Code for implementing the new tests is publicly available in JavaScript and through Gorilla (gorilla.sc).},
	urldate = {2021-05-03},
	journal = {Behavior Research Methods},
	author = {Milne, Alice E. and Bianco, Roberta and Poole, Katarina C. and Zhao, Sijia and Oxenham, Andrew J. and Billig, Alexander J. and Chait, Maria},
	month = dec,
	year = {2020},
}

@inproceedings{mendonca_statistical_2018,
	title = {Statistical tests with {MUSHRA} data},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=19402},
	booktitle = {Audio engineering society convention 144},
	author = {Mendonça, Catarina and Delikaris-Manias, Symeon},
	month = may,
	year = {2018},
}

@inproceedings{nistal_drumgan_2020,
	address = {Montréal},
	title = {{DrumGAN}: {Synthesis} of drum sounds with timbral feature conditioning using generative adversarial networks},
	shorttitle = {{DrumGAN}},
	abstract = {Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the 21th international society for music information retrieval conference},
	author = {Nistal, J. and Lattner, S. and Richard, G.},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@book{reiss_audio_2015,
	address = {Boca Raton London New York},
	title = {Audio effects: theory, implementation and application},
	isbn = {978-1-4665-6029-1 978-1-4665-6028-4},
	shorttitle = {Audio effects},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Reiss, Joshua D. and McPherson, Andrew P.},
	year = {2015},
}

@article{roche_make_2021,
	title = {Make that sound more \textit{{Metallic}}: {Towards} a perceptually relevant control of the timbre of synthesizer sounds using a variational autoencoder},
	volume = {4},
	issn = {2514-3298},
	shorttitle = {Make {That} {Sound} {More} \textit{{Metallic}}},
	url = {http://transactions.ismir.net/articles/10.5334/tismir.76/},
	doi = {10.5334/tismir.76},
	abstract = {Article: Make That Sound More \textit{Metallic}: Towards a Perceptually Relevant Control of the Timbre of Synthesizer Sounds Using a Variational Autoencoder},
	number = {1},
	urldate = {2021-05-20},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Roche, Fanny and Hueber, Thomas and Garnier, Maëva and Limier, Samuel and Girin, Laurent},
	month = may,
	year = {2021},
	note = {tex.copyright: Authors who publish with this journal agree to the following terms: Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a Creative Commons Attribution License that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal. Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal. Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See The Effect of Open Access ). All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on Educational Fair Use , please see this useful checklist prepared by Columbia University Libraries . All copyright of third-party content posted here for research purposes belongs to its original owners. Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	pages = {52--66},
}

@inproceedings{kingma_improved_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Improved variational inference with inverse autoregressive flow},
	isbn = {978-1-5108-3881-9},
	abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
	urldate = {2021-05-31},
	booktitle = {Proceedings of the 30th international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	month = dec,
	year = {2016},
	pages = {4743--4751},
}

@inproceedings{lucic_are_2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Are {GANs} created equal? a large-scale study},
	shorttitle = {Are {GANs} created equal?},
	abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].},
	urldate = {2021-06-15},
	booktitle = {Proceedings of the 32nd international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain},
	month = dec,
	year = {2018},
	pages = {698--707},
}

@article{le-khac_contrastive_2020,
	title = {Contrastive {Representation} {Learning}: {A} {Framework} and {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Contrastive {Representation} {Learning}},
	doi = {10.1109/ACCESS.2020.3031549},
	abstract = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.},
	year = {2020},
	keywords = {Computational modeling, Contrastive learning, Data models, Feature extraction, Learning systems, Machine learning, Natural language processing, Task analysis, deep learning, machine learning, representation learning, self-supervised learning, unsupervised learning},
	pages = {193907--193934},
}

@incollection{reiss_overdrive_2015,
	address = {Boca Raton London New York},
	title = {Overdrive, {Distortion}, and {Fuzz}},
	isbn = {978-1-4665-6029-1 978-1-4665-6028-4},
	booktitle = {Audio effects: theory, implementation and application},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Reiss, Joshua D. and McPherson, Andrew P.},
	year = {2015},
	pages = {167--188},
}

@inproceedings{sitzmann_implicit_2020,
	title = {Implicit neural representations with periodic activation functions},
	volume = {33},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
	urldate = {2020-10-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \_tablet},
}

@incollection{siedenburg_semantics_2019,
	address = {Cham},
	title = {The {Semantics} of {Timbre}},
	volume = {69},
	isbn = {978-3-030-14831-7 978-3-030-14832-4},
	abstract = {Because humans lack a sensory vocabulary for auditory experiences, timbral qualities of sounds are often conceptualized and communicated through readily available sensory attributes from different modalities (e.g., bright, warm, sweet) but also through the use of onomatopoeic attributes (e.g., ringing, buzzing, shrill) or nonsensory attributes relating to abstract constructs (e.g., rich, complex, harsh). The analysis of the linguistic description of timbre, or timbre semantics, can be considered as one way to study its perceptual representation empirically. In the most commonly adopted approach, timbre is considered as a set of verbally defined perceptual attributes that represent the dimensions of a semantic timbre space. Previous studies have identified three salient semantic dimensions for timbre along with related acoustic properties. Comparisons with similarity-based multidimensional models confirm the strong link between perceiving timbre and talking about it. Still, the cognitive and neural mechanisms of timbre semantics remain largely unknown and underexplored, especially when one looks beyond the case of acoustic musical instruments.},
	urldate = {2020-05-19},
	booktitle = {Timbre: {Acoustics}, {Perception}, and {Cognition}},
	publisher = {Springer International Publishing},
	author = {Saitis, Charalampos and Weinzierl, Stefan},
	editor = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	month = may,
	year = {2019},
	keywords = {unread},
	pages = {119--149},
}

@incollection{siedenburg_present_2019,
	address = {Cham},
	title = {The {Present}, {Past}, and {Future} of {Timbre} {Research}},
	isbn = {978-3-030-14832-4},
	booktitle = {Timbre: {Acoustics}, {Perception}, and {Cognition}},
	publisher = {Springer International Publishing},
	author = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen},
	editor = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	month = may,
	year = {2019},
	doi = {10.1007/978-3-030-14832-4_1},
	pages = {1--19},
}

@article{wessel_timbre_1979,
	title = {Timbre {Space} as a {Musical} {Control} {Structure}},
	volume = {3},
	issn = {01489267},
	doi = {10.2307/3680283},
	number = {2},
	urldate = {2019-12-14},
	journal = {Computer Music Journal},
	author = {Wessel, David L.},
	month = jun,
	year = {1979},
	keywords = {unread},
	pages = {45},
}

@article{aramaki_controlling_2011,
	title = {Controlling the perceived material in an impact sound synthesizer},
	volume = {19},
	issn = {1558-7924},
	doi = {10.1109/TASL.2010.2047755},
	abstract = {In this paper, we focused on the identification of the perceptual properties of impacted materials to provide an intuitive control of an impact sound synthesizer. To investigate such properties, impact sounds from everyday life objects, made of different materials (wood, metal and glass), were recorded and analyzed. These sounds were synthesized using an analysis-synthesis technique and tuned to the same chroma. Sound continua were created to simulate progressive transitions between materials. Sounds from these continua were then used in a categorization experiment to determine sound categories representative of each material (called typical sounds). We also examined changes in electrical brain activity (using event related potentials (ERPs) method) associated with the categorization of these typical sounds. Moreover, acoustic analysis was conducted to investigate the relevance of acoustic descriptors known to be relevant for both timbre perception and material identification. Both acoustic and electrophysiological data confirmed the importance of damping and highlighted the relevance of spectral content for material perception. Based on these findings, controls for damping and spectral shaping were tested in synthesis applications. A global control strategy, with a three-layer architecture, was proposed for the synthesizer allowing the user to intuitively navigate in a “material space” and defining impact sounds directly from the material label. A formal perceptual evaluation was finally conducted to validate the proposed control strategy.},
	number = {2},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Aramaki, Mitsuko and Besson, Mireille and Kronland-Martinet, Richard and Ystad, Sølvi},
	month = feb,
	year = {2011},
	keywords = {Acoustic materials, Analysis–synthesis, Brain modeling, Conducting materials, Damping, Electrophysiology, Glass, Inorganic materials, Shape control, Synthesizers, Timbre, control, event related potentials, impact sounds, mapping, material, sound categorization, timbre},
	pages = {301--314},
}

@article{mcadams_perceptual_1995,
	title = {Perceptual scaling of synthesized musical timbres: {Common} dimensions, specificities, and latent subject classes},
	volume = {58},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Perceptual scaling of synthesized musical timbres},
	doi = {10.1007/BF00419633},
	number = {3},
	urldate = {2019-10-14},
	journal = {Psychological Research},
	author = {McAdams, Stephen and Winsberg, Suzanne and Donnadieu, Sophie and De Soete, Geert and Krimphoff, Jochen},
	month = dec,
	year = {1995},
	pages = {177--192},
}

@article{kendall_verbal_1993,
	title = {Verbal attributes of simultaneous wind instrument timbres: {I}. von bismarck's adjectives},
	volume = {10},
	issn = {0730-7829},
	shorttitle = {Verbal {Attributes} of {Simultaneous} {Wind} {Instrument} {Timbres}},
	doi = {10.2307/40285583},
	abstract = {A study on the verbal attributes of timbre was conducted in an effort to interpret the dimensional configuration of the similarity spaces of simultaneously sounding wind instrument timbres. In the first experiment, subjects rated 10 wind instrument dyads on eight factorially pure semantic differentials from von Bismarck's (1974a) experiments. Results showed that the semantic differentials failed to differentiate among the 10 timbres. The semantic differential methodology was changed to verbal attribute magnitude estimation (VAME), in which a timbre is assigned an amount of a given attribute. This procedure resulted in better differentiation among the 10 timbres, the first factor including attributes such as heavy, hard, and loud, the second factor involving sharp and complex, a contrast with von Bismarck's results. Results of the VAME analysis separated alto saxophone dyads from all others, but mapped only moderately well onto the perceptual similarity spaces. It was suggested that many of the von Bismarck adjectives lacked ecological validity.},
	number = {4},
	urldate = {2020-12-15},
	journal = {Music Perception},
	author = {Kendall, Roger A. and Carterette, Edward C.},
	month = jul,
	year = {1993},
	pages = {445--467},
}

@inproceedings{disley_timbral_2006,
	title = {Timbral description of musical instruments},
	abstract = {Musicians intuitively describe timbre using adjectives such as bright or clear. This research aims to establish a set of uncorrelated and consistently used timbral adjectives that could be used to control a future synthesis system. Fifteen timbral adjectives were selected from previous studies and refined in a pilot experiment to: bright, clear, warm, thin,},
	booktitle = {Proceedings of the 9th international conference of music perception and cognition and 6th conference of the european society for the cognitive sciences of music},
	author = {Disley, Alastair C. and Howard, David M. and Hunt, Andy D.},
	month = aug,
	year = {2006},
	keywords = {unread},
}

@incollection{siedenburg_audio_2019,
	address = {Cham},
	title = {Audio {Content} {Descriptors} of {Timbre}},
	volume = {69},
	isbn = {978-3-030-14831-7 978-3-030-14832-4},
	urldate = {2021-01-29},
	booktitle = {Timbre: {Acoustics}, {Perception}, and {Cognition}},
	publisher = {Springer International Publishing},
	author = {Caetano, Marcelo and Saitis, Charalampos and Siedenburg, Kai},
	editor = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	month = may,
	year = {2019},
	doi = {10.1007/978-3-030-14832-4_11},
	pages = {297--333},
}

@article{elliott_acoustic_2013,
	title = {Acoustic structure of the five perceptual dimensions of timbre in orchestral instrument tones},
	volume = {133},
	issn = {0001-4966},
	doi = {10.1121/1.4770244},
	abstract = {Attempts to relate the perceptual dimensions of timbre to quantitative acoustical dimensions have been tenuous, leading to claims that timbre is an emergent property, if measurable at all. Here, a three-pronged analysis shows that the timbre space of sustained instrument tones occupies 5 dimensions and that a specific combination of acoustic properties uniquely determines gestalt perception of timbre. Firstly, multidimensional scaling (MDS) of dissimilarity judgments generated a perceptual timbre space in which 5 dimensions were cross-validated and selected by traditional model comparisons. Secondly, subjects rated tones on semantic scales. A discriminant function analysis (DFA) accounting for variance of these semantic ratings across instruments and between subjects also yielded 5 significant dimensions with similar stimulus ordination. The dimensions of timbre space were then interpreted semantically by rotational and reflectional projection of the MDS solution into two DFA dimensions. Thirdly, to relate this final space to acoustical structure, the perceptual MDS coordinates of each sound were regressed with its joint spectrotemporal modulation power spectrum. Sound structures correlated significantly with distances in perceptual timbre space. Contrary to previous studies, most perceptual timbre dimensions are not the result of purely temporal or spectral features but instead depend on signature spectrotemporal patterns.},
	number = {1},
	urldate = {2019-12-14},
	journal = {The Journal of the Acoustical Society of America},
	author = {Elliott, Taffeta M. and Hamilton, Liberty S. and Theunissen, Frédéric E.},
	month = jan,
	year = {2013},
	pmid = {23297911},
	note = {tex.pmcid: PMC3548835},
	keywords = {\_tablet, to read this week},
	pages = {389--404},
}

@article{zacharakis_interlanguage_2014,
	title = {An interlanguage study of musical timbre semantic dimensions and their acoustic correlates},
	volume = {31},
	issn = {07307829, 15338312},
	doi = {10.1525/mp.2014.31.4.339},
	number = {4},
	urldate = {2019-10-14},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
	month = apr,
	year = {2014},
	pages = {339--358},
}

@article{zacharakis_revisiting_2016,
	title = {Revisiting the luminance-{Texture}-{Mass} model for musical timbre semantics: {A} confirmatory approach and perspectives of extension},
	volume = {64},
	issn = {15494950},
	shorttitle = {Revisiting the luminance-{Texture}-{Mass} model for musical timbre semantics},
	doi = {10.17743/jaes.2016.0032},
	number = {9},
	urldate = {2020-08-18},
	journal = {Journal of the Audio Engineering Society},
	author = {Zacharakis, Asterios and Pastiadis, Konstantinos},
	month = sep,
	year = {2016},
	pages = {636--645},
}

@inproceedings{zacharakis_additive_2011,
	title = {An additive synthesis technique for independent modification of the auditory perceptions of brightness and warmth},
	abstract = {An algorithm that achieves independent modification of two low-level features that are correlated with the auditory perceptions of brightness and warmth was implemented. The perceptual validity of the algorithm was tested through a series of listening tests in order to examine whether the low-level modification was indeed perceived as independent and to investigate the influence of the fundamental frequency on the perceived modification. A Multidimensional Scaling analysis (MDS) on listener...},
	urldate = {2019-12-21},
	publisher = {Audio Engineering Society},
	author = {Zacharakis, Asteris and Reiss, Joshua},
	month = may,
	year = {2011},
	keywords = {unread},
}

@article{wallmark_creating_2019,
	title = {Creating novel tones from adjectives: {An} exploratory study using {FM} synthesis.},
	volume = {29},
	issn = {2162-1535, 0275-3987},
	shorttitle = {Creating novel tones from adjectives},
	doi = {10.1037/pmu0000240},
	abstract = {Perceptual studies of timbre semantics have revealed certain consistencies in the linguistic conceptualization of acoustic attributes. In the standard experimental paradigm, participants hear timbral stimuli and provide behavioral responses. However, it remains unclear the extent to which descriptive consistency would be observed if this paradigm were reversed, that is, if participants were instructed to create novel timbres in response to target adjectives. Given an unfamiliar synthesis interface, would musically trained participants craft similar timbral profiles for the same familiar adjectives? In this study, we explore timbre semantics using a novel frequency modulation (FM) synthesis production task. Participants (N ϭ 64) created unique timbral outputs in response to 20 common timbre descriptors drawn from orchestration treatises (e.g., brilliant, dull, harsh). Acoustic analyses of the resultant 1,280 signals, in conjunction with linear mixed-effects modeling and clustering analysis, indicate that participants were moderately consistent in their timbral creations. Word valence and arousal interacted to influence average spectral centroid and noisiness. Specifically, clearly positive and negative words produced significantly different acoustical profiles than more affectively neutral words. This result confirms a number of findings from the perceptual literature while offering preliminary evidence that affective dimensions of timbre semantics systematically influence sound production in an unfamiliar context.},
	number = {4},
	urldate = {2019-11-09},
	journal = {Psychomusicology: Music, Mind, and Brain},
	author = {Wallmark, Zachary and Frank, Robert J. and Nghiem, Linh},
	month = jul,
	year = {2019},
	keywords = {\_tablet, to read this week},
	pages = {188--199},
}

@article{steele_is_2006,
	title = {Is the {Bandwidth} for {Timbre} {Invariance} {Only} {One} {Octave}?},
	volume = {23},
	issn = {0730-7829, 1533-8312},
	doi = {10.1525/mp.2006.23.3.215},
	number = {3},
	urldate = {2019-10-14},
	journal = {Music Perception},
	author = {Steele, Kenneth M. and Williams, Amber K.},
	month = feb,
	year = {2006},
	pages = {215--220},
}

@article{mullensiefen_musicality_2014,
	title = {The musicality of non-{Musicians}: {An} index for assessing musical sophistication in the general population},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {Musicality} of {Non}-{Musicians}},
	doi = {10.1371/journal.pone.0089642},
	abstract = {Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of ‘musical sophistication’ which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.},
	number = {2},
	urldate = {2020-04-13},
	journal = {PLOS ONE},
	author = {Müllensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
	month = feb,
	year = {2014},
	keywords = {Behavior, Bioacoustics, Emotions, Memory, Music cognition, Music perception, Professions, Psychometrics},
	pages = {e89642},
}

@inproceedings{zacharakis_evidence_2020,
	address = {Thessaloniki, Greece (Online)},
	title = {Evidence for timbre space robustness to an uncontrolled online stimulus presentation},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Timbre}},
	author = {Zacharakis, Asterios and Hayes, Ben and Saitis, Charalampos and Pastiadis, Konstantinos},
	month = sep,
	year = {2020},
	note = {tex.copyright: All rights reserved},
}

@article{fabrigar_evaluating_1999,
	title = {Evaluating the use of exploratory factor analysis in psychological research},
	volume = {4},
	doi = {10.1037/1082-989X.4.3.272},
	number = {3},
	journal = {Psychological Methods},
	author = {Fabrigar, Leandre R and Wegener, Duane T and MacCallum, Robert C and Strahan, Erin J},
	month = sep,
	year = {1999},
	pages = {272--299},
}

@article{marozeau_effect_2007,
	title = {The effect of fundamental frequency on the brightness dimension of timbre},
	volume = {121},
	issn = {0001-4966},
	doi = {10.1121/1.2384910},
	number = {1},
	urldate = {2019-10-14},
	journal = {The Journal of the Acoustical Society of America},
	author = {Marozeau, Jeremy and de Cheveigné, Alain},
	month = jan,
	year = {2007},
	pages = {383--387},
}

@article{horn_rationale_1965,
	title = {A rationale and test for the number of factors in factor analysis},
	volume = {30},
	issn = {1860-0980},
	doi = {10.1007/BF02289447},
	abstract = {It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares “capitalization” on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted.},
	number = {2},
	urldate = {2020-07-09},
	journal = {Psychometrika},
	author = {Horn, John L.},
	month = jun,
	year = {1965},
	pages = {179--185},
}

@techreport{peeters_large_2004,
	title = {A large set of audio features for sound description (similarity and classification) in the {CUIDADO} project},
	institution = {IRCAM},
	author = {Peeters, Geoffroy},
	month = jan,
	year = {2004},
}

@article{krumhansl_tracing_nodate,
	title = {Tracing the dynamic changes in perceived tonal organization in a spatial representation of musical keys},
	doi = {10.1037/0033-295X.89.4.334},
	author = {Krumhansl, Carol L and Kessler, Edward J},
	pages = {35},
}

@article{bitton_vector-quantized_2020,
	title = {Vector-{Quantized} {Timbre} {Representation}},
	url = {http://arxiv.org/abs/2007.06349},
	abstract = {Timbre is a set of perceptual attributes that identifies different types of sound sources. Although its definition is usually elusive, it can be seen from a signal processing viewpoint as all the spectral features that are perceived independently from pitch and loudness. Some works have studied high-level timbre synthesis by analyzing the feature relationships of different instruments, but acoustic properties remain entangled and generation bound to individual sounds. This paper targets a more flexible synthesis of an individual timbre by learning an approximate decomposition of its spectral properties with a set of generative features. We introduce an auto-encoder with a discrete latent space that is disentangled from loudness in order to learn a quantized representation of a given timbre distribution. Timbre transfer can be performed by encoding any variable-length input signals into the quantized latent features that are decoded according to the learned timbre. We detail results for translating audio between orchestral instruments and singing voice, as well as transfers from vocal imitations to instruments as an intuitive modality to drive sound synthesis. Furthermore, we can map the discrete latent space to acoustic descriptors and directly perform descriptor-based synthesis.},
	urldate = {2020-10-29},
	journal = {arXiv:2007.06349 [cs, eess]},
	author = {Bitton, Adrien and Esling, Philippe and Harada, Tatsuya},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{demany_perceptual_nodate,
	title = {The perceptual reality of tone chroma in early infancy},
	author = {Demany, Laurent},
	keywords = {❓ Multiple DOI},
	pages = {11},
}

@article{kaiser_application_1960,
	title = {The application of electronic computers to factor analysis},
	volume = {20},
	issn = {1552-3888(Electronic),0013-1644(Print)},
	doi = {10.1177/001316446002000116},
	abstract = {Electronic computers facilitate greatly carrying out factor analysis. Computers will help in solving the communality problem and the question of the number of factors as well as the question of arbitrary factoring and the problem of rotation. "Cloacal short-cuts will not be necessary and the powerful methods of Guttman will be feasible." A library of programs essential for factor analysis is described, and the use of medium sized computers as the IBM 650 deprecated for factor analysis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Educational and Psychological Measurement},
	author = {Kaiser, Henry F.},
	month = apr,
	year = {1960},
	pages = {141--151},
}

@article{cattell_scree_1966,
	title = {The {Scree} {Test} {For} {The} {Number} {Of} {Factors}},
	volume = {1},
	issn = {0027-3171},
	doi = {10.1207/s15327906mbr0102_10},
	number = {2},
	urldate = {2020-08-19},
	journal = {Multivariate Behavioral Research},
	author = {Cattell, Raymond B.},
	year = {1966},
	pages = {245--276},
}

@article{zwick_comparison_1986,
	title = {Comparison of five rules for determining the number of components to retain},
	volume = {99},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.99.3.432},
	abstract = {Investigated the performance of 5 methods for determining the number of components to retain—J. L. Horn's (see record 1965-13273-001) parallel analysis, W. F. Velicer's (see record 1977-00166-001) minimum average partial (MAP), R. B. Cattell's (see PA, Vol 41:969) scree test, M. S. Bartlett's (1950) chi-square test, and H. F. Kaiser's (see record 1960-06772-001) eigenvalue greater than 1 rule—across 7 systematically varied conditions (sample size, number of variables, number of components, component saturation, equal or unequal numbers of variables for each component, and the presence or absence of unique and complex variables). Five sample correlation matrices were generated at each of 2 sample sizes from the 48 known population correlation matrices representing 6 levels of component pattern complexity. Results indicate that the performance of the parallel analysis and MAP methods was generally the best across all situations; the scree test was generally accurate but variable; and Bartlett's chi-square test was less accurate and more variable than the scree test. Kaiser's method tended to severely overestimate the number of components. (65 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Zwick, William R. and Velicer, Wayne F.},
	month = may,
	year = {1986},
	keywords = {Factor Analysis, Statistical Correlation},
	pages = {432--442},
}

@article{ren_using_2020,
	title = {Using faust {DSL} to develop custom, sample accurate {DSP} code and audio plugins for the web browser},
	volume = {68},
	url = {https://www-aes-org.ezproxy.library.qmul.ac.uk/e-lib/browse.cfm?elib=20987},
	doi = {10.17743/jaes.2020.0014},
	abstract = {The development and porting of virtual instruments or audio effects on the Web is a hot topic. Several initiatives are emerging, from industry-driven ones (e.g., Propellerhead Rack Extension running on the Web1) to more community-based open-source projects [1]. Most of them aim at adapting existing code bases (usually developed in native languages like C/C++) as well as facilitating the use of existing audio Digital Signal Processing (DSP) languages and platforms. Our two teams previously...},
	number = {10},
	urldate = {2020-12-10},
	journal = {Journal of the Audio Engineering Society},
	author = {Ren, Shihong and Letz, Stéphane and Orlarey, Yann and Michon, Romain and Fober, Dominique and Buffa, Michel and Lebrun, Jerome},
	month = nov,
	year = {2020},
	pages = {703--716},
}

@inproceedings{finnsson_simulation-based_2008,
	title = {Simulation-based approach to general game playing},
	booktitle = {23rd {AAAI}},
	author = {Finnsson, H and Björnsson, Y},
	year = {2008},
	keywords = {⛔ No DOI found},
	pages = {259--264},
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984875},
	doi = {10.1111/j.2517-6161.1977.tb01600.x},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	number = {1},
	urldate = {2020-07-22},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	pages = {1--38},
}

@article{morgan_stockhausens_1975,
	title = {Stockhausen's {Writings} on {Music}},
	volume = {61},
	issn = {0027-4631},
	url = {https://www.jstor.org/stable/741681},
	doi = {10.1093/mq/LXI.1.1},
	number = {1},
	urldate = {2020-05-11},
	journal = {The Musical Quarterly},
	author = {Morgan, Robert P.},
	year = {1975},
	pages = {1--16},
}

@article{bartlett_recognition_nodate,
	title = {Recognition of familiar and unfamiliar melodies in normal aging and {Alzheimer}'s disease},
	doi = {10.3758/BF03197255},
	author = {Bartlett, Jamesc and Halpern, Andrea R},
	pages = {16},
}

@article{chaslot_progressive_2008,
	title = {Progressive strategies for {Monte}-{Carlo} tree search},
	volume = {4},
	doi = {10.1142/S1793005708001094},
	number = {3},
	journal = {New Mathematics and Natural Computation},
	author = {Chaslot, Guillaume M J-B and Winands, Mark H M and Bouzy, Bruno},
	year = {2008},
	pages = {343--357},
}

@article{bruna_invariant_2012,
	title = {Invariant {Scattering} {Convolution} {Networks}},
	volume = {35},
	url = {http://arxiv.org/abs/1203.1513},
	doi = {10.1109/TPAMI.2012.230},
	abstract = {A wavelet scattering network computes a translation invariant image representation, which is stable to deformations and preserves high frequency information for classification. It cascades wavelet transform convolutions with non-linear modulus and averaging operators. The first network layer outputs SIFT-type descriptors whereas the next layers provide complementary invariant information which improves classification. The mathematical analysis of wavelet scattering networks explains important properties of deep convolution networks for classification. A scattering representation of stationary processes incorporates higher order moments and can thus discriminate textures having the same Fourier power spectrum. State of the art classification results are obtained for handwritten digits and texture discrimination, using a Gaussian kernel SVM and a generative PCA classifier.},
	number = {8},
	urldate = {2019-12-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bruna, Joan and Mallat, Stéphane},
	month = mar,
	year = {2012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, nonstationary, scattering, signal analysis, transform method, unread, wavelet},
	pages = {1872--1886},
}

@inproceedings{higgins_-vae_2017,
	title = {β-{VAE}: {LEARNING} {BASIC} {VISUAL} {CONCEPTS} {WITH} {A} {CONSTRAINED} {VARIATIONAL} {FRAMEWORK}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {\textgreater} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@article{smith_unmixer_2019,
	title = {Unmixer: {An} interface for extracting and remixing loops},
	abstract = {To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mashups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations.},
	author = {Smith, Jordan B L and Kawasaki, Yuta and Goto, Masataka},
	year = {2019},
	keywords = {source separation, tensor factorisation, unread, ⛔ No DOI found},
	pages = {8},
}

@article{drude_unsupervised_2019,
	title = {Unsupervised training of a deep clustering model for multichannel blind source separation},
	url = {https://arxiv.org/abs/1904.01340v1},
	abstract = {We propose a training scheme to train neural network-based source separation algorithms from scratch when parallel clean data is unavailable. In particular, we demonstrate that an unsupervised spatial clustering algorithm is sufficient to guide the training of a deep clustering system. We argue that previous work on deep clustering requires strong supervision and elaborate on why this is a limitation. We demonstrate that (a) the single-channel deep clustering system trained according to the proposed scheme alone is able to achieve a similar performance as the multi-channel teacher in terms of word error rates and (b) initializing the spatial clustering approach with the deep clustering result yields a relative word error rate reduction of 26 \% over the unsupervised teacher.},
	urldate = {2020-02-28},
	author = {Drude, Lukas and Hasenklever, Daniel and Haeb-Umbach, Reinhold},
	month = apr,
	year = {2019},
	keywords = {⛔ No DOI found},
}

@inproceedings{lucas_understanding_2019,
	title = {Understanding posterior collapse in generative latent variable models},
	abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identiﬁable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in non-linear VAEs. Our ﬁndings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the eﬀect of the KL term in the ELBO to alleviate posterior collapse.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {16},
}

@article{deutsch_absolute_nodate,
	title = {Absolute pitch among {American} and {Chinese} conservatory students: {Prevalence} differences, and evidence for a speech-related critical period ({L})a)},
	doi = {10.1121/1.2151799},
	author = {Deutsch, Diana and Henthorn, Trevor and Marvin, Elizabeth and Xu, HongShuai},
	pages = {4},
}

@article{che_your_nodate,
	title = {Your {GAN} is secretly an energy-based model and you should use discriminator driven latent sampling},
	abstract = {The sum of the implicit generator log-density log pg of a GAN with the logit score of the discriminator deﬁnes an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal. This makes it possible to improve on the typical generator (with implicit density pg). We show that samples can be generated from this modiﬁed density by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. We call this process of running Markov Chain Monte Carlo in the latent space, and then applying the generator function, Discriminator Driven Latent Sampling (DDLS). We show that DDLS is highly efﬁcient compared to previous methods which work in the high-dimensional pixel space, and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN [1] from 8.22 to 9.09 which is comparable to the class-conditional BigGAN [2] model. This achieves a new state-of-the-art in the unconditional image synthesis setting without introducing extra parameters or additional training.},
	author = {Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
	keywords = {⛔ No DOI found},
	pages = {13},
}

@inproceedings{stoller_wave-u-net_2018,
	title = {Wave-{U}-{Net}: {A} multi-{Scale} neural network for end-to-end audio source separation},
	abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids ﬁxed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difﬁcult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
	booktitle = {{ISMIR}},
	author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
	year = {2018},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{tolstikhin_wasserstein_2018,
	title = {Wasserstein auto-encoders},
	abstract = {We propose the Wasserstein Auto-Encoder (WAE)—a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) (Kingma \& Welling, 2014). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE) (Makhzani et al., 2016). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
	author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and Scholkopf, Bernhard},
	year = {2018},
	keywords = {⛔ No DOI found},
	pages = {16},
}

@inproceedings{nercessian_zero-shot_2020,
	title = {Zero-{Shot} {Singing} {Voice} {Conversion}},
	abstract = {In this paper, we propose the use of speaker embedding networks to perform zero-shot singing voice conversion, and suggest two architectures for its realization. The use of speaker embedding networks not only enables the capability to adapt to new voices on-the-ﬂy, but also allows for model training on unlabeled data. This not only facilitates the collection of suitable singing voice data, but also allows networks to be pretrained on large speech corpora before being reﬁned on singing voice datasets, improving network generalization. We illustrate the effectiveness of the proposed zero-shot singing voice conversion algorithms by both qualitative and quantitative means.},
	booktitle = {Proceedings of the 21th international society for music information retrieval conference},
	author = {Nercessian, Shahan},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{chowdhury_towards_2019,
	title = {Towards explainable music emotion recognition: {The} route via mid-level features},
	abstract = {Emotional aspects play an important part in our interaction with music. However, modelling these aspects in MIR systems have been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difﬁcult to quantify or predict in the ﬁrst place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its predictions, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features and observe that the loss in predictive performance of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the midlevel features is justiﬁed by the gain in explainability of the predictions.},
	author = {Chowdhury, Shreyan and Vall, Andreu and Haunschmid, Verena and Widmer, Gerhard},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@inproceedings{hayes_theres_2020,
	address = {Thessaloniki, Greece (Online)},
	title = {There’s more to timbre than musical instruments: semantic dimensions of {FM} sounds},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Timbre}},
	author = {Hayes, Ben and Saitis, Charalampos},
	year = {2020},
	note = {tex.copyright: All rights reserved},
	keywords = {⛔ No DOI found},
}

@article{lostanlen_time-frequency_2020,
	title = {Time-frequency scattering accurately models auditory similarities between instrumental playing techniques},
	url = {http://arxiv.org/abs/2007.10926},
	abstract = {Instrumental playing techniques such as vibratos, glissandos, and trills often denote musical expressivity, both in classical and folk contexts. However, most existing approaches to music similarity retrieval fail to describe timbre beyond the so-called “ordinary” technique, use instrument identity as a proxy for timbre quality, and do not allow for customization to the perceptual idiosyncrasies of a new subject. In this article, we ask 31 human subjects to organize 78 isolated notes into a set of timbre clusters. Analyzing their responses suggests that timbre perception operates within a more flexible taxonomy than those provided by instruments or playing techniques alone. In addition, we propose a machine listening model to recover the cluster graph of auditory similarities across instruments, mutes, and techniques. Our model relies on joint time–frequency scattering features to extract spectrotemporal modulations as acoustic features. Furthermore, it minimizes triplet loss in the cluster graph by means of the large-margin nearest neighbor (LMNN) metric learning algorithm. Over a dataset of 9346 isolated notes, we report a state-of-the-art average precision at rank five (AP@5) of \$99.0{\textbackslash}\%{\textbackslash}pm1\$. An ablation study demonstrates that removing either the joint time–frequency scattering transform or the metric learning algorithm noticeably degrades performance.},
	urldate = {2020-08-17},
	journal = {arXiv:2007.10926 [cs, eess]},
	author = {Lostanlen, Vincent and El-Hajj, Christian and Rossignol, Mathias and Lafay, Grégoire and Andén, Joakim and Lagrange, Mathieu},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{tzinis_unsupervised_2018,
	title = {Unsupervised deep clustering for source separation: {Direct} learning from mixtures using spatial information},
	shorttitle = {Unsupervised {Deep} {Clustering} for {Source} {Separation}},
	url = {http://arxiv.org/abs/1811.01531},
	abstract = {We present a monophonic source separation system that is trained by only observing mixtures with no ground truth separation information. We use a deep clustering approach which trains on multi-channel mixtures and learns to project spectrogram bins to source clusters that correlate with various spatial features. We show that using such a training process we can obtain separation performance that is as good as making use of ground truth separation information. Once trained, this system is capable of performing sound separation on monophonic inputs, despite having learned how to do so using multi-channel recordings.},
	urldate = {2020-02-28},
	journal = {arXiv:1811.01531 [cs, eess, stat]},
	author = {Tzinis, Efthymios and Venkataramani, Shrikant and Smaragdis, Paris},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{rezende_variational_2015,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2019-12-03},
	booktitle = {Proceedings of the 32nd international conference on machine learning},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	year = {2015},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology, to read this week, ⛔ No DOI found},
	pages = {1530--1538},
}

@inproceedings{esling_ultra-light_2020,
	address = {Montréal},
	title = {Ultra-light {Deep} {MIR} by {Trimming} {Lottery} {Tickets}},
	abstract = {Current state-of-the-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success.},
	booktitle = {Proceedings of the 21th international society for music information retrieval conference},
	author = {Esling, Philippe and Bazin, Theis and Bitton, Adrien and Carsault, Tristan and Devis, Ninon},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@article{moorer_synthesis_1976,
	title = {The synthesis of complex audio spectra by means of discrete summation formulas},
	volume = {24},
	number = {9},
	journal = {Journal of the Audio Engineering Society},
	author = {Moorer, James A.},
	month = nov,
	year = {1976},
	keywords = {⛔ No DOI found},
	pages = {717--727},
}

@article{von_bismarck_timbre_1974,
	title = {Timbre of steady sounds: {A} factorial investigation of its verbal attributes},
	volume = {30},
	journal = {Acustica},
	author = {von Bismarck, G},
	month = mar,
	year = {1974},
	keywords = {⛔ No DOI found},
	pages = {146--159},
}

@article{de_silva_sparse_nodate,
	title = {Sparse multidimensional scaling using landmark points},
	abstract = {In this paper, we discuss a computationally eﬃcient approximation to the classical multidimensional scaling (MDS) algorithm, called Landmark MDS (LMDS), for use when the number of data points is very large. The ﬁrst step of the algorithm is to run classical MDS to embed a chosen subset of the data, referred to as the ‘landmark points’, in a low-dimensional space. Each remaining data point can be located within this space given knowledge of its distances to the landmark points. We give an elementary and explicit theoretical analysis of this procedure, and demonstrate with examples that LMDS is eﬀective in practical use.},
	author = {de Silva, Vin and Tenenbaum, Joshua B},
	keywords = {embedding, mds, sparse, unsupervised, ⛔ No DOI found},
	pages = {41},
}

@article{patwari_semantically_2020,
	title = {Semantically meaningful attributes from co-listen embeddings for playlist exploration and expansion},
	abstract = {Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent tripletloss distances. Instead of directly using these co-listen–based embeddings, we explore making recommendations based on a second, smaller embedding space of humanintelligible musical attributes. To do this, we use the colisten–based audio embeddings as inputs to small attribute classiﬁers, trained on a small hand-labeled dataset. These classiﬁers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen–audio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attributeembedding space to human curators for automatically extending thematic playlists.},
	author = {Patwari, Ayush and Kong, Nicholas and Wang, Jun and Gargi, Ullas and Covell, Michele and Jansen, Aren},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{birnbaum_temporal_2020,
	title = {Temporal {FiLM}: {Capturing} long-{Range} sequence dependencies with feature-{Wise} modulations},
	shorttitle = {Temporal {FiLM}},
	url = {http://arxiv.org/abs/1909.06628},
	abstract = {Learning representations that accurately capture long-range dependencies in sequential inputs — including text, audio, and genomic data — is a key problem in deep learning. Feed-forward convolutional models capture only feature interactions within finite receptive fields while recurrent architectures can be slow and difficult to train due to vanishing gradients. Here, we propose Temporal Feature-Wise Linear Modulation (TFiLM) — a novel architectural component inspired by adaptive batch normalization and its extensions — that uses a recurrent neural network to alter the activations of a convolutional model. This approach expands the receptive field of convolutional sequence models with minimal computational overhead. Empirically, we find that TFiLM significantly improves the learning speed and accuracy of feed-forward neural networks on a range of generative and discriminative learning tasks, including text classification and audio super-resolution},
	urldate = {2021-02-24},
	journal = {arXiv:1909.06628 [cs, stat]},
	author = {Birnbaum, Sawyer and Kuleshov, Volodymyr and Enam, Zayd and Koh, Pang Wei and Ermon, Stefano},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{liutkus_2016_2017,
	address = {Cham},
	title = {The 2016 signal separation evaluation campaign},
	booktitle = {Latent variable analysis and signal separation - 12th international conference, {LVA}/{ICA} 2015, liberec, czech republic, august 25-28, 2015, proceedings},
	publisher = {Springer International Publishing},
	author = {Liutkus, Antoine and Stöter, Fabian-Robert and Rafii, Zafar and Kitamura, Daichi and Rivet, Bertrand and Ito, Nobutaka and Ono, Nobutaka and Fontecave, Julie},
	editor = {Tichavský, Petr and Babaie-Zadeh, Massoud and Michel, Olivier J.J. and Thirion-Moreau, Nadège},
	year = {2017},
	keywords = {⛔ No DOI found},
	pages = {323--332},
}

@inproceedings{jansson_singing_2017,
	title = {Singing voice separation with deep {U}-{Net} convolutional networks},
	abstract = {The decomposition of a music audio signal into its vocal and backing track components is analogous to image-toimage translation, where a mixed spectrogram is transformed into its constituent sources. We propose a novel application of the U-Net architecture — initially developed for medical imaging — for the task of source separation, given its proven capacity for recreating the ﬁne, low-level detail required for high-quality audio reproduction. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed algorithm achieves state-of-the-art performance.},
	booktitle = {{ISMIR}},
	author = {Jansson, Andreas and Humphrey, Eric and Montecchio, Nicola and Bittner, Rachel and Kumar, Aparna and Weyde, Tillman},
	year = {2017},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{crsitovao_sparsity_2020,
	title = {Sparsity enforcement on latent variables for better disentanglement in {VAE}},
	author = {Crsitovao, Paulino and Nakada, Hidemoto and Tanimura, Yusuke and Asoh, Hideki},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {4},
}

@article{smith_sparse_nodate,
	title = {Sparse tensor factorization: {Algorithms}, data structures, and challenges},
	author = {Smith, Shaden and Karypis, George},
	keywords = {tensor factorisation, unread, ⛔ No DOI found},
	pages = {80},
}

@article{zhang_unreasonable_2018,
	title = {The unreasonable effectiveness of deep features as a perceptual metric},
	url = {http://arxiv.org/abs/1801.03924},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	urldate = {2020-10-09},
	journal = {arXiv:1801.03924 [cs]},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No DOI found},
}

@article{germain_speech_2018,
	title = {Speech {Denoising} with {Deep} {Feature} {Losses}},
	url = {http://arxiv.org/abs/1806.10522},
	abstract = {We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.},
	urldate = {2020-10-16},
	journal = {arXiv:1806.10522 [cs, eess]},
	author = {Germain, Francois G. and Chen, Qifeng and Koltun, Vladlen},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{jang_universal_2021,
	title = {Universal {MelGAN}: {A} robust neural vocoder for high-{Fidelity} waveform generation in multiple domains},
	shorttitle = {Universal {MelGAN}},
	url = {http://arxiv.org/abs/2011.09631},
	abstract = {We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech in multiple domains. To preserve sound quality when the MelGAN-based structure is trained with a dataset of hundreds of speakers, we added multi-resolution spectrogram discriminators to sharpen the spectral resolution of the generated waveforms. This enables the model to generate realistic waveforms of multi-speakers, by alleviating the over-smoothing problem in the high frequency band of the large footprint model. Our structure generates signals close to ground-truth data without reducing the inference speed, by discriminating the waveform and spectrogram during training. The model achieved the best mean opinion score (MOS) in most scenarios using ground-truth mel-spectrogram as an input. Especially, it showed superior performance in unseen domains with regard of speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech scenario using mel-spectrogram generated by a transformer model, it synthesized high-fidelity speech of 4.22 MOS. These results, achieved without external domain information, highlight the potential of the proposed model as a universal vocoder.},
	urldate = {2021-03-24},
	journal = {arXiv:2011.09631 [cs, eess]},
	author = {Jang, Won and Lim, Dan and Yoon, Jaesam},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{chang_principled_2020,
	title = {Principled {Weight} {Initialisation} for {Hypernetworks}},
	abstract = {Hypernetworks are meta neural networks that generate weights for a main neural network in an end-to-end differentiable manner. Despite extensive applications ranging from multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been studied to date. We observe that classical weight initialization methods like Glorot \& Bengio (2010) and He et al. (2015), when applied directly on a hypernet, fail to produce weights for the mainnet in the correct scale. We develop principled techniques for weight initialization in hypernets, and show that they lead to more stable mainnet weights, lower training loss, and faster convergence.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chang, Oscar and Flokas, Lampros and Lipson, Hod},
	year = {2020},
	keywords = {⛔ No DOI found},
}

@article{song_score-based_2020,
	title = {Score-based generative modeling through stochastic differential equations},
	url = {http://arxiv.org/abs/2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and score-based generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of \$1024 {\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
	urldate = {2021-01-17},
	journal = {arXiv:2011.13456 [cs, stat]},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{caracalla_sound_2019,
	title = {Sound texture synthesis using convolutional neural networks},
	url = {http://arxiv.org/abs/1905.03637},
	abstract = {The following article introduces a new parametric synthesis algorithm for sound textures inspired by existing methods used for visual textures. Using a 2D Convolutional Neural Network (CNN), a sound signal is modified until the temporal cross-correlations of the feature maps of its log-spectrogram resemble those of a target texture. We show that the resulting synthesized sound signal is both different from the original and of high quality, while being able to reproduce singular events appearing in the original. This process is performed in the time domain, discarding the harmful phase recovery step which usually concludes synthesis performed in the time-frequency domain. It is also straightforward and flexible, as it does not require any fine tuning between several losses when synthesizing diverse sound textures. A way of extending the synthesis in order to produce a sound of any length is also presented, after which synthesized spectrograms and sound signals are showcased. We also discuss on the choice of CNN, on border effects in our synthesized signals and on possible ways of modifying the algorithm in order to improve its current long computation time.},
	urldate = {2021-01-30},
	journal = {arXiv:1905.03637 [cs, eess]},
	author = {Caracalla, Hugo and Roebel, Axel},
	month = may,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{karras_progressive_2018,
	title = {Progressive growing of {GANs} for improved quality, stability, and variation},
	url = {https://openreview.net/forum?id=Hk99zCeAb},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
	keywords = {⛔ No DOI found},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2021-04-03},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{ghisi_real-time_nodate,
	title = {Real-time corpus-{Based} concatenative synthesis for symbolic notation},
	abstract = {We introduce a collection of modules designed to segment, analyze, display and sequence symbolic scores in real-time. This mechanism, inspired from CataRT’s corpus-based concatenative synthesis, is implemented as a part of the dada library for Max, currently under development.},
	author = {Ghisi, Daniele and Agon, Carlos},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{chun_probabilistic_2021,
	title = {Probabilistic {Embeddings} for {Cross}-{Modal} {Retrieval}},
	url = {http://arxiv.org/abs/2101.05068},
	abstract = {Cross-modal retrieval methods build a common representation space for samples from multiple modalities, typically from the vision and the language domains. For images and their captions, the multiplicity of the correspondences makes the task particularly challenging. Given an image (respectively a caption), there are multiple captions (respectively images) that equally make sense. In this paper, we argue that deterministic functions are not sufficiently powerful to capture such one-to-many correspondences. Instead, we propose to use Probabilistic Cross-Modal Embedding (PCME), where samples from the different modalities are represented as probabilistic distributions in the common embedding space. Since common benchmarks such as COCO suffer from non-exhaustive annotations for cross-modal matches, we propose to additionally evaluate retrieval on the CUB dataset, a smaller yet clean database where all possible image-caption pairs are annotated. We extensively ablate PCME and demonstrate that it not only improves the retrieval performance over its deterministic counterpart, but also provides uncertainty estimates that render the embeddings more interpretable.},
	urldate = {2021-01-23},
	journal = {arXiv:2101.05068 [cs]},
	author = {Chun, Sanghyuk and Oh, Seong Joon and de Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
}

@article{wyse_real-valued_2018,
	title = {Real-valued parametric conditioning of an {RNN} for interactive sound synthesis},
	url = {http://arxiv.org/abs/1805.10808},
	abstract = {A Recurrent Neural Network (RNN) for audio synthesis is trained by augmenting the audio input with information about signal characteristics such as pitch, amplitude, and instrument. The result after training is an audio synthesizer that is played like a musical instrument with the desired musical characteristics provided as continuous parametric control. The focus of this paper is on conditioning data-driven synthesis models with real-valued parameters, and in particular, on the ability of the system a) to generalize and b) to be responsive to parameter values and sequences not seen during training.},
	urldate = {2021-03-16},
	journal = {arXiv:1805.10808 [cs, eess]},
	author = {Wyse, Lonce},
	month = may,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{goto_rwc_2002,
	address = {Paris, France},
	title = {{RWC} music database: {Popular}, classical, and jazz music databases},
	booktitle = {Proceedings of the 3rd international society for music information retrieval conference},
	author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
	year = {2002},
	keywords = {⛔ No DOI found},
	pages = {287--288},
}

@inproceedings{hayes_perceptual_2020,
	address = {London, UK},
	title = {Perceptual {Similarities} in {Neural} {Timbre} {Embeddings}},
	booktitle = {{DMRN}+15: {Digital} music research network one-day workshop 2020},
	author = {Hayes, Ben and Brosnahan, Luke and Saitis, Charalampos and Fazekas, George},
	year = {2020},
	note = {tex.copyright: All rights reserved},
	keywords = {⛔ No DOI found},
}

@article{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	urldate = {2021-03-10},
	journal = {arXiv:1711.10433 [cs]},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{welburn_object-coding_2007,
	title = {Object-coding for resolution-free musical audio},
	abstract = {Object-based coding of audio represents the signal as a parameter stream for a set of sound-producing objects. Encoding in this manner can provide a resolution-free representation of an audio signal. Given a robust estimation of the object-parameters and a multi-resolution synthesis engine, the signal can be “intelligently” upsampled, extending the bandwidth and getting best use out of a high-resolution signal-chain. We present some initial ﬁndings on extending bandwidth using harmonic models.},
	author = {Welburn, Stephen and Plumbley, Mark and Vincent, Emmanuel},
	year = {2007},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@inproceedings{gfeller_now_2017,
	title = {Now {Playing}: {Continuous} low-power music recognition},
	shorttitle = {Now {Playing}},
	booktitle = {{NIPS} 2017 {Workshop}: {Machine} {Learning} on the {Phone}},
	author = {Gfeller, Beat and Aguera-Arcas, Blaise and Roblek, Dominik and Lyon, James David and Odell, Julian James and Kilgour, Kevin and Ritter, Marvin and Sharifi, Matt and Velimirović, Mihajlo and Guo, Ruiqi and Kumar, Sanjiv},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@article{kolbaek_loss_2019,
	title = {On loss functions for supervised monaural time-{Domain} speech enhancement},
	url = {http://arxiv.org/abs/1909.01019},
	abstract = {Many deep learning-based speech enhancement algorithms are designed to minimize the mean-square error (MSE) in some transform domain between a predicted and a target speech signal. However, optimizing for MSE does not necessarily guarantee high speech quality or intelligibility, which is the ultimate goal of many speech enhancement algorithms. Additionally, only little is known about the impact of the loss function on the emerging class of time-domain deep learning-based speech enhancement systems. We study how popular loss functions influence the performance of deep learning-based speech enhancement systems. First, we demonstrate that perceptually inspired loss functions might be advantageous if the receiver is the human auditory system. Furthermore, we show that the learning rate is a crucial design parameter even for adaptive gradient-based optimizers, which has been generally overlooked in the literature. Also, we found that waveform matching performance metrics must be used with caution as they in certain situations can fail completely. Finally, we show that a loss function based on scale-invariant signal-to-distortion ratio (SI-SDR) achieves good general performance across a range of popular speech enhancement evaluation metrics, which suggests that SI-SDR is a good candidate as a general-purpose loss function for speech enhancement systems.},
	urldate = {2019-10-21},
	journal = {arXiv:1909.01019 [cs, eess]},
	author = {Kolbæk, Morten and Tan, Zheng-Hua and Jensen, Søren Holdt and Jensen, Jesper},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, unread, ⛔ No DOI found},
}

@article{yildiz_ode2vae_2019,
	title = {{ODE}\$ˆ2\${VAE}: {Deep} generative second order {ODEs} with bayesian neural networks},
	shorttitle = {{ODE}\$ˆ2\${VAE}},
	url = {http://arxiv.org/abs/1905.10994},
	abstract = {We present Ordinary Differential Equation Variational Auto-Encoder (ODE\$ˆ2\$VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE\$ˆ2\$VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.},
	urldate = {2020-11-09},
	journal = {arXiv:1905.10994 [cs, stat]},
	author = {Yıldız, Çağatay and Heinonen, Markus and Lähdesmäki, Harri},
	month = oct,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{schwarz_real-time_2006,
	title = {Real-{Time} {Corpus}-{Based} {Concatenative} {Synthesis} with {CataRT}},
	abstract = {The concatenative real-time sound synthesis system CataRT plays grains from a large corpus of segmented and descriptor-analysed sounds according to proximity to a target position in the descriptor space. This can be seen as a content-based extension to granular synthesis providing direct access to specific sound characteristics. CataRT is implemented as a collection of Max/MSP patches using the FTM library and an SQL database. Segmentation and MPEG-7 descriptors are loaded from SDIF files or generated on-the-fly. The},
	booktitle = {In {Proc}. of the {Int}. {Conf}. on {Digital} {Audio} {Effects} (dafx-06},
	author = {Schwarz, Diemo and Beller, Grégory and Verbrugghe, Bruno and Britton, Sam},
	year = {2006},
	keywords = {⛔ No DOI found},
	pages = {279--282},
}

@article{kumari_perceptnet_2020,
	title = {{PerceptNet}: {Learning} perceptual similarity of haptic textures in presence of unorderable triplets},
	shorttitle = {{PerceptNet}},
	url = {http://arxiv.org/abs/1905.03302},
	abstract = {In order to design haptic icons or build a haptic vocabulary, we require a set of easily distinguishable haptic signals to avoid perceptual ambiguity, which in turn requires a way to accurately estimate the perceptual (dis)similarity of such signals. In this work, we present a novel method to learn such a perceptual metric based on data from human studies. Our method is based on a deep neural network that projects signals to an embedding space where the natural Euclidean distance accurately models the degree of dissimilarity between two signals. The network is trained only on non-numerical comparisons of triplets of signals, using a novel triplet loss that considers both types of triplets that are easy to order (inequality constraints), as well as those that are unorderable/ambiguous (equality constraints). Unlike prior MDS-based non-parametric approaches, our method can be trained on a partial set of comparisons and can embed new haptic signals without retraining the model from scratch. Extensive experimental evaluations show that our method is significantly more effective at modeling perceptual dissimilarity than alternatives.},
	urldate = {2020-10-13},
	journal = {arXiv:1905.03302 [cs, stat]},
	author = {Kumari, Priyadarshini and Chaudhuri, Siddhartha and Chaudhuri, Subhasis},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{ma_probabilistic_2021,
	title = {Probabilistic metric learning with adaptive margin for top-{K} recommendation},
	url = {http://arxiv.org/abs/2101.04849},
	abstract = {Personalized recommender systems are playing an increasingly important role as more content and services become available and users struggle to identify what might interest them. Although matrix factorization and deep learning based methods have proved effective in user preference modeling, they violate the triangle inequality and fail to capture fine-grained preference information. To tackle this, we develop a distance-based recommendation model with several novel aspects: (i) each user and item are parameterized by Gaussian distributions to capture the learning uncertainties; (ii) an adaptive margin generation scheme is proposed to generate the margins regarding different training triplets; (iii) explicit user-user/item-item similarity modeling is incorporated in the objective function. The Wasserstein distance is employed to determine preferences because it obeys the triangle inequality and can measure the distance between probabilistic distributions. Via a comparison using five real-world datasets with state-of-the-art methods, the proposed model outperforms the best existing models by 4-22\% in terms of recall@K on Top-K recommendation.},
	urldate = {2021-01-17},
	journal = {arXiv:2101.04849 [cs]},
	author = {Ma, Chen and Ma, Liheng and Zhang, Yingxue and Tang, Ruiming and Liu, Xue and Coates, Mark},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{rauscher_music_nodate,
	title = {Music and {Spatial} {Task} {Performance}: {A} {Causal} {Relationship}},
	author = {Rauscher, Frances H and Shaw, Gordon L and Levine, Linda J},
	keywords = {⛔ No DOI found},
	pages = {26},
}

@article{ramirez_modeling_2019,
	title = {Modeling of nonlinear audio effects with end-to-end deep neural networks},
	url = {http://arxiv.org/abs/1810.06603},
	abstract = {In the context of music production, distortion effects are mainly used for aesthetic reasons and are usually applied to electric musical instruments. Most existing methods for nonlinear modeling are often either simplified or optimized to a very specific circuit. In this work, we investigate deep learning architectures for audio processing and we aim to find a general purpose end-to-end deep neural network to perform modeling of nonlinear audio effects. We show the network modeling various nonlinearities and we discuss the generalization capabilities among different instruments.},
	urldate = {2020-06-01},
	journal = {arXiv:1810.06603 [cs, eess]},
	author = {Ramirez, Marco A. Martínez and Reiss, Joshua D.},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, unread, ⛔ No DOI found},
}

@article{ricard_morphological_2004,
	title = {Morphological sound description: {Computational} model and usability evaluation},
	abstract = {Sound samples metadata are usually limited to a source label and several related textual labels. In the context of sound retrieval this makes the retrieval of sounds having no identifiable source (‘abstract sounds’) a hard task. We propose a description framework focusing on intrinsic perceptual sound qualities, based on Schaeffer’s research on sound objects, which could be used to represent and retrieve abstract sounds and to refine traditional search by source for non-abstract sounds. We show that some perceptual labels can be automatically extracted with good performance, avoiding the time-consuming manual labelling task, and that the resulting representation is evaluated as useful and usable by a pool of users.},
	author = {Ricard, Julien and Herrera, Perfecto},
	year = {2004},
	keywords = {⛔ No DOI found},
	pages = {9},
}

@article{hou_neural_2016,
	title = {Neural networks with smooth adaptive activation functions for regression},
	url = {http://arxiv.org/abs/1608.06557},
	abstract = {In Neural Networks (NN), Adaptive Activation Functions (AAF) have parameters that control the shapes of activation functions. These parameters are trained along with other parameters in the NN. AAFs have improved performance of Neural Networks (NN) in multiple classification tasks. In this paper, we propose and apply AAFs on feedforward NNs for regression tasks. We argue that applying AAFs in the regression (second-to-last) layer of a NN can significantly decrease the bias of the regression NN. However, using existing AAFs may lead to overfitting. To address this problem, we propose a Smooth Adaptive Activation Function (SAAF) with piecewise polynomial form which can approximate any continuous function to arbitrary degree of error. NNs with SAAFs can avoid overfitting by simply regularizing the parameters. In particular, an NN with SAAFs is Lipschitz continuous given a bounded magnitude of the NN parameters. We prove an upper-bound for model complexity in terms of fat-shattering dimension for any Lipschitz continuous regression model. Thus, regularizing the parameters in NNs with SAAFs avoids overfitting. We empirically evaluated NNs with SAAFs and achieved state-of-the-art results on multiple regression datasets.},
	urldate = {2021-03-12},
	journal = {arXiv:1608.06557 [cs]},
	author = {Hou, Le and Samaras, Dimitris and Kurc, Tahsin M. and Gao, Yi and Saltz, Joel H.},
	month = aug,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
}

@article{mcfee_metric_nodate,
	title = {Metric {Learning} to {Rank}},
	abstract = {We study metric learning as a problem of information retrieval. We present a general metric learning algorithm, based on the structural SVM framework, to learn a metric such that rankings of data induced by distance from a query can be optimized against various ranking measures, such as AUC, Precision-at-k, MRR, MAP or NDCG. We demonstrate experimental results on standard classiﬁcation data sets, and a large-scale online dating recommendation problem.},
	author = {McFee, Brian and Lanckriet, Gert},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@inproceedings{andreux_music_2018,
	address = {Paris, France},
	title = {Music generation and transformation with moment matching-{Scattering} inverse networks},
	abstract = {We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform deﬁned from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MMSIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefﬁcients.},
	booktitle = {Proceedings of the 19th international society for music information retrieval conference},
	author = {Andreux, Mathieu and Mallat, Stephane},
	year = {2018},
	keywords = {unread, ⛔ No DOI found},
	pages = {7},
}

@article{radford_learning_2021,
	title = {Learning transferable visual models from natural language supervision},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2021-04-05},
	journal = {arXiv:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{carlson_new_2020,
	title = {New metrics between rational spectra and their connection to optimal transport},
	url = {http://arxiv.org/abs/2004.09152},
	abstract = {We propose a series of metrics between pairs of signals, linear systems or rational spectra, based on optimal transport and linear-systems theory. The metrics operate on the locations of the poles of rational functions and admit very efficient computation of distances, barycenters, displacement interpolation and projections. We establish the connection to the Wasserstein distance between rational spectra, and demonstrate the use of the metrics in tasks such as signal classification, clustering, detection and approximation.},
	urldate = {2021-04-08},
	journal = {arXiv:2004.09152 [cs, eess, math, stat]},
	author = {Carlson, Fredrik Bagge and Chitre, Mandar},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Dynamical Systems, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{lee_metric_2020,
	title = {Metric learning vs classification for disentangled music representation learning},
	abstract = {Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classiﬁcation, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classiﬁcation, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classiﬁcation, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We ﬁnd that classiﬁcation-based models are generally advantageous for training time, similarity retrieval, and autotagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.},
	author = {Lee, Jongpil and Bryan, Nicholas J and Salamon, Justin and Jin, Zeyu and Nam, Juhan},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{oh_modeling_2019,
	title = {Modeling {Uncertainty} with {Hedged} {Instance} {Embedding}},
	url = {http://arxiv.org/abs/1810.00319},
	abstract = {Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty arising when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by hedging the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle. Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of hedging its bets across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure that is correlated with downstream performance.},
	urldate = {2021-01-23},
	journal = {arXiv:1810.00319 [cs, stat]},
	author = {Oh, Seong Joon and Murphy, Kevin and Pan, Jiyan and Roth, Joseph and Schroff, Florian and Gallagher, Andrew},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{krause_multiplicative_2017,
	title = {Multiplicative {LSTM} for sequence modelling},
	url = {http://arxiv.org/abs/1609.07959},
	abstract = {We introduce multiplicative LSTM (mLSTM), a recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level language modelling tasks. In this version of the paper, we regularise mLSTM to achieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also apply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a character level entropy of 1.26 bits/char, corresponding to a word level perplexity of 88.8, which is comparable to word level LSTMs regularised in similar ways on the same task.},
	urldate = {2021-02-11},
	journal = {arXiv:1609.07959 [cs, stat]},
	author = {Krause, Ben and Lu, Liang and Murray, Iain and Renals, Steve},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{brock_large_2019,
	title = {Large scale {GAN} training for high fidelity natural image synthesis},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2021-04-05},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{andreux_kymatio_2019,
	title = {Kymatio: {Scattering} {Transforms} in {Python}},
	shorttitle = {Kymatio},
	url = {http://arxiv.org/abs/1812.11214},
	abstract = {The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at https://www.kymat.io/},
	urldate = {2019-12-22},
	journal = {arXiv:1812.11214 [cs, eess, stat]},
	author = {Andreux, Mathieu and Angles, Tomás and Exarchakis, Georgios and Leonarduzzi, Roberto and Rochette, Gaspar and Thiry, Louis and Zarka, John and Mallat, Stéphane and Andén, Joakim and Belilovsky, Eugene and Bruna, Joan and Lostanlen, Vincent and Hirn, Matthew J. and Oyallon, Edouard and Zhang, Sixin and Cella, Carmine and Eickenberg, Michael},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, unread, ⛔ No DOI found},
}

@article{tian_latent_2019,
	title = {Latent translation: {Crossing} modalities by bridging generative models},
	shorttitle = {Latent {Translation}},
	url = {http://arxiv.org/abs/1902.08261},
	abstract = {End-to-end optimization has achieved state-of-the-art performance on many specific problems, but there is no straight-forward way to combine pretrained models for new problems. Here, we explore improving modularity by learning a post-hoc interface between two existing models to solve a new task. Specifically, we take inspiration from neural machine translation, and cast the challenging problem of cross-modal domain transfer as unsupervised translation between the latent spaces of pretrained deep generative models. By abstracting away the data representation, we demonstrate that it is possible to transfer across different modalities (e.g., image-to-audio) and even different types of generative models (e.g., VAE-to-GAN). We compare to state-of-the-art techniques and find that a straight-forward variational autoencoder is able to best bridge the two generative models through learning a shared latent space. We can further impose supervised alignment of attributes in both domains with a classifier in the shared latent space. Through qualitative and quantitative evaluations, we demonstrate that locality and semantic alignment are preserved through the transfer process, as indicated by high transfer accuracies and smooth interpolations within a class. Finally, we show this modular structure speeds up training of new interface models by several orders of magnitude by decoupling it from expensive retraining of base generative models.},
	urldate = {2019-10-26},
	journal = {arXiv:1902.08261 [cs, stat]},
	author = {Tian, Yingtao and Engel, Jesse},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, unread, ⛔ No DOI found},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2021-05-03},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{oord_neural_2017,
	address = {Red Hook, NY, USA},
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" – where the latents are ignored when they are paired with a powerful autoregressive decoder – typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the 31st international conference on neural information processing systems},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	year = {2017},
	keywords = {Computer Science - Machine Learning, ⛔ No DOI found},
	pages = {6309--6318},
}

@article{lattner_learning_2019,
	title = {Learning complex basis functions for invariant representations of audio},
	url = {http://arxiv.org/abs/1907.05982},
	abstract = {Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant "magnitude space" and a transformation-variant "phase space". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method, is available online.},
	urldate = {2019-11-10},
	journal = {arXiv:1907.05982 [cs, eess]},
	author = {Lattner, Stefan and Dörfler, Monika and Arzt, Andreas},
	month = jul,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, unread, ⛔ No DOI found},
}

@article{dhariwal_jukebox_2020,
	title = {Jukebox: {A} {Generative} {Model} for {Music}},
	shorttitle = {Jukebox},
	url = {http://arxiv.org/abs/2005.00341},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	urldate = {2020-10-29},
	journal = {arXiv:2005.00341 [cs, eess, stat]},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{zeghidour_leaf_2021,
	title = {{LEAF}: {A} {Learnable} {Frontend} for {Audio} {Classification}},
	shorttitle = {{LEAF}},
	url = {http://arxiv.org/abs/2101.08596},
	abstract = {Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.},
	urldate = {2021-03-11},
	journal = {arXiv:2101.08596 [cs, eess]},
	author = {Zeghidour, Neil and Teboul, Olivier and Quitry, Félix de Chaumont and Tagliasacchi, Marco},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{chen_learning_2020,
	title = {Learning audio embeddings with user listening data for content-based music recommendation},
	url = {http://arxiv.org/abs/2010.15389},
	abstract = {Personalized recommendation on new track releases has always been a challenging problem in the music industry. To combat this problem, we first explore user listening history and demographics to construct a user embedding representing the user's music preference. With the user embedding and audio data from user's liked and disliked tracks, an audio embedding can be obtained for each track using metric learning with Siamese networks. For a new track, we can decide the best group of users to recommend by computing the similarity between the track's audio embedding and different user embeddings, respectively. The proposed system yields state-of-the-art performance on content-based music recommendation tested with millions of users and tracks. Also, we extract audio embeddings as features for music genre classification tasks. The results show the generalization ability of our audio embeddings.},
	urldate = {2020-10-30},
	journal = {arXiv:2010.15389 [cs, eess]},
	author = {Chen, Ke and Liang, Beici and Ma, Xiaoshuan and Gu, Minwei},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{fujii_humangan_2019,
	title = {{HumanGAN}: generative adversarial network with human-based discriminator and its evaluation in speech perception modeling},
	shorttitle = {{HumanGAN}},
	url = {http://arxiv.org/abs/1909.11391},
	abstract = {We propose the HumanGAN, a generative adversarial network (GAN) incorporating human perception as a discriminator. A basic GAN trains a generator to represent a real-data distribution by fooling the discriminator that distinguishes real and generated data. Therefore, the basic GAN cannot represent the outside of a real-data distribution. In the case of speech perception, humans can recognize not only human voices but also processed (i.e., a non-existent human) voices as human voice. Such a human-acceptable distribution is typically wider than a real-data one and cannot be modeled by the basic GAN. To model the human-acceptable distribution, we formulate a backpropagation-based generator training algorithm by regarding human perception as a black-boxed discriminator. The training efficiently iterates generator training by using a computer and discrimination by crowdsourcing. We evaluate our HumanGAN in speech naturalness modeling and demonstrate that it can represent a human-acceptable distribution that is wider than a real-data distribution.},
	urldate = {2020-05-21},
	journal = {arXiv:1909.11391 [cs, eess]},
	author = {Fujii, Kazuki and Saito, Yuki and Takamichi, Shinnosuke and Baba, Yukino and Saruwatari, Hiroshi},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, to read this week, ⛔ No DOI found},
}

@article{song_how_2021,
	title = {How to {Train} {Your} {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/2101.03288},
	abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
	urldate = {2021-01-14},
	journal = {arXiv:2101.03288 [cs, stat]},
	author = {Song, Yang and Kingma, Diederik P.},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{du_implicit_2019,
	title = {Implicit {Generation} and {Generalization} in {Energy}-{Based} {Models}},
	volume = {32},
	url = {http://arxiv.org/abs/1903.08689},
	abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
	urldate = {2021-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Du, Yilun and Mordatch, Igor},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, \_tablet, ⛔ No DOI found},
}

@inproceedings{masri_improved_1996,
	title = {Improved modeling of attack transients in music analysis-resynthesis},
	abstract = {Current music analysis-resynthesis models represent sounds through a set of features, which are extracted from a time-frequency representation. So that each time-frame can present a good approximation to the instantaneous spectrum, it is necessary to analyse the waveform in short segments. This is achieved with a window function whose position is advanced by a fixed amount between frames. When the window encompasses a transient event, such as the percussive onset of a note, it contains information both before and after the event. These partiallycorrelated spectra often become confused during analysis and cause audible ‘diffusion ’ upon resynthesis. This paper presents a simple, novel technique to avoid the problem, by synchronising the analysis window to transient events. Event locations are identified by observing short-term changes in the spectrum. Thereafter the position of the analysis window is constrained, to prevent it capturing the signal both sides of an event simultaneously. This method, which has been automated, yields an improvement that is clearly audible, particularly for percussive sounds which retain their ‘crispness’. 1.},
	booktitle = {Proceedings of the {International} {Computer} {Music} {Conference}},
	author = {Masri, Paul and Bateman, Andrew},
	year = {1996},
	keywords = {⛔ No DOI found},
	pages = {100--103},
}

@inproceedings{engel_gansynth_2019,
	address = {New Orleans, LA, USA},
	title = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {17},
}

@article{chu_gaussian_nodate,
	title = {Gaussian {Processes} for {Ordinal} {Regression}},
	abstract = {We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach.},
	author = {Chu, Wei and Ghahramani, Zoubin},
	keywords = {⛔ No DOI found},
	pages = {23},
}

@article{song_improved_2020,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2006.09011},
	abstract = {Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.},
	urldate = {2021-01-17},
	journal = {arXiv:2006.09011 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{song_improved_2021,
	title = {Improved parallel {WaveGAN} vocoder with perceptually weighted spectrogram loss},
	url = {http://arxiv.org/abs/2101.07412},
	abstract = {This paper proposes a spectral-domain perceptual weighting technique for Parallel WaveGAN-based text-to-speech (TTS) systems. The recently proposed Parallel WaveGAN vocoder successfully generates waveform sequences using a fast non-autoregressive WaveNet model. By employing multi-resolution short-time Fourier transform (MR-STFT) criteria with a generative adversarial network, the light-weight convolutional networks can be effectively trained without any distillation process. To further improve the vocoding performance, we propose the application of frequency-dependent weighting to the MR-STFT loss function. The proposed method penalizes perceptually-sensitive errors in the frequency domain; thus, the model is optimized toward reducing auditory noise in the synthesized speech. Subjective listening test results demonstrate that our proposed method achieves 4.21 and 4.26 TTS mean opinion scores for female and male Korean speakers, respectively.},
	urldate = {2021-03-24},
	journal = {arXiv:2101.07412 [cs, eess]},
	author = {Song, Eunwoo and Yamamoto, Ryuichi and Hwang, Min-Jae and Kim, Jin-Seob and Kwon, Ohsung and Kim, Jae-Min},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{grnarova_generative_2021,
	title = {Generative minimization networks: {Training} {GANs} without competition},
	shorttitle = {Generative {Minimization} {Networks}},
	url = {http://arxiv.org/abs/2103.12685},
	abstract = {Many applications in machine learning can be framed as minimization problems and solved efficiently using gradient-based techniques. However, recent applications of generative models, particularly GANs, have triggered interest in solving min-max games for which standard optimization techniques are often not suitable. Among known problems experienced by practitioners is the lack of convergence guarantees or convergence to a non-optimum cycle. At the heart of these problems is the min-max structure of the GAN objective which creates non-trivial dependencies between the players. We propose to address this problem by optimizing a different objective that circumvents the min-max structure using the notion of duality gap from game theory. We provide novel convergence guarantees on this objective and demonstrate why the obtained limit point solves the problem better than known techniques.},
	urldate = {2021-03-24},
	journal = {arXiv:2103.12685 [cs]},
	author = {Grnarova, Paulina and Kilcher, Yannic and Levy, Kfir Y. and Lucchi, Aurelien and Hofmann, Thomas},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ⛔ No DOI found},
}

@inproceedings{goodfellow_generative_2014,
	address = {Montreal, Canada},
	series = {{NIPS}'14},
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	keywords = {⛔ No DOI found},
	pages = {2672--2680},
}

@article{deutsch_generating_2018,
	title = {Generating {Neural} {Networks} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1801.01952},
	abstract = {Hypernetworks are neural networks that generate weights for another neural network. We formulate the hypernetwork training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We explain how this simple formulation generalizes variational inference. We use multi-layered perceptrons to form the mapping from the low dimensional input random vector to the high dimensional weight space, and demonstrate how to reduce the number of parameters in this mapping by parameter sharing. We perform experiments and show that the generated weights are diverse and lie on a non-trivial manifold.},
	urldate = {2021-02-24},
	journal = {arXiv:1801.01952 [cs, stat]},
	author = {Deutsch, Lior},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{soltani_higher_2016,
	title = {Higher {Order} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.00064},
	abstract = {In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.},
	urldate = {2020-06-01},
	journal = {arXiv:1605.00064 [cs]},
	author = {Soltani, Rohollah and Jiang, Hui},
	month = apr,
	year = {2016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, unread, ⛔ No DOI found},
}

@article{simpson_finding_2018,
	title = {Finding convincing arguments using scalable bayesian preference learning},
	url = {http://arxiv.org/abs/1806.02418},
	abstract = {We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard rat- ings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.},
	urldate = {2020-12-16},
	journal = {arXiv:1806.02418 [cs]},
	author = {Simpson, Edwin and Gurevych, Iryna},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Computation and Language, ⛔ No DOI found},
}

@article{roebel_efficient_2005,
	title = {Efficient {Spectral} {Envelope} {Estimation} and its application to pitch shifting and envelope preservation},
	abstract = {In this article the estimation of the spectral envelope of sound signals is addressed. The intended application for the developed algorithm is pitch shifting with preservation of the spectral envelope in the phase vocoder. As a ﬁrst step the different existing envelope estimation algorithms are investigated and their speciﬁc properties discussed. As the most promising algorithm the cepstrum based iterative true envelope estimator is selected. By means of controlled sub-sampling of the log amplitude spectrum and by means of a simple step size control for the iterative algorithm the run time of the algorithm can be decreased by a factor of 2.5-11. As a remedy for the ringing effects in the the spectral envelope that are due to the rectangular ﬁlter used for spectral smoothing we propose the use of a Hamming window as smoothing ﬁlter. The resulting implementation of the algorithm has slightly increased computational complexity compared to the standard LPC algorithm but offers signiﬁcantly improved control over the envelope characteristics. The application of the true envelope estimator in a pitch shifting application is investigated. The main problems for pitch shifting with envelope preservation in a phase vocoder are identiﬁed and a simple yet efﬁcient remedy is proposed.},
	author = {Roebel, Axel and Rodet, Xavier},
	year = {2005},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using {Hierarchical} {Navigable} {Small} {World} graphs},
	url = {http://arxiv.org/abs/1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	urldate = {2020-04-27},
	journal = {arXiv:1603.09320 [cs]},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = aug,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Social and Information Networks, ⛔ No DOI found},
}

@article{kong_diffwave_2020-1,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {http://arxiv.org/abs/2009.09761},
	abstract = {In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in Different Waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality{\textasciitilde}(MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	urldate = {2020-09-22},
	journal = {arXiv:2009.09761 [cs, eess, stat]},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{mallat_group_2012,
	title = {Group {Invariant} {Scattering}},
	url = {http://arxiv.org/abs/1101.2286},
	abstract = {This paper constructs translation invariant operators on L2(Rˆd), which are Lipschitz continuous to the action of diffeomorphisms. A scattering propagator is a path ordered product of non-linear and non-commuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz continuous to the action of diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform which is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2 (G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on L2(Rˆd) and on Ld (SO(d)) defines a translation and rotation invariant scattering on L2(Rˆd).},
	urldate = {2019-12-21},
	journal = {arXiv:1101.2286 [cs, math]},
	author = {Mallat, Stéphane},
	month = apr,
	year = {2012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Mathematics - Functional Analysis, unread, ⛔ No DOI found},
}

@article{steinmetz_efficient_2021,
	title = {Efficient neural networks for real-time analog audio effect modeling},
	url = {http://arxiv.org/abs/2102.06200},
	abstract = {Deep learning approaches have demonstrated success in the task of modeling analog audio effects such as distortion and overdrive. Nevertheless, challenges remain in modeling more complex effects, such as dynamic range compressors, along with their variable parameters. Previous methods are computationally complex, and noncausal, prohibiting real-time operation, which is critical for use in audio production contexts. They additionally utilize large training datasets, which are time-intensive to generate. In this work, we demonstrate that shallower temporal convolutional networks (TCNs) that exploit very large dilation factors for significant receptive field can achieve state-of-the-art performance, while remaining efficient. Not only are these models found to be perceptually similar to the original effect, they achieve a 4x speedup, enabling real-time operation on CPU, and can be trained using only 1\% of the data from previous methods.},
	urldate = {2021-05-04},
	journal = {arXiv:2102.06200 [cs, eess]},
	author = {Steinmetz, Christian J. and Reiss, Joshua D.},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{girin_dynamical_2020,
	title = {Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}},
	shorttitle = {Dynamical {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2008.12595},
	abstract = {The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space that is learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In the recent years, a series of papers have presented different extensions of the VAE to sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and/or corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders (DVAEs) that encompass a large subset of these temporal VAE extensions. Then we present in details seven different instances of DVAE that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate those models with existing classical temporal models (that are also presented for the sake of completeness). We reimplemented those seven DVAE models and we present the results of an experimental benchmark that we conducted on the speech analysis-resynthesis task (the PyTorch code will be made publicly available). An extensive discussion is presented at the end of the paper, aiming to comment on important issues concerning the DVAE class of models and to describe future research guidelines.},
	urldate = {2020-11-09},
	journal = {arXiv:2008.12595 [cs, stat]},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{li_disentangled_2018,
	title = {Disentangled {Sequential} {Autoencoder}},
	url = {http://arxiv.org/abs/1803.02991},
	abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
	urldate = {2020-11-09},
	journal = {arXiv:1803.02991 [cs]},
	author = {Li, Yingzhen and Mandt, Stephan},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{kong_diffwave_2020,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {http://arxiv.org/abs/2009.09761},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	urldate = {2021-03-02},
	journal = {arXiv:2009.09761 [cs, eess, stat]},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{mei_disentangle_2020,
	title = {Disentangle perceptual learning through online contrastive learning},
	url = {http://arxiv.org/abs/2006.13511},
	abstract = {Pursuing realistic results according to human visual perception is the central concern in the image transformation tasks. Perceptual learning approaches like perceptual loss are empirically powerful for such tasks but they usually rely on the pre-trained classification network to provide features, which are not necessarily optimal in terms of visual perception of image transformation. In this paper, we argue that, among the features representation from the pre-trained classification network, only limited dimensions are related to human visual perception, while others are irrelevant, although both will affect the final image transformation results. Under such an assumption, we try to disentangle the perception-relevant dimensions from the representation through our proposed online contrastive learning. The resulted network includes the pre-training part and a feature selection layer, followed by the contrastive learning module, which utilizes the transformed results, target images, and task-oriented distorted images as the positive, negative, and anchor samples, respectively. The contrastive learning aims at activating the perception-relevant dimensions and suppressing the irrelevant ones by using the triplet loss, so that the original representation can be disentangled for better perceptual quality. Experiments on various image transformation tasks demonstrate the superiority of our framework, in terms of human visual perception, to the existing approaches using pre-trained networks and empirically designed losses.},
	urldate = {2020-10-13},
	journal = {arXiv:2006.13511 [cs]},
	author = {Mei, Kangfu and Lu, Yao and Yi, Qiaosi and Wu, Haoyu and Li, Juncheng and Huang, Rui},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
}

@article{bando_deep_2019,
	title = {Deep bayesian unsupervised source separation based on a complex gaussian mixture model},
	url = {http://arxiv.org/abs/1908.11307},
	abstract = {This paper presents an unsupervised method that trains neural source separation by using only multichannel mixture signals. Conventional neural separation methods require a lot of supervised data to achieve excellent performance. Although multichannel methods based on spatial information can work without such training data, they are often sensitive to parameter initialization and degraded with the sources located close to each other. The proposed method uses a cost function based on a spatial model called a complex Gaussian mixture model (cGMM). This model has the time-frequency (TF) masks and direction of arrivals (DoAs) of sources as latent variables and is used for training separation and localization networks that respectively estimate these variables. This joint training solves the frequency permutation ambiguity of the spatial model in a unified deep Bayesian framework. In addition, the pre-trained network can be used not only for conducting monaural separation but also for efficiently initializing a multichannel separation algorithm. Experimental results with simulated speech mixtures showed that our method outperformed a conventional initialization method.},
	urldate = {2020-02-28},
	journal = {arXiv:1908.11307 [cs, eess, stat]},
	author = {Bando, Yoshiaki and Sasaki, Yoko and Yoshii, Kazuyoshi},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{arfib_digital_1978,
	title = {Digital synthesis of complex spectra by means of multiplication of non linear distorted sine waves},
	journal = {Journal of the Audio Engineering Society},
	author = {Arfib, Daniel},
	month = feb,
	year = {1978},
	keywords = {⛔ No DOI found},
	pages = {EP --},
}

@article{saeed_contrastive_2020,
	title = {Contrastive {Learning} of {General}-{Purpose} {Audio} {Representations}},
	url = {http://arxiv.org/abs/2010.10915},
	abstract = {We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. Our approach is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning to design a lightweight, easy-to-implement self-supervised model of audio. We pre-train embeddings on the large-scale Audioset database and transfer these representations to 9 diverse classification tasks, including speech, music, animal sounds, and acoustic scenes. We show that despite its simplicity, our method significantly outperforms previous self-supervised systems. We furthermore conduct ablation studies to identify key design choices and release a library to pre-train and fine-tune COLA models.},
	urldate = {2020-10-22},
	journal = {arXiv:2010.10915 [cs, eess]},
	author = {Saeed, Aaqib and Grangier, David and Zeghidour, Neil},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{le_brun_digital_1979,
	title = {Digital {Waveshaping} {Synthesis}},
	volume = {27},
	number = {4},
	journal = {Journal of the Audio Engineering Society},
	author = {Le Brun, Marc},
	month = apr,
	year = {1979},
	keywords = {⛔ No DOI found},
	pages = {250--266},
}

@article{spijkervet_contrastive_2021,
	title = {Contrastive {Learning} of {Musical} {Representations}},
	url = {http://arxiv.org/abs/2103.09410},
	abstract = {While supervised learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations, to form a simple framework for self-supervised learning of raw waveforms of music: CLMR. This approach requires no manual labeling and no preprocessing of music to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets. A linear classifier fine-tuned on representations from a pre-trained CLMR model achieves an average precision of 35.4\% on the MagnaTagATune dataset, superseding fully supervised models that currently achieve a score of 34.9\%. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that they capture important musical knowledge. Lastly, we show that self-supervised pre-training allows us to learn efficiently on smaller labeled datasets: we still achieve a score of 33.1\% despite using only 259 labeled songs during fine-tuning. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper on GitHub.},
	urldate = {2021-04-03},
	journal = {arXiv:2103.09410 [cs, eess]},
	author = {Spijkervet, Janne and Burgoyne, John Ashley},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{bruna_classification_2013,
	title = {Classification with {Scattering} {Operators}},
	url = {http://arxiv.org/abs/1011.3023},
	abstract = {A scattering vector is a local descriptor including multiscale and multi-direction co-occurrence information. It is computed with a cascade of wavelet decompositions and complex modulus. This scattering representation is locally translation invariant and linearizes deformations. A supervised classification algorithm is computed with a PCA model selection on scattering vectors. State of the art results are obtained for handwritten digit recognition and texture classification.},
	urldate = {2019-12-21},
	journal = {arXiv:1011.3023 [cs]},
	author = {Bruna, Joan and Mallat, Stéphane},
	month = nov,
	year = {2013},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, unread, ⛔ No DOI found},
}

@article{gardenfors_conceptual_nodate,
	title = {Conceptual {Spaces} as a {Framework} for {Knowledge} {Representation}},
	abstract = {The dominating models of information processes have been based on symbolic representations of information and knowledge. During the last decades, a variety of non-symbolic models have been proposed as superior. The prime examples of models within the non-symbolic approach are neural networks. However, to a large extent they lack a higher-level theory of representation. In this paper, conceptual spaces are suggested as an appropriate framework for non-symbolic models. Conceptual spaces consist of a number of “quality dimensions” that often are derived from perceptual mechanisms. It will be outlined how conceptual spaces can represent various kind of information and how they can be used to describe concept learning. The connections to prototype theory will also be presented.},
	author = {Gardenfors, Peter},
	keywords = {unread, ⛔ No DOI found},
	pages = {20},
}

@article{veit_conditional_2017,
	title = {Conditional {Similarity} {Networks}},
	url = {http://arxiv.org/abs/1603.07810},
	abstract = {What makes images similar? To measure the similarity between images, they are typically embedded in a feature-vector space, in which their distance preserve the relative dissimilarity. However, when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. A main reason for this is that contradicting notions of similarities cannot be captured in a single space. To address this shortcoming, we propose Conditional Similarity Networks (CSNs) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. CSNs jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion. We show that our approach learns interpretable image representations with visually relevant semantic subspaces. Further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.},
	urldate = {2020-10-13},
	journal = {arXiv:1603.07810 [cs]},
	author = {Veit, Andreas and Belongie, Serge and Karaletsos, Theofanis},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{spurr_cross-modal_2018,
	title = {Cross-modal {Deep} {Variational} {Hand} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1803.11404},
	abstract = {The human hand moves in complex and high-dimensional ways, making estimation of 3D hand pose configurations from images alone a challenging task. In this work we propose a method to learn a statistical hand model represented by a cross-modal trained latent space via a generative deep neural network. We derive an objective function from the variational lower bound of the VAE framework and jointly optimize the resulting cross-modal KL-divergence and the posterior reconstruction objective, naturally admitting a training regime that leads to a coherent latent space across multiple modalities such as RGB images, 2D keypoint detections or 3D hand configurations. Additionally, it grants a straightforward way of using semi-supervision. This latent space can be directly used to estimate 3D hand poses from RGB images, outperforming the state-of-the art in different settings. Furthermore, we show that our proposed method can be used without changes on depth images and performs comparably to specialized methods. Finally, the model is fully generative and can synthesize consistent pairs of hand configurations across modalities. We evaluate our method on both RGB and depth datasets and analyze the latent space qualitatively.},
	urldate = {2019-10-29},
	journal = {arXiv:1803.11404 [cs]},
	author = {Spurr, Adrian and Song, Jie and Park, Seonwook and Hilliges, Otmar},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, unread, ⛔ No DOI found},
}

@article{colonel_conditioning_2020,
	title = {Conditioning autoencoder latent spaces for real-{Time} timbre interpolation and synthesis},
	url = {http://arxiv.org/abs/2001.11296},
	abstract = {We compare standard autoencoder topologies' performances for timbre generation. We demonstrate how different activation functions used in the autoencoder's bottleneck distributes a training corpus's embedding. We show that the choice of sigmoid activation in the bottleneck produces a more bounded and uniformly distributed embedding than a leaky rectified linear unit activation. We propose a one-hot encoded chroma feature vector for use in both input augmentation and latent space conditioning. We measure the performance of these networks, and characterize the latent embeddings that arise from the use of this chroma conditioning vector. An open source, real-time timbre synthesis algorithm in Python is outlined and shared.},
	urldate = {2020-02-13},
	journal = {arXiv:2001.11296 [cs, eess]},
	author = {Colonel, Joseph T. and Keene, Sam},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, unread, ⛔ No DOI found},
}

@article{zhai_classification_2019,
	title = {Classification is a {Strong} {Baseline} for {Deep} {Metric} {Learning}},
	url = {http://arxiv.org/abs/1811.12649},
	abstract = {Deep metric learning aims to learn a function mapping image pixels to embedding feature vectors that model the similarity between images. Two major applications of metric learning are content-based image retrieval and face verification. For the retrieval tasks, the majority of current state-of-the-art (SOTA) approaches are triplet-based non-parametric training. For the face verification tasks, however, recent SOTA approaches have adopted classification-based parametric training. In this paper, we look into the effectiveness of classification based approaches on image retrieval datasets. We evaluate on several standard retrieval datasets such as CAR-196, CUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval and clustering, and establish that our classification-based approach is competitive across different feature dimensions and base feature networks. We further provide insights into the performance effects of subsampling classes for scalable classification-based training, and the effects of binarization, enabling efficient storage and computation for practical applications.},
	urldate = {2020-10-13},
	journal = {arXiv:1811.12649 [cs]},
	author = {Zhai, Andrew and Wu, Hao-Yu},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
}

@article{du_compositional_2020,
	title = {Compositional visual generation and inference with energy based models},
	url = {http://arxiv.org/abs/2004.06030},
	abstract = {A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.},
	urldate = {2021-01-17},
	journal = {arXiv:2004.06030 [cs, stat]},
	author = {Du, Yilun and Li, Shuang and Mordatch, Igor},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{von_oswald_continual_2020,
	title = {Continual learning with hypernetworks},
	url = {http://arxiv.org/abs/1906.00695},
	abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.},
	urldate = {2021-02-11},
	journal = {arXiv:1906.00695 [cs, stat]},
	author = {von Oswald, Johannes and Henning, Christian and Sacramento, João and Grewe, Benjamin F.},
	month = feb,
	year = {2020},
	keywords = {68T99, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{steinmetz_auraloss_2020,
	title = {auraloss: {Audio} focused loss functions in {PyTorch}},
	booktitle = {Digital music research network one-day workshop ({DMRN}+15)},
	author = {Steinmetz, Christian J. and Reiss, Joshua D.},
	year = {2020},
	keywords = {⛔ No DOI found},
}

@inproceedings{nuanain_interactive_2016,
	title = {An interactive software instrument for real-time rhythmic concatenative synthesis},
	abstract = {In this paper we describe an approach for generating and visualising new rhythmic patterns from existing audio in real-time using concatenative synthesis. We introduce a graph-based model enabling novel visualisation and manipulation of new patterns that mimics the rhythmic and timbral character of an existing target seed pattern using a separate database of palette sounds. Our approach is described, reporting on those features that may be useful in describing units of sound related to rhythm and how they might then be projected into two-dimensional space for visualisation using reduction techniques and clustering. We conclude the paper with our qualitative appraisal of using the interface and outline scope for future work.},
	booktitle = {New {Interfaces} for {Musical} {Expression} ({NIME})},
	author = {Nuanáin, Cárthach Ó and Jordà, Sergi and Herrera, Perfecto},
	year = {2016},
	keywords = {⛔ No DOI found},
	pages = {5},
}

@article{esling_creativity_2020,
	title = {Creativity in the era of artificial intelligence},
	url = {http://arxiv.org/abs/2008.05959},
	abstract = {Creativity is a deeply debated topic, as this concept is arguably quintessential to our humanity. Across different epochs, it has been infused with an extensive variety of meanings relevant to that era. Along these, the evolution of technology have provided a plurality of novel tools for creative purposes. Recently, the advent of Artificial Intelligence (AI), through deep learning approaches, have seen proficient successes across various applications. The use of such technologies for creativity appear in a natural continuity to the artistic trend of this century. However, the aura of a technological artefact labeled as intelligent has unleashed passionate and somewhat unhinged debates on its implication for creative endeavors. In this paper, we aim to provide a new perspective on the question of creativity at the era of AI, by blurring the frontier between social and computational sciences. To do so, we rely on reflections from social science studies of creativity to view how current AI would be considered through this lens. As creativity is a highly context-prone concept, we underline the limits and deficiencies of current AI, requiring to move towards artificial creativity. We argue that the objective of trying to purely mimic human creative traits towards a self-contained ex-nihilo generative machine would be highly counterproductive, putting us at risk of not harnessing the almost unlimited possibilities offered by the sheer computational power of artificial agents.},
	urldate = {2020-12-15},
	journal = {arXiv:2008.05959 [cs]},
	author = {Esling, Philippe and Devis, Ninon},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{romero_ckconv_2021,
	title = {{CKConv}: {Continuous} {Kernel} {Convolution} {For} {Sequential} {Data}},
	shorttitle = {{CKConv}},
	url = {http://arxiv.org/abs/2102.02611},
	abstract = {Conventional neural architectures for sequential data present important limitations. Recurrent networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional networks are unable to handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that all these problems can be solved by formulating convolutional kernels in CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) allows us to model arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a much faster and simpler manner.},
	urldate = {2021-04-28},
	journal = {arXiv:2102.02611 [cs]},
	author = {Romero, David W. and Kuzina, Anna and Bekkers, Erik J. and Tomczak, Jakub M. and Hoogendoorn, Mark},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{stilson_alias-free_1996,
	title = {Alias-{Free} {Digital} {Synthesis} of {Classic} {Analog} {Waveforms}},
	abstract = {Techniques are reviewed and presented for alias-free digital synthesis of classical analog synthesizer waveforms such as pulse train and sawtooth waves. Techniques described include summation of bandlimited primitive waveforms as well as table-lookup techniques. Bandlimited pulse and triangle waveforms are obtained by integrating the difference of two out-of-phase bandlimited impulse trains. Bandlimited impulse trains are generated as a superposition of windowed sinc functions. Methods for more general bandlimited waveform synthesis are also reviewed. Methods are evaluated from the perspectives of sound quality, computational economy, and ease of control.},
	author = {Stilson, Tim and Smith, Julius},
	year = {1996},
	keywords = {synthesis, ⛔ No DOI found},
	pages = {12},
}

@inproceedings{kawai_attributes-aware_2020,
	address = {Montréal},
	title = {Attributes-aware {Deep} {Music} {Transformation}},
	abstract = {Recent machine learning techniques have enabled a large variety of novel music generation processes. However, most approaches do not provide any form of interpretable control over musical attributes, such as pitch and rhythm. Obtaining control over the generation process is critically important for its use in real-life creative setups. Nevertheless, this problem remains arduous, as there are no known functions nor differentiable approximations to transform symbolic music with control of musical attributes.},
	booktitle = {Proceedings of the 21th international society for music information retrieval conference},
	author = {Kawai, Lisa and Esling, Philippe and Harada, Tatsuya},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@inproceedings{perez2019analysis,
	title = {Analysis of statistical forward planning methods in {Pommerman}},
	volume = {15},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence and interactive digital entertainment},
	author = {Perez-Liebana, Diego and Gaina, Raluca D and Drageset, Olve and Ilhan, Ercüment and Balla, Martin and Lucas, Simon M},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {66--72},
}

@article{jolicoeur-martineau_adversarial_2020,
	title = {Adversarial score matching and improved sampling for image generation},
	url = {http://arxiv.org/abs/2009.05475},
	abstract = {Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fr{\textbackslash}'echet Inception Distance, a standard metric for generative models. We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.},
	urldate = {2021-02-20},
	journal = {arXiv:2009.05475 [cs, stat]},
	author = {Jolicoeur-Martineau, Alexia and Piché-Taillefer, Rémi and Combes, Rémi Tachet des and Mitliagkas, Ioannis},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{skorokhodov_adversarial_2020,
	title = {Adversarial {Generation} of {Continuous} {Images}},
	url = {http://arxiv.org/abs/2011.12026},
	abstract = {In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) – an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed architectural design improves the performance of continuous image generators by x6-40 times and reaches FID scores of 6.27 on LSUN bedroom 256x256 and 16.32 on FFHQ 1024x1024, greatly reducing the gap between continuous image GANs and pixel-based ones. To the best of our knowledge, these are the highest reported scores for an image generator, that consists entirely of fully-connected layers. Apart from that, we explore several exciting properties of INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries and strong geometric prior. The source code is available at https://github.com/universome/inr-gan},
	urldate = {2021-06-22},
	journal = {arXiv:2011.12026 [cs]},
	author = {Skorokhodov, Ivan and Ignatyev, Savva and Elhoseiny, Mohamed},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No DOI found},
}

@inproceedings{helmbold_all-moves-as-first_2009,
	address = {Las Vegas, NV, USA},
	title = {All-{Moves}-{As}-{First} {Heuristics} in {Monte}-{Carlo} {Go}},
	volume = {2},
	booktitle = {Proceedings of the 2009 international conference on artificial intelligence},
	author = {Helmbold, David P. and Parker-Wood, Aleatha},
	year = {2009},
	keywords = {⛔ No DOI found},
}

@article{yu_tutorial_2020,
	title = {A {Tutorial} on {VAEs}: {From} {Bayes}' {Rule} to {Lossless} {Compression}},
	shorttitle = {A {Tutorial} on {VAEs}},
	url = {http://arxiv.org/abs/2006.10273},
	abstract = {The Variational Auto-Encoder (VAE) is a simple, efficient, and popular deep maximum likelihood model. Though usage of VAEs is widespread, the derivation of the VAE is not as widely understood. In this tutorial, we will provide an overview of the VAE and a tour through various derivations and interpretations of the VAE objective. From a probabilistic standpoint, we will examine the VAE through the lens of Bayes' Rule, importance sampling, and the change-of-variables formula. From an information theoretic standpoint, we will examine the VAE through the lens of lossless compression and transmission through a noisy channel. We will then identify two common misconceptions over the VAE formulation and their practical consequences. Finally, we will visualize the capabilities and limitations of VAEs using a code example (with an accompanying Jupyter notebook) on toy 2D data.},
	urldate = {2020-06-23},
	journal = {arXiv:2006.10273 [cs, stat]},
	author = {Yu, Ronald},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{verma_framework_2021,
	title = {A framework for generative and contrastive learning of audio representations},
	url = {http://arxiv.org/abs/2010.11459},
	abstract = {In this paper, we present a framework for contrastive learning for audio representations, in a self supervised frame work without access to any ground truth labels. The core idea in self supervised contrastive learning is to map an audio signal and its various augmented versions (representative of salient aspects of audio like pitch, timbre etc.) to a space where they are close together, and are separated from other different signals. In addition we also explore generative models based on state of the art transformer based architectures for learning latent spaces for audio signals, without access to any labels. Here, we map audio signals on a smaller scale to discrete dictionary elements and train transformers to predict the next dictionary element. We only use data as a method of supervision, bypassing the need of labels needed to act as a supervision for training the deep neural networks. We then use a linear classifier head in order to evaluate the performance of our models, for both self supervised contrastive and generative transformer based representations that are learned. Our system achieves considerable performance, compared to a fully supervised method, with access to ground truth labels to train the neural network model. These representations, with avail-ability of large scale audio data show promise in various tasks for audio understanding tasks},
	urldate = {2021-04-03},
	journal = {arXiv:2010.11459 [cs, eess]},
	author = {Verma, Prateek and Smith, Julius},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{simpson_bayesian_2019,
	title = {A {Bayesian} {Approach} for {Sequence} {Tagging} with {Crowds}},
	url = {http://arxiv.org/abs/1811.00780},
	abstract = {Current methods for sequence tagging, a core task in NLP, are data hungry, which motivates the use of crowdsourcing as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods cannot capture common types of span annotation errors. To address this, we propose a Bayesian method for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations.},
	urldate = {2020-12-16},
	journal = {arXiv:1811.00780 [cs]},
	author = {Simpson, Edwin and Gurevych, Iryna},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{vuong_modulation-domain_2021,
	title = {A modulation-{Domain} loss for neural-{Network}-based real-time speech enhancement},
	url = {http://arxiv.org/abs/2102.07330},
	abstract = {We describe a modulation-domain loss function for deep-learning-based speech enhancement systems. Learnable spectro-temporal receptive fields (STRFs) were adapted to optimize for a speaker identification task. The learned STRFs were then used to calculate a weighted mean-squared error (MSE) in the modulation domain for training a speech enhancement system. Experiments showed that adding the modulation-domain MSE to the MSE in the spectro-temporal domain substantially improved the objective prediction of speech quality and intelligibility for real-time speech enhancement systems without incurring additional computation during inference.},
	urldate = {2021-03-12},
	journal = {arXiv:2102.07330 [eess]},
	author = {Vuong, Tyler and Xia, Yangyang and Stern, Richard M.},
	month = feb,
	year = {2021},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{roche_autoencoders_2019,
	title = {Autoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models},
	shorttitle = {Autoencoders for music sound modeling},
	url = {http://arxiv.org/abs/1806.04096},
	abstract = {This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders (AEs), deep autoencoders (DAEs), recurrent autoencoders (with Long Short-Term Memory cells – LSTM-AEs) and variational autoencoders (VAEs) with principal component analysis (PCA) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multi-instrument and multi-pitch database NSynth. Interestingly and contrary to the recent literature on image processing, we can show that PCA systematically outperforms shallow AE. Only deep and recurrent architectures (DAEs and LSTM-AEs) lead to a lower reconstruction error. The optimization criterion in VAEs being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than DAEs but we show that VAEs are still able to outperform PCA while providing a low-dimensional latent space with nice "usability" properties. We also provide corresponding objective measures of perceptual audio quality (PEMO-Q scores), which generally correlate well with the reconstruction error.},
	urldate = {2021-03-16},
	journal = {arXiv:1806.04096 [cs, eess]},
	author = {Roche, Fanny and Hueber, Thomas and Limier, Samuel and Girin, Laurent},
	month = may,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{henderson_audio_2019,
	title = {Audio transport: {A} generalized portamento via optimal transport},
	shorttitle = {Audio {Transport}},
	url = {http://arxiv.org/abs/1906.06763},
	abstract = {This paper proposes a new method to interpolate between two audio signals. As an interpolation parameter is changed, the pitches in one signal slide to the pitches in the other, producing a portamento, or musical glide. The assignment of pitches in one sound to pitches in the other is accomplished by solving a 1-dimensional optimal transport problem. In addition, we introduce several techniques that preserve the audio fidelity over this highly non-linear transformation. A portamento is a natural way for a musician to transition between notes, but traditionally it has only been possible for instruments with a continuously variable pitch like the human voice or the violin. Audio transport extends the portamento to any instrument, even polyphonic ones. Moreover, the effect can be used to transition between different instruments, groups of instruments, or really any transient-less audio signals. The audio transport effect operates in real-time; we open-source implementation is provided. In experiments with sinusoidal inputs, the interpolating effect is indistinguishable from ideal sine sweeps. In general, the effect produces clear, musical results for a wide variety of inputs.},
	urldate = {2021-04-08},
	journal = {arXiv:1906.06763 [cs, eess]},
	author = {Henderson, Trevor and Solomon, Justin},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{shi_notitle_2021,
	url = {http://arxiv.org/abs/2107.11506},
	abstract = {Timbre representations of musical instruments, essential for diverse applications such as musical audio synthesis and separation, might be learned as bottleneck features from an instrumental recognition model. Given the similarities between speaker recognition and musical instrument recognition, in this paper, we investigate how to adapt successful speaker recognition algorithms to musical instrument recognition to learn meaningful instrumental timbre representations. To address the mismatch between musical audio and models devised for speech, we introduce a group of trainable filters to generate proper acoustic features from input raw waveforms, making it easier for a model to be optimized in an input-agnostic and end-to-end manner. Through experiments on both the NSynth and RWC databases in both musical instrument closed-set identification and open-set verification scenarios, the modified speaker recognition model was capable of generating discriminative embeddings for instrument and instrument-family identities. We further conducted extensive experiments to characterize the encoded information in learned timbre embeddings.},
	urldate = {2021-07-29},
	journal = {arXiv:2107.11506 [cs, eess]},
	author = {Shi, Xuan and Cooper, Erica and Yamagishi, Junichi},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{grill_identification_2011,
	address = {Coimbra, Portugal},
	title = {Identification of perceptual qualities in textural sounds using the repertory grid method},
	isbn = {978-1-4503-1081-9},
	doi = {10.1145/2095667.2095677},
	abstract = {This paper is about exploring which perceptual qualities are relevant to people listening to textural sounds. Knowledge about those personal constructs shall eventually lead to more intuitive interfaces for browsing large sound libraries. By conducting mixed qualitative-quantitative interviews within the repertory grid framework ten bi-polar qualities are identiﬁed. A subsequent web-based study yields measures for inter-rater agreement and mutual similarity of the perceptual qualities based on a selection of 100 textural sounds. Additionally, some initial experiments are conducted to test standard audio descriptors for their correlation with the perceptual qualities.},
	urldate = {2020-05-28},
	booktitle = {Proceedings of the 6th audio mostly conference on a conference on interaction with sound - {AM} '11},
	publisher = {ACM Press},
	author = {Grill, Thomas and Flexer, Arthur and Cunningham, Stuart},
	month = sep,
	year = {2011},
	keywords = {unread},
	pages = {67--74},
}

@article{conan_intuitive_2014,
	title = {An intuitive synthesizer of continuous interaction sounds: {Rubbing}, scratching and rolling},
	volume = {38},
	shorttitle = {An {Intuitive} {Synthesizer} of {Continuous} {Interaction} {Sounds}},
	doi = {10.1162/COMJ_a_00266},
	abstract = {In this paper, we propose a control strategy for synthesized continuous interaction sounds. The framework of our research is based on the {\textbackslash}emph\{action-object\} paradigm that describes the sound as the result of an action on an object and that presumes the existence of sound invariants, i.e. perceptually relevant signal morphologies that carry information about the action's or the object's attributes. Auditory cues are here investigated for the evocations of rubbing, scratching and rolling interactions. A generic sound synthesis model that simulates these interactions is detailed. Then, we propose an intuitive control strategy that enables users to navigate continuously from one interaction to another in an ”action space”, hereby offering the possibility to simulate morphed interactions, for instance between rubbing and rolling.},
	number = {4},
	urldate = {2021-11-24},
	journal = {Computer Music Journal},
	author = {Conan, Simon and Thoret, Etienne and Mitsuko, Aramaki and Derrien, Olivier and Charles, Gondre and Ystad, Solvi and Kronland-Martinet, Richard},
	month = dec,
	year = {2014},
}

@article{vertegaal_isee_1994-1,
	title = {{ISEE}: {An} {Intuitive} {Sound} {Editing} {Environment}},
	volume = {18},
	issn = {0148-9267},
	shorttitle = {{ISEE}},
	doi = {10.2307/3680440},
	number = {2},
	urldate = {2021-12-07},
	journal = {Computer Music Journal},
	author = {Vertegaal, Roel and Bonis, Ernst},
	month = jan,
	year = {1994},
	pages = {21--29},
}

@article{warriner_norms_2013,
	title = {Norms of valence, arousal, and dominance for 13,915 {English} lemmas},
	volume = {45},
	issn = {1554-3528},
	doi = {10.3758/s13428-012-0314-x},
	number = {4},
	urldate = {2021-03-08},
	journal = {Behavior Research Methods},
	author = {Warriner, Amy Beth and Kuperman, Victor and Brysbaert, Marc},
	month = feb,
	year = {2013},
	pages = {1191--1207},
}

@article{vertegaal_isee_1994,
	title = {{ISEE}: {An} {Intuitive} {Sound} {Editing} {Environment}},
	volume = {18},
	issn = {01489267},
	shorttitle = {{ISEE}},
	doi = {10.2307/3680440},
	number = {2},
	urldate = {2020-07-11},
	journal = {Computer Music Journal},
	author = {Vertegaal, Roel and Bonis, Ernst},
	month = jan,
	year = {1994},
	pages = {21},
}

@article{sankaranarayanan_cdinn_2021,
	title = {{CDiNN} -{Convex} {Difference} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2103.17231},
	abstract = {Neural networks with ReLU activation function have been shown to be universal function approximators and learn function mapping as non-smooth functions. Recently, there is considerable interest in the use of neural networks in applications such as optimal control. It is well-known that optimization involving non-convex, non-smooth functions are computationally intensive and have limited convergence guarantees. Moreover, the choice of optimization hyper-parameters used in gradient descent/ascent significantly affect the quality of the obtained solutions. A new neural network architecture called the Input Convex Neural Networks (ICNNs) learn the output as a convex function of inputs thereby allowing the use of efficient convex optimization methods. Use of ICNNs for determining the input for minimizing output has two major problems: learning of a non-convex function as a convex mapping could result in significant function approximation error, and we also note that the existing representations cannot capture simple dynamic structures like linear time delay systems. We attempt to address the above problems by introduction of a new neural network architecture, which we call the CDiNN, which learns the function as a difference of polyhedral convex functions from data. We also discuss that, in some cases, the optimal input can be obtained from CDiNN through difference of convex optimization with convergence guarantees and that at each iteration, the problem is reduced to a linear programming problem.},
	urldate = {2022-02-03},
	journal = {arXiv:2103.17231 [cs, math]},
	author = {Sankaranarayanan, Parameswaran and Rengaswamy, Raghunathan},
	month = apr,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, ⛔ No DOI found},
}

@article{zaheer_deep_2018,
	title = {Deep {Sets}},
	url = {http://arxiv.org/abs/1703.06114},
	abstract = {We study the problem of designing models for machine learning tasks defined on {\textbackslash}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics {\textbackslash}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams {\textbackslash}cite\{Jung15Exploration\}, to cosmology {\textbackslash}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	urldate = {2022-02-03},
	journal = {arXiv:1703.06114 [cs, stat]},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{lam_timbre_2021,
	address = {Shanghai, China},
	title = {The timbre explorer: {A} synthesizer interface for educational purposes and perceptual studies},
	shorttitle = {The {Timbre} {Explorer}},
	doi = {10.21428/92fbeb44.92a95683},
	urldate = {2021-12-07},
	booktitle = {{NIME} 2021},
	publisher = {PubPub},
	author = {Lam, Joshua Ryan and Saitis, Charalampos},
	month = jun,
	year = {2021},
}

@inproceedings{bromham_impact_2019,
	address = {Nottingham, United Kingdom},
	title = {The impact of audio effects processing on the perception of brightness and warmth},
	isbn = {978-1-4503-7297-8},
	doi = {10.1145/3356590.3356618},
	abstract = {CCS Concepts: • Information systems → Speech / audio search; • Applied computing → Sound and music computing; Media arts; • Computing methodologies → Perception; • Software and its engineering → Semantics.},
	urldate = {2020-04-17},
	booktitle = {Proceedings of the 14th international audio mostly conference: {A} journey in sound},
	publisher = {ACM Press},
	author = {Bromham, Gary and Moffat, David and Barthet, Mathieu and Danielsen, Anne and Fazekas, György},
	month = sep,
	year = {2019},
	keywords = {unread},
	pages = {183--190},
}

@article{bechtle_meta-learning_2021,
	title = {Meta-{Learning} via {Learned} {Loss}},
	url = {http://arxiv.org/abs/1906.05374},
	abstract = {Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for meta-training such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree.},
	urldate = {2022-02-03},
	journal = {arXiv:1906.05374 [cs, stat]},
	author = {Bechtle, Sarah and Molchanov, Artem and Chebotar, Yevgen and Grefenstette, Edward and Righetti, Ludovic and Sukhatme, Gaurav and Meier, Franziska},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{mauch_pyin_2014,
	address = {Florence, Italy},
	title = {{PYIN}: {A} fundamental frequency estimator using probabilistic threshold distributions},
	isbn = {978-1-4799-2893-4},
	shorttitle = {{PYIN}},
	url = {http://ieeexplore.ieee.org/document/6853678/},
	doi = {10.1109/ICASSP.2014.6853678},
	abstract = {We propose the Probabilistic YIN (PYIN) algorithm, a modiﬁcation of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online1 as an open source C++ library for Vamp hosts.},
	urldate = {2022-02-25},
	booktitle = {2014 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Mauch, Matthias and Dixon, Simon},
	month = may,
	year = {2014},
	pages = {659--663},
}

@article{thome_polyphonic_nodate,
	title = {Polyphonic pitch detection with convolutional recurrent neural networks},
	abstract = {Recent directions in automatic speech recognition (ASR) research have shown that applying deep learning models from image recognition challenges in computer vision is beneﬁcial. As automatic music transcription (AMT) is superﬁcially similar to ASR, in the sense that methods often rely on transforming spectrograms to symbolic sequences of events (e.g. words or notes), deep learning should beneﬁt AMT as well. In this work, we outline an online polyphonic pitch detection system that streams audio to MIDI by ConvLSTMs. Our system achieves state-of-the-art results on the 2007 MIREX multi-F0 development set, with an f-measure of 83\% on the bassoon, clarinet, ﬂute, horn and oboe ensemble recording without requiring any musical language modelling or assumptions of instrument timbre.},
	author = {Thome, Carl and Ahlback, Sven},
	keywords = {⛔ No DOI found},
	pages = {4},
}

@article{thome_polyphonic_2022,
	title = {Polyphonic pitch detection with convolutional recurrent neural networks},
	url = {http://arxiv.org/abs/2202.02115},
	abstract = {Recent directions in automatic speech recognition (ASR) research have shown that applying deep learning models from image recognition challenges in computer vision is beneficial. As automatic music transcription (AMT) is superficially similar to ASR, in the sense that methods often rely on transforming spectrograms to symbolic sequences of events (e.g. words or notes), deep learning should benefit AMT as well. In this work, we outline an online polyphonic pitch detection system that streams audio to MIDI by ConvLSTMs. Our system achieves state-of-the-art results on the 2007 MIREX multi-F0 development set, with an F-measure of 83{\textbackslash}\% on the bassoon, clarinet, flute, horn and oboe ensemble recording without requiring any musical language modelling or assumptions of instrument timbre.},
	urldate = {2022-02-25},
	journal = {arXiv:2202.02115 [cs, eess]},
	author = {Thomé, Carl and Ahlbäck, Sven},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{elowsson_polyphonic_2019,
	title = {Polyphonic {Pitch} {Tracking} with {Deep} {Layered} {Learning}},
	url = {http://arxiv.org/abs/1804.02918},
	abstract = {This paper presents a polyphonic pitch tracking system able to extract both framewise and note-based estimates from audio. The system uses several artificial neural networks in a deep layered learning setup. First, cascading networks are applied to a spectrogram for framewise fundamental frequency (f0) estimation. A sparse receptive field is learned by the first network and then used as a filter kernel for parameter sharing throughout the system. The f0 activations are connected across time to extract pitch contours. These contours define a framework within which subsequent networks perform onset and offset detection, operating across both time and smaller pitch fluctuations at the same time. As input, the networks use, e.g., variations of latent representations from the f0 estimation network. Finally, incorrect tentative notes are removed one by one in an iterative procedure that allows a network to classify notes within an accurate context. The system was evaluated on four public test sets: MAPS, Bach10, TRIOS, and the MIREX Woodwind quintet, and performed state-of-the-art results for all four datasets. It performs well across all subtasks: f0, pitched onset, and pitched offset tracking.},
	urldate = {2022-02-25},
	journal = {arXiv:1804.02918 [cs, eess]},
	author = {Elowsson, Anders},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{nicolet_large_2021,
	title = {Large steps in inverse rendering of geometry},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480501},
	doi = {10.1145/3478513.3480501},
	abstract = {Inverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity. Such robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi's iterative method for solving linear equations that is known for its exceptionally slow convergence. We propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our pre-conditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps. Our method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.},
	number = {6},
	urldate = {2022-03-02},
	journal = {ACM Transactions on Graphics},
	author = {Nicolet, Baptiste and Jacobson, Alec and Jakob, Wenzel},
	month = dec,
	year = {2021},
	pages = {1--13},
}

@inproceedings{wang_autoregressive_2017,
	address = {New Orleans, LA},
	title = {An autoregressive recurrent mixture density network for parametric speech synthesis},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953087/},
	doi = {10.1109/ICASSP.2017.7953087},
	urldate = {2022-03-08},
	booktitle = {2017 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = mar,
	year = {2017},
	pages = {4895--4899},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	keywords = {Convex functions, Mathematical optimization},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-03-08},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing flows: {An} introduction and review of current methods},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {https://ieeexplore.ieee.org/document/9089305/},
	doi = {10.1109/TPAMI.2020.2992934},
	number = {11},
	urldate = {2022-03-09},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J.D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	pages = {3964--3979},
}

@article{chen_wavegrad_2020-1,
	title = {{WaveGrad}: {Estimating} {Gradients} for {Waveform} {Generation}},
	shorttitle = {{WaveGrad}},
	url = {http://arxiv.org/abs/2009.00713},
	abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
	urldate = {2022-03-09},
	journal = {arXiv:2009.00713 [cs, eess, stat]},
	author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{chen_wavegrad_2020,
	title = {{WaveGrad}: {Estimating} {Gradients} for {Waveform} {Generation}},
	shorttitle = {{WaveGrad}},
	url = {http://arxiv.org/abs/2009.00713},
	abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
	urldate = {2022-03-09},
	journal = {arXiv:2009.00713 [cs, eess, stat]},
	author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{song_generative_2019,
	title = {Generative modeling by estimating gradients of the data distribution},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
	urldate = {2022-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
	keywords = {⛔ No DOI found},
}

@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2022-03-10},
	journal = {arXiv:2006.11239 [cs, stat]},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{manocha_cdpam_2021,
	address = {Toronto, ON, Canada},
	title = {{CDPAM}: {Contrastive} {Learning} for {Perceptual} {Audio} {Similarity}},
	isbn = {978-1-72817-605-5},
	shorttitle = {{CDPAM}},
	url = {https://ieeexplore.ieee.org/document/9413711/},
	doi = {10.1109/ICASSP39728.2021.9413711},
	urldate = {2022-03-11},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Manocha, Pranay and Jin, Zeyu and Zhang, Richard and Finkelstein, Adam},
	month = jun,
	year = {2021},
	pages = {196--200},
}

@article{xie_neural_2021,
	title = {Neural {Fields} in {Visual} {Computing} and {Beyond}},
	url = {http://arxiv.org/abs/2111.11426},
	abstract = {Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.},
	urldate = {2022-03-12},
	journal = {arXiv:2111.11426 [cs]},
	author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{chan_pi-gan_2021,
	title = {pi-{GAN}: {Periodic} implicit generative adversarial networks for {3D}-{Aware} image synthesis},
	shorttitle = {pi-{GAN}},
	url = {http://arxiv.org/abs/2012.00926},
	abstract = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (\${\textbackslash}pi\$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. \${\textbackslash}pi\$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.},
	urldate = {2022-03-12},
	journal = {arXiv:2012.00926 [cs]},
	author = {Chan, Eric R. and Monteiro, Marco and Kellnhofer, Petr and Wu, Jiajun and Wetzstein, Gordon},
	month = apr,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No DOI found},
}

@incollection{feragen_deep_2015,
	address = {Cham},
	title = {Deep {Metric} {Learning} {Using} {Triplet} {Network}},
	volume = {9370},
	isbn = {978-3-319-24260-6 978-3-319-24261-3},
	url = {http://link.springer.com/10.1007/978-3-319-24261-3%5F7},
	urldate = {2022-03-13},
	booktitle = {Similarity-{Based} {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Hoffer, Elad and Ailon, Nir},
	editor = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	year = {2015},
	doi = {10.1007/978-3-319-24261-3_7},
	pages = {84--92},
}

@article{metz_gradients_2022,
	title = {Gradients are {Not} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2111.05803},
	abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
	urldate = {2022-03-13},
	journal = {arXiv:2111.05803 [cs, stat]},
	author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{chen_simple_2020,
	title = {A simple framework for contrastive learning of visual representations},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2022-03-13},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	pages = {1597--1607},
}

@article{hospedales_meta-learning_2021,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9428530/},
	doi = {10.1109/TPAMI.2021.3079209},
	urldate = {2022-03-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hospedales, Timothy M and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
	year = {2021},
	pages = {1--1},
}

@article{huisman_survey_2021,
	title = {A survey of deep meta-learning},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-021-10004-4},
	doi = {10.1007/s10462-021-10004-4},
	abstract = {Abstract Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into (i) metric-, (ii) model-, and (iii) optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.},
	number = {6},
	urldate = {2022-03-13},
	journal = {Artificial Intelligence Review},
	author = {Huisman, Mike and van Rijn, Jan N. and Plaat, Aske},
	month = aug,
	year = {2021},
	pages = {4483--4541},
}

@article{snell_prototypical_2017,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
	urldate = {2022-03-13},
	journal = {arXiv:1703.05175 [cs, stat]},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{wang_learning_2017,
	title = {Learning to reinforcement learn},
	url = {http://arxiv.org/abs/1611.05763},
	abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
	urldate = {2022-03-13},
	journal = {arXiv:1611.05763 [cs, stat]},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{finn_guided_nodate,
	title = {Guided cost learning: {Deep} inverse optimal control via policy optimization},
	abstract = {Reinforcement learning can acquire complex behaviors from high-level speciﬁcations. However, deﬁning a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: ﬁrst, the need for informative features and effective regularization to impose structure on the cost, and second, the difﬁculty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efﬁcient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efﬁciency.},
	author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
	keywords = {⛔ No DOI found},
	pages = {10},
}

@misc{noauthor_170305175_nodate,
	title = {[1703.05175] {Prototypical} {Networks} for {Few}-shot {Learning}},
	url = {https://arxiv.org/abs/1703.05175},
	urldate = {2022-03-13},
}

@inproceedings{vinyals_matching_2016,
	title = {Matching networks for one shot learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {⛔ No DOI found},
}

@inproceedings{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html},
	urldate = {2022-03-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Andrychowicz, Marcin and Denil, Misha and Gómez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	year = {2016},
	keywords = {⛔ No DOI found},
}

@inproceedings{chen_deep_2019,
	address = {Seoul, Korea (South)},
	title = {Deep {Meta} {Metric} {Learning}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009474/},
	doi = {10.1109/ICCV.2019.00964},
	urldate = {2022-03-13},
	booktitle = {2019 {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	publisher = {IEEE},
	author = {Chen, Guangyi and Zhang, Tianren and Lu, Jiwen and Zhou, Jie},
	month = oct,
	year = {2019},
	pages = {9546--9555},
}

@inproceedings{sung_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning to {Compare}: {Relation} {Network} for {Few}-{Shot} {Learning}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Learning to {Compare}},
	url = {https://ieeexplore.ieee.org/document/8578229/},
	doi = {10.1109/CVPR.2018.00131},
	abstract = {We present a conceptually simple, ﬂexible, and general framework for few-shot learning, where a classiﬁer must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on ﬁve benchmarks demonstrate that our simple approach provides a uniﬁed and effective approach for both of these two tasks.},
	urldate = {2022-03-13},
	booktitle = {2018 {IEEE}/{CVF} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H.S. and Hospedales, Timothy M.},
	month = jun,
	year = {2018},
	pages = {1199--1208},
}

@article{duan_rl2_2016,
	title = {{RL}\$ˆ2\$: {Fast} reinforcement learning via slow reinforcement learning},
	shorttitle = {{RL}\$ˆ2\$},
	url = {http://arxiv.org/abs/1611.02779},
	abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$ˆ2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$ˆ2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$ˆ2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$ˆ2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
	urldate = {2022-03-13},
	journal = {arXiv:1611.02779 [cs, stat]},
	author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, ⛔ No DOI found},
}

@inproceedings{perez_film_2018,
	address = {New Orleans, Louisiana, USA},
	series = {{AAAI}'18/{IAAI}'18/{EAAI}'18},
	title = {{FiLM}: visual reasoning with a general conditioning layer},
	isbn = {978-1-57735-800-8},
	shorttitle = {{FiLM}},
	abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
	urldate = {2022-03-13},
	booktitle = {Proceedings of the thirty-{Second} {AAAI} conference on artificial intelligence and thirtieth innovative applications of artificial intelligence conference and eighth {AAAI} symposium on educational advances in artificial intelligence},
	publisher = {AAAI Press},
	author = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	month = feb,
	year = {2018},
	pages = {3942--3951},
}

@inproceedings{munkhdalai_meta_2017,
	series = {Proceedings of machine learning research},
	title = {Meta networks},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/munkhdalai17a.html},
	abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
	booktitle = {Proceedings of the 34th international conference on machine learning},
	publisher = {PMLR},
	author = {Munkhdalai, Tsendsuren and Yu, Hong},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {2554--2563},
}

@inproceedings{finn_model-agnostic_2017,
	series = {Proceedings of machine learning research},
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	booktitle = {Proceedings of the 34th international conference on machine learning},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {1126--1135},
}

@inproceedings{takikawa_neural_2021,
	address = {Nashville, TN, USA},
	title = {Neural geometric level of detail: {Real}-time rendering with implicit {3D} shapes},
	isbn = {978-1-66544-509-2},
	shorttitle = {Neural {Geometric} {Level} of {Detail}},
	url = {https://ieeexplore.ieee.org/document/9578205/},
	doi = {10.1109/CVPR46437.2021.01120},
	abstract = {Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-theart methods typically encode the SDF with a large, ﬁxedsize neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efﬁcient neural representation that, for the ﬁrst time, enables real-time rendering of high-ﬁdelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively ﬁts shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efﬁcient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efﬁcient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.},
	urldate = {2022-03-14},
	booktitle = {2021 {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	publisher = {IEEE},
	author = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
	month = jun,
	year = {2021},
	pages = {11353--11362},
}

@inproceedings{fathony_multiplicative_2021,
	title = {Multiplicative filter networks},
	url = {https://openreview.net/forum?id=OmtmcPkkhT},
	booktitle = {International conference on learning representations},
	author = {Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J Zico},
	year = {2021},
	keywords = {⛔ No DOI found},
}

@inproceedings{sitzmann_metasdf_2020,
	title = {{MetaSDF}: {Meta}-learning signed distance functions},
	url = {https://proceedings.neurips.cc/paper/2020/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html},
	booktitle = {Advances in neural information processing systems 33: {Annual} conference on neural information processing systems 2020, {NeurIPS} 2020, december 6-12, 2020, virtual},
	author = {Sitzmann, Vincent and Chan, Eric R. and Tucker, Richard and Snavely, Noah and Wetzstein, Gordon},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	keywords = {⛔ No DOI found},
}

@article{ethington1994seawave,
	title = {{SeaWave}: {A} system for musical timbre description},
	volume = {18},
	doi = {10.2307/3680520},
	number = {1},
	journal = {Computer Music Journal},
	author = {Ethington, Russ and Punch, Bill},
	month = jan,
	year = {1994},
	pages = {30--39},
}

@article{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} scenes as neural radiance fields for view synthesis},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2022-03-14},
	journal = {arXiv:2003.08934 [cs]},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No DOI found},
}

@inproceedings{nguyen_point-set_2021,
	address = {Montreal, QC, Canada},
	title = {Point-set distances for learning representations of {3D} point clouds},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711238/},
	doi = {10.1109/ICCV48922.2021.01031},
	abstract = {Learning an effective representation of 3D point clouds requires a good metric to measure the discrepancy between two 3D point sets, which is non-trivial due to their irregularity. Most of the previous works resort to using the Chamfer discrepancy or Earth Mover’s distance, but those metrics are either ineffective in measuring the differences between point clouds or computationally expensive. In this paper, we conduct a systematic study with extensive experiments on distance metrics for 3D point clouds. From this study, we propose to use sliced Wasserstein distance and its variants for learning representations of 3D point clouds. In addition, we introduce a new algorithm to estimate sliced Wasserstein distance that guarantees that the estimated value is close enough to the true one. Experiments show that the sliced Wasserstein distance and its variants allow the neural network to learn a more efficient representation compared to the Chamfer discrepancy. We demonstrate the efficiency of the sliced Wasserstein metric and its variants on several tasks in 3D computer vision including training a point cloud autoencoder, generative modeling, transfer learning, and point cloud registration.},
	urldate = {2022-03-15},
	booktitle = {2021 {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	publisher = {IEEE},
	author = {Nguyen, Trung and Pham, Quang-Hieu and Le, Tam and Pham, Tung and Ho, Nhat and Hua, Binh-Son},
	month = oct,
	year = {2021},
	pages = {10458--10467},
}

@inproceedings{wessel1987control,
	address = {San Francisco, CA},
	title = {Control of phrasing and articulation in synthesis},
	booktitle = {Proceedings of the 1987 international computer music conference},
	author = {Wessel, David},
	year = {1987},
	keywords = {⛔ No DOI found},
	pages = {108--116},
}

@article{saitis_brightness_2020,
	title = {Brightness perception for musical instrument sounds: {Relation} to timbre dissimilarity and source-cause categories},
	volume = {148},
	issn = {0001-4966},
	shorttitle = {Brightness perception for musical instrument sounds},
	url = {http://asa.scitation.org/doi/10.1121/10.0002275},
	doi = {10.1121/10.0002275},
	number = {4},
	urldate = {2022-03-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Saitis, Charalampos and Siedenburg, Kai},
	month = oct,
	year = {2020},
	pages = {2256--2266},
}

@article{giordano_sound_2010,
	title = {Sound source mechanics and musical timbre perception: {Evidence} from previous studies},
	volume = {28},
	issn = {0730-7829, 1533-8312},
	shorttitle = {Sound {Source} {Mechanics} and {Musical} {Timbre} {Perception}},
	url = {https://online.ucpress.edu/mp/article/28/2/155/62475/Sound-Source-Mechanics-and-Musical-Timbre},
	doi = {10.1525/mp.2010.28.2.155},
	abstract = {Timbre has been conceived of as a multidimensional sensory attribute and as a carrier of perceptually useful information about the mechanics of the sound source. To date, research on musical timbre has focused on defining its acoustical correlates, whereas fragmentary evidence is available on the influence of mechanical parameters. We quantified the extent to which mechanical properties of the sound source are associated with structures in the data from published identification and dissimilarity-rating studies. We focus on two macroscopic mechanical properties: the musical instrument family and excitation type. Identification confusions are significantly more frequent for same-family instruments. With dissimilarity ratings, same-family or same-excitation tones are judged more similar and tend to occupy the same region of multidimensional-scaling spaces. As such, significant associations between the perception of musical timbre and the mechanics of the sound source emerge even when not explicitly demanded by the task.},
	number = {2},
	urldate = {2022-03-15},
	journal = {Music Perception},
	author = {Giordano, Bruno L. and McAdams, Stephen},
	month = dec,
	year = {2010},
	pages = {155--168},
}

@book{mcadams_thinking_1993,
	title = {Thinking in {Sound}: {The} {Cognitive} {Psychology} of {Human} {Audition}},
	isbn = {978-0-19-852257-7},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198522577.001.0001/acprof-9780198522577},
	urldate = {2022-03-15},
	publisher = {Oxford University Press},
	author = {McAdams, Stephen and Bigand, Emmanuel},
	month = apr,
	year = {1993},
	doi = {10.1093/acprof:oso/9780198522577.001.0001},
}

@article{morfi_deep_2021,
	title = {Deep perceptual embeddings for unlabelled animal sound events},
	volume = {150},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/10.0005475},
	doi = {10.1121/10.0005475},
	number = {1},
	urldate = {2022-03-16},
	journal = {The Journal of the Acoustical Society of America},
	author = {Morfi, Veronica and Lachlan, Robert F. and Stowell, Dan},
	month = jul,
	year = {2021},
	pages = {2--11},
}

@inproceedings{ashley1986knowledge,
	address = {The Hague, The Netherlands},
	title = {A knowledge-based approach to assistance in timbral design},
	booktitle = {Proceedings of the 1986 international computer music conference},
	author = {Ashley, Richard},
	month = oct,
	year = {1986},
	pages = {11--16},
}

@article{mcadams_psychomechanics_2004,
	title = {The psychomechanics of simulated sound sources: {Material} properties of impacted bars},
	volume = {115},
	issn = {0001-4966},
	shorttitle = {The psychomechanics of simulated sound sources},
	url = {http://asa.scitation.org/doi/10.1121/1.1645855},
	doi = {10.1121/1.1645855},
	number = {3},
	urldate = {2022-03-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {McAdams, Stephen and Chaigne, Antoine and Roussarie, Vincent},
	month = mar,
	year = {2004},
	pages = {1306--1320},
}

@article{siedenburg_acoustic_2016,
	title = {Acoustic and categorical dissimilarity of musical timbre: {Evidence} from asymmetries between acoustic and chimeric sounds},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Acoustic and {Categorical} {Dissimilarity} of {Musical} {Timbre}},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01977/abstract},
	doi = {10.3389/fpsyg.2015.01977},
	urldate = {2022-03-15},
	journal = {Frontiers in Psychology},
	author = {Siedenburg, Kai and Jones-Mollerup, Kiray and McAdams, Stephen},
	month = jan,
	year = {2016},
}

@inproceedings{li_novel_2013,
	address = {Sydney, Australia},
	title = {A novel earth mover's distance methodology for image matching with gaussian mixture models},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751320/},
	doi = {10.1109/ICCV.2013.212},
	urldate = {2022-03-22},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Li, Peihua and Wang, Qilong and Zhang, Lei},
	month = dec,
	year = {2013},
	pages = {1689--1696},
}

@inproceedings{logan_music_2001,
	address = {Tokyo, Japan},
	title = {A music similarity function based on signal analysis},
	isbn = {978-0-7695-1198-6},
	url = {http://ieeexplore.ieee.org/document/1237829/},
	doi = {10.1109/ICME.2001.1237829},
	urldate = {2022-03-23},
	booktitle = {{IEEE} international conference on multimedia and expo, 2001. {ICME} 2001.},
	publisher = {IEEE},
	author = {Logan, B. and Salomon, A.},
	year = {2001},
	pages = {745--748},
}

@article{van_niekerk_exploration_2022,
	title = {Exploration strategies for articulatory synthesis of complex syllable onsets},
	url = {http://arxiv.org/abs/2204.09381},
	abstract = {High-quality articulatory speech synthesis has many potential applications in speech science and technology. However, developing appropriate mappings from linguistic specification to articulatory gestures is difficult and time consuming. In this paper we construct an optimisation-based framework as a first step towards learning these mappings without manual intervention. We demonstrate the production of syllables with complex onsets and discuss the quality of the articulatory gestures with reference to coarticulation.},
	urldate = {2022-04-21},
	journal = {arXiv:2204.09381 [cs, eess]},
	author = {van Niekerk, Daniel R. and Xu, Anqi and Gerazov, Branislav and Krug, Paul K. and Birkholz, Peter and Xu, Yi},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{liu_simple_2022,
	title = {Simple and {Effective} {Unsupervised} {Speech} {Synthesis}},
	url = {http://arxiv.org/abs/2204.02524},
	abstract = {We introduce the first unsupervised speech synthesis system based on a simple, yet effective recipe. The framework leverages recent work in unsupervised speech recognition as well as existing neural-based speech synthesis. Using only unlabeled speech audio and unlabeled text as well as a lexicon, our method enables speech synthesis without the need for a human-labeled corpus. Experiments demonstrate the unsupervised system can synthesize speech similar to a supervised counterpart in terms of naturalness and intelligibility measured by human evaluation.},
	urldate = {2022-04-21},
	journal = {arXiv:2204.02524 [cs, eess]},
	author = {Liu, Alexander H. and Lai, Cheng-I. Jeff and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei and Glass, James},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{zeghidour_soundstream_2022,
	title = {{SoundStream}: {An} {End}-to-{End} {Neural} {Audio} {Codec}},
	volume = {30},
	issn = {2329-9304},
	shorttitle = {{SoundStream}},
	doi = {10.1109/TASLP.2021.3129994},
	abstract = {We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, SoundStream at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco},
	year = {2022},
	keywords = {Audio compression, Bit rate, Codecs, Computational modeling, Convolutional codes, Decoding, Psychoacoustic models, Speech coding, codecs, convolution, neural networks, speech enhancement},
	pages = {495--507},
}

@article{grinewitschus_harmonic_2022,
	title = {The harmonic shift algorithm for efficient multi-{Pitch} detection},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2021.3129344},
	abstract = {Existing approaches for multi-pitch detection have still been highly complex. In this paper, the harmonic shift algorithm (HSA) for multi-pitch detection is illustrated. The HSA is a noncoherent blind detection scheme, exploiting the constant-Q transform (CQT). The only requirement for the successful application of the HSA is that all the sources are harmonic sources. Due to this simplicity, the HSA is considered viable for both single-pitch and multi-pitch detection, making its attractive for the application in other areas of signal processing. When illustrating the performance capabilities of the HSA, we demonstrate that it outperforms the state of the art regarding the computational complexity providing detection precision, accuracy and a recall factor comparable with existing schemes.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Grinewitschus, Lukas and Jung, Peter},
	year = {2022},
	keywords = {Audio signal processing, Computational complexity, Convolutional neural networks, Frequency estimation, Harmonic analysis, Hidden Markov models, Signal processing algorithms, Transforms, blind detection, constant-Q transform, harmonic shift algorithm, multi-pitch detection, noncoherent detection},
	pages = {548--561},
}

@article{wang_neuraldps_2022,
	title = {{NeuralDPS}: {Neural} deterministic plus stochastic model with multiband excitation for noise-{Controllable} waveform generation},
	volume = {30},
	issn = {2329-9304},
	shorttitle = {{NeuralDPS}},
	doi = {10.1109/TASLP.2022.3140480},
	abstract = {The traditional vocoders have the advantages of high synthesis efficiency, strong interpretability, and speech editability, while the neural vocoders have the advantage of high synthesis quality. To combine the advantages of two vocoders, inspired by the traditional deterministic plus stochastic model, this paper proposes a novel neural vocoder named NeuralDPS which can retain high speech quality and acquire high synthesis efficiency and noise controllability. Firstly, this framework contains four modules: a deterministic source module, a stochastic source module, a neural V/UV decision module and a neural filter module. The input required by the vocoder is just the spectral parameter, which avoids the error caused by estimating additional parameters, such as F0. Secondly, to solve the problem that different frequency bands may have different proportions of deterministic components and stochastic components, a multiband excitation strategy is used to generate a more accurate excitation signal and reduce the neural filter’s burden. Thirdly, a method to control noise components of speech is proposed. In this way, the signal-to-noise ratio (SNR) of speech can be adjusted easily. Objective and subjective experimental results show that our proposed NeuralDPS vocoder can obtain similar performance with the WaveNet and it generates waveforms at least 280 times faster than the WaveNet vocoder. It is also 28\% faster than WaveGAN’s synthesis efficiency on a single CPU core. We have also verified through experiments that this method can effectively control the noise components in the predicted speech and adjust the SNR of speech.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Tao and Fu, Ruibo and Yi, Jiangyan and Tao, Jianhua and Wen, Zhengqi},
	year = {2022},
	keywords = {Acoustics, Neural networks, Signal to noise ratio, Speech enhancement, Speech processing, Stochastic processes, Vocoder, Vocoders, deterministic plus stochastic, multiband excitation, noise control, speech synthesis},
	pages = {865--878},
}

@article{lee_non-autoregressive_2022,
	title = {Non-autoregressive fully parallel deep convolutional neural speech synthesis},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3156797},
	abstract = {Deep learning-based speech synthesis evolves by employing a sequence-to-sequence (seq2seq) structure with an attention mechanism. The seq2seq speech synthesis model consists of a pair of the encoder for delivering the linguistic features and the decoder for predicting the mel-spectrogram, and learns the alignment between text and speech through the attention mechanism. The decoder predicts the mel-spectrogram by an autoregressive flow that considers the current input and what they have learned from previous inputs. This is beneficial when processing the sequential data, as in speech synthesis. However, the recursive generation of speech typically requires extensive training time, which slows the speed of synthesis. To overcome these obstacles, we propose a non-autoregressive framework for fully parallel deep convolutional neural speech synthesis. Firstly, we design a new synthesis paradigm that integrates a time-varying metatemplate (TVMT), whose length is modeled with a separate conditional distribution, to prepare the decoder input. The decoding step converts the TVMT into spectral features, which eliminates the autoregressive flow. Secondly, we propose a structure that uses multiple decoders interconnected by up-down chains with an iterative attention mechanism. The decoder chains distribute the burden of decoding, progressively infusing the information obtained from the training target example into the chains to refine the predicted spectral features at each decoding step. For each decoder, the attention mechanism is repeatedly applied to produce the elaborated alignment between the linguistic features and the TVMT, which is gradually transformed into the spectral features. The proposed architecture substantially improves the synthesis speed, and the resulting speech quality is superior to that of a conventional autoregressive model.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Lee, Moa and Lee, Junmo and Chang, Joon-Hyuk},
	year = {2022},
	keywords = {Data models, Decoding, Iterative decoding, Linguistics, Spectrogram, Speech synthesis, Training, attention-based end-to-end speech synthesis, deep learning, text-to-speech},
	pages = {1150--1159},
}

@misc{noauthor_220407064_nodate,
	title = {[2204.07064] streamable neural audio synthesis with non-{Causal} convolutions},
	url = {https://arxiv.org/abs/2204.07064},
	urldate = {2022-04-21},
}

@article{pepe_deep_2022,
	title = {Deep optimization of parametric {IIR} filters for audio equalization},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3155289},
	abstract = {This paper describes a novel Deep Learning method for the design of IIR parametric filters for automatic multipoint audio equalization, that is the task of improving the sound quality of a listening environment at multiple listening points employing multiple loudspeakers. The filters are designed to approximate the inverse of the RIR and achieve almost flat magnitude response. A simple and effective neural architecture, named BiasNet, is proposed to determine the IIR equalizer parameters. This novel architecture is conceived for optimization and, as such, is able to produce optimal IIR equalizer parameters at its output, after training, with no input required. In absence of input, the presence of learnable non-zero bias terms ensures that the network works properly. An output scaling method is used to obtain accurate tuning of the IIR filters center frequency, quality factor and gain. All layers involved in the proposed method are shown to be differentiable, allowing backpropagation to optimize the network weights and achieve, after a number of training iterations, the optimal output according to a given RIR. The parameters are optimized with respect to a loss function based on a spectral distance between the measured and desired magnitude response, and a regularization term is used to keep the same microphone-loudspeaker energy balance after equalization. Two experimental scenarios are employed, a room and a car cabin, with several loudspeakers. The performance of the proposed method improves over the baseline techniques and achieves an almost flat band at a lower computational cost.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Pepe, Giovanni and Gabrielli, Leonardo and Squartini, Stefano and Tripodi, Carlo and Strozzi, Nicolò},
	year = {2022},
	keywords = {Deep learning, Equalizers, Finite impulse response filters, IIR filtering, MIMO communication, Neural networks, Optimization, Signal processing algorithms, Speech processing, audio equalization, inverse filtering, machine learning, optimization, parametric equalizer},
	pages = {1136--1149},
}

@article{caillon_streamable_2022,
	title = {Streamable neural audio synthesis with non-{Causal} convolutions},
	url = {http://arxiv.org/abs/2204.07064},
	abstract = {Deep learning models are mostly used in an offline inference fashion. However, this strongly limits the use of these models inside audio generation setups, as most creative workflows are based on real-time digital signal processing. Although approaches based on recurrent networks can be naturally adapted to this buffer-based computation, the use of convolutions still poses some serious challenges. To tackle this issue, the use of causal streaming convolutions have been proposed. However, this requires specific complexified training and can impact the resulting audio quality. In this paper, we introduce a new method allowing to produce non-causal streaming models. This allows to make any convolutional model compatible with real-time buffer-based processing. As our method is based on a post-training reconfiguration of the model, we show that it is able to transform models trained without causal constraints into a streaming model. We show how our method can be adapted to fit complex architectures with parallel branches. To evaluate our method, we apply it on the recent RAVE model, which provides high-quality real-time audio synthesis. We test our approach on multiple music and speech datasets and show that it is faster than overlap-add methods, while having no impact on the generation quality. Finally, we introduce two open-source implementation of our work as Max/MSP and PureData externals, and as a VST audio plugin. This allows to endow traditional digital audio workstation with real-time neural audio synthesis on a laptop CPU.},
	urldate = {2022-04-21},
	journal = {arXiv:2204.07064 [cs, eess, stat]},
	author = {Caillon, Antoine and Esling, Philippe},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{wang_stft-domain_2022,
	title = {{STFT}-{Domain} neural speech enhancement with very low algorithmic latency},
	url = {http://arxiv.org/abs/2204.09911},
	abstract = {Deep learning based speech enhancement in the short-term Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window contains more samples and the frequency resolution can be higher for potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed based on the same 32 ms window size. To reduce this inherent latency, we adapt a conventional dual window size approach, where a regular input window size is used for STFT but a shorter output window is used for the overlap-add in the iSTFT, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT and iSTFT configuration, we employ single- or multi-microphone complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the RI components predicted by the DNN to conduct frame-online beamforming, the results of which are then used as extra features for a second DNN to perform frame-online post-filtering. The frequency-domain beamforming in between the two DNNs can be easily integrated with complex spectral mapping and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation results on a noisy-reverberant speech enhancement task demonstrate the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.},
	urldate = {2022-04-25},
	journal = {arXiv:2204.09911 [cs, eess]},
	author = {Wang, Zhong-Qiu and Wichern, Gordon and Watanabe, Shinji and Roux, Jonathan Le},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@article{limoyo_learning_2022,
	title = {Learning sequential latent variable models from multimodal time series data},
	url = {http://arxiv.org/abs/2204.10419},
	abstract = {Sequential modelling of high-dimensional data is an important problem that appears in many domains including model-based reinforcement learning and dynamics identification for control. Latent variable models applied to sequential data (i.e., latent dynamics models) have been shown to be a particularly effective probabilistic approach to solve this problem, especially when dealing with images. However, in many application areas (e.g., robotics), information from multiple sensing modalities is available – existing latent dynamics methods have not yet been extended to effectively make use of such multimodal sequential data. Multimodal sensor streams can be correlated in a useful manner and often contain complementary information across modalities. In this work, we present a self-supervised generative modelling framework to jointly learn a probabilistic latent state representation of multimodal data and the respective dynamics. Using synthetic and real-world datasets from a multimodal robotic planar pushing task, we demonstrate that our approach leads to significant improvements in prediction and representation quality. Furthermore, we compare to the common learning baseline of concatenating each modality in the latent space and show that our principled probabilistic formulation performs better. Finally, despite being fully self-supervised, we demonstrate that our method is nearly as effective as an existing supervised approach that relies on ground truth labels.},
	urldate = {2022-04-25},
	journal = {arXiv:2204.10419 [cs]},
	author = {Limoyo, Oliver and Ablett, Trevor and Kelly, Jonathan},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No DOI found},
}

@article{dattorro_part_nodate,
	title = {Part 1: {Reverberator} and {Other} {Filters}},
	author = {Dattorro, Jon},
	keywords = {⛔ No DOI found},
	pages = {25},
}

@misc{turetzky_deep_2022,
	title = {Deep {Audio} {Waveform} {Prior}},
	url = {http://arxiv.org/abs/2207.10441},
	abstract = {Convolutional neural networks contain strong priors for generating natural looking images [1]. These priors enable image denoising, super resolution, and inpainting in an unsupervised manner. Previous attempts to demonstrate similar ideas in audio, namely deep audio priors, (i) use hand picked architectures such as harmonic convolutions, (ii) only work with spectrogram input, and (iii) have been used mostly for eliminating Gaussian noise [2]. In this work we show that existing SOTA architectures for audio source separation contain deep priors even when working with the raw waveform. Deep priors can be discovered by training a neural network to generate a single corrupted signal when given white noise as input. A network with relevant deep priors is likely to generate a cleaner version of the signal before converging on the corrupted signal. We demonstrate this restoration effect with several corruptions: background noise, reverberations, and a gap in the signal (audio inpainting).},
	urldate = {2022-07-23},
	publisher = {arXiv},
	author = {Turetzky, Arnon and Michelson, Tzvi and Adi, Yossi and Peleg, Shmuel},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2207.10441},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{yang_diffsound_2022,
	title = {Diffsound: {Discrete} {Diffusion} {Model} for {Text}-to-sound {Generation}},
	shorttitle = {Diffsound},
	url = {http://arxiv.org/abs/2207.09983},
	abstract = {Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 {\textbackslash}textit\{v.s\} 2.786, and the generation speed is five times faster than the AR decoder.},
	urldate = {2022-07-23},
	publisher = {arXiv},
	author = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2207.09983},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{noauthor_solidex_nodate,
	title = {Solidex - {A} yield optimizer for {Solidly} built on {Fantom}},
	url = {https://solidexfinance.com/#/home},
	abstract = {Solidex - A yield optimizer for Solidly built on Fantom},
	urldate = {2022-06-24},
}

@article{hayes_disembodied_2022,
	title = {Disembodied timbres: {A} study on semantically prompted {FM} synthesis},
	volume = {70},
	issn = {15494950},
	shorttitle = {Disembodied {Timbres}},
	url = {https://www.aes.org/e-lib/browse.cfm?elib=21740},
	doi = {10.17743/jaes.2022.0006},
	number = {5},
	urldate = {2022-07-21},
	journal = {Journal of the Audio Engineering Society},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = may,
	year = {2022},
	note = {tex.copyright: All rights reserved},
	pages = {373--391},
}

@misc{nistal_drumgan_2022,
	title = {{DrumGAN} {VST}: {A} plugin for drum sound analysis/{Synthesis} with autoencoding generative adversarial networks},
	shorttitle = {{DrumGAN} {VST}},
	url = {http://arxiv.org/abs/2206.14723},
	abstract = {In contemporary popular music production, drum sound design is commonly performed by cumbersome browsing and processing of pre-recorded samples in sound libraries. One can also use specialized synthesis hardware, typically controlled through low-level, musically meaningless parameters. Today, the field of Deep Learning offers methods to control the synthesis process via learned high-level features and allows generating a wide variety of sounds. In this paper, we present DrumGAN VST, a plugin for synthesizing drum sounds using a Generative Adversarial Network. DrumGAN VST operates on 44.1 kHz sample-rate audio, offers independent and continuous instrument class controls, and features an encoding neural network that maps sounds into the GAN's latent space, enabling resynthesis and manipulation of pre-existing drum sounds. We provide numerous sound examples and a demo of the proposed VST plugin.},
	urldate = {2022-07-23},
	publisher = {arXiv},
	author = {Nistal, Javier and Aouameur, Cyran and Velarde, Ithan and Lattner, Stefan},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.14723},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{anwyl-irvine_gorilla_2020,
	title = {Gorilla in our midst: {An} online behavioral experiment builder},
	volume = {52},
	issn = {1554-3528},
	shorttitle = {Gorilla in our midst},
	url = {http://link.springer.com/10.3758/s13428-019-01237-x},
	doi = {10.3758/s13428-019-01237-x},
	number = {1},
	urldate = {2022-08-02},
	journal = {Behavior Research Methods},
	author = {Anwyl-Irvine, Alexander L. and Massonnié, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
	month = feb,
	year = {2020},
	pages = {388--407},
}

@article{casler_separate_2013,
	title = {Separate but equal? {A} comparison of participants and data gathered via {Amazon}’s {MTurk}, social media, and face-to-face behavioral testing},
	volume = {29},
	issn = {07475632},
	shorttitle = {Separate but equal?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S074756321300160X},
	doi = {10.1016/j.chb.2013.05.009},
	number = {6},
	urldate = {2022-08-02},
	journal = {Computers in Human Behavior},
	author = {Casler, Krista and Bickel, Lydia and Hackett, Elizabeth},
	month = nov,
	year = {2013},
	pages = {2156--2160},
}

@article{lumsden_attrition_2017,
	title = {Attrition from web-{Based} cognitive testing: {A} repeated measures comparison of gamification techniques},
	volume = {19},
	issn = {1438-8871},
	shorttitle = {Attrition from {Web}-{Based} {Cognitive} {Testing}},
	url = {http://www.jmir.org/2017/11/e395/},
	doi = {10.2196/jmir.8473},
	number = {11},
	urldate = {2022-08-02},
	journal = {Journal of Medical Internet Research},
	author = {Lumsden, Jim and Skinner, Andy and Coyle, David and Lawrence, Natalia and Munafo, Marcus},
	month = nov,
	year = {2017},
	pages = {e395},
}

@article{von_ahn_designing_2008,
	title = {Designing games with a purpose},
	volume = {51},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1378704.1378719},
	doi = {10.1145/1378704.1378719},
	abstract = {Data generated as a side effect of game play also solves computational problems and trains AI algorithms.},
	number = {8},
	urldate = {2022-08-02},
	journal = {Communications of the ACM},
	author = {von Ahn, Luis and Dabbish, Laura},
	month = aug,
	year = {2008},
	pages = {58--67},
}

@article{morschheuser_gamified_2017,
	title = {Gamified crowdsourcing: {Conceptualization}, literature review, and future agenda},
	volume = {106},
	issn = {10715819},
	shorttitle = {Gamified crowdsourcing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581917300642},
	doi = {10.1016/j.ijhcs.2017.04.005},
	urldate = {2022-08-02},
	journal = {International Journal of Human-Computer Studies},
	author = {Morschheuser, Benedikt and Hamari, Juho and Koivisto, Jonna and Maedche, Alexander},
	month = oct,
	year = {2017},
	pages = {26--43},
}

@techreport{moskalev_contrasting_2022,
	title = {Contrasting quadratic assignments for set-based representation learning},
	url = {http://arxiv.org/abs/2205.15814},
	abstract = {The standard approach to contrastive learning is to maximize the agreement between different views of the data. The views are ordered in pairs, such that they are either positive, encoding different views of the same object, or negative, corresponding to views of different objects. The supervisory signal comes from maximizing the total similarity over positive pairs, while the negative pairs are needed to avoid collapse. In this work, we note that the approach of considering individual pairs cannot account for both intra-set and inter-set similarities when the sets are formed from the views of the data. It thus limits the information content of the supervisory signal available to train representations. We propose to go beyond contrasting individual pairs of objects by focusing on contrasting objects as sets. For this, we use combinatorial quadratic assignment theory designed to evaluate set and graph similarities and derive set-contrastive objective as a regularizer for contrastive learning methods. We conduct experiments and demonstrate that our method improves learned representations for the tasks of metric learning and self-supervised classification.},
	urldate = {2022-07-25},
	author = {Moskalev, Artem and Sosnovik, Ivan and Fischer, Volker and Smeulders, Arnold},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{adjerid_big_2018,
	title = {Big data in psychology: {A} framework for research advancement.},
	volume = {73},
	issn = {1935-990X, 0003-066X},
	shorttitle = {Big data in psychology},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000190},
	doi = {10.1037/amp0000190},
	number = {7},
	urldate = {2022-08-02},
	journal = {American Psychologist},
	author = {Adjerid, Idris and Kelley, Ken},
	month = oct,
	year = {2018},
	pages = {899--917},
}

@inproceedings{deterding_gamifying_2015,
	address = {Seoul Republic of Korea},
	title = {Gamifying research: {Strategies}, opportunities, challenges, ethics},
	isbn = {978-1-4503-3146-3},
	shorttitle = {Gamifying {Research}},
	url = {https://dl.acm.org/doi/10.1145/2702613.2702646},
	doi = {10.1145/2702613.2702646},
	urldate = {2022-08-02},
	booktitle = {Proceedings of the 33rd annual {ACM} conference extended abstracts on human factors in computing systems},
	publisher = {ACM},
	author = {Deterding, Sebastian and Canossa, Alessandro and Harteveld, Casper and Cooper, Seth and Nacke, Lennart E. and Whitson, Jennifer R.},
	month = apr,
	year = {2015},
	pages = {2421--2424},
}

@article{keusch_review_2017,
	title = {A {Review} of {Issues} in {Gamified} {Surveys}},
	volume = {35},
	issn = {0894-4393, 1552-8286},
	url = {http://journals.sagepub.com/doi/10.1177/0894439315608451},
	doi = {10.1177/0894439315608451},
	abstract = {The term “gamification” is used for a wide variety of techniques aimed to increase respondent engagement while filling out web surveys. Suggested approaches range from rephrasing questions to sound more game-like to embedding the entire survey into a game where respondent avatars adventure through a fantasy land while answering survey questions. So far, only a few experimental studies regarding the influence of gamification on survey responding have been published in peer-reviewed journals. This article systematically reviews the current research literature on gamification in surveys by first answering the question as to what gamification entails in the context of web surveys by identifying relevant gamification elements. Next, the article discusses how these elements could influence survey data quality using the Total Survey Error framework. Finally, a systematic review of empirical evidence on gamified surveys from published and unpublished studies is provided. While most studies found in our literature review reported a positive effect of using game elements, such as challenges, story/narrative, rewards, goals/objectives, and badges, on psychological outcomes among survey respondents (e.g., fun, interest, satisfaction), the influence of these elements on behavioral outcomes (e.g., completion and break-offs, item omission, satisficing, responses) and, therefore, survey quality is more unclear. This article informs survey researchers and practitioners of the current state of research on survey gamification and identifies potential areas for future research.},
	number = {2},
	urldate = {2022-08-02},
	journal = {Social Science Computer Review},
	author = {Keusch, Florian and Zhang, Chan},
	month = apr,
	year = {2017},
	pages = {147--166},
}

@article{estelles-arolas_towards_2012,
	title = {Towards an integrated crowdsourcing definition},
	volume = {38},
	issn = {0165-5515, 1741-6485},
	url = {http://journals.sagepub.com/doi/10.1177/0165551512437638},
	doi = {10.1177/0165551512437638},
	abstract = {‘Crowdsourcing’ is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of internet-based collaborative activity, such as co-creation or user innovation. Varying definitions of crowdsourcing exist, and therefore some authors present certain specific examples of crowdsourcing as paradigmatic, while others present the same examples as the opposite. In this article, existing definitions of crowdsourcing are analysed to extract common elements and to establish the basic characteristics of any crowdsourcing initiative. Based on these existing definitions, an exhaustive and consistent definition for crowdsourcing is presented and contrasted in 11 cases.},
	number = {2},
	urldate = {2022-08-02},
	journal = {Journal of Information Science},
	author = {Estellés-Arolas, Enrique and González-Ladrón-de-Guevara, Fernando},
	month = apr,
	year = {2012},
	pages = {189--200},
}

@book{helmholtz_sensations_1954,
	address = {Mineola, NY},
	edition = {2},
	title = {On the sensations of tone as a physiological basis for the theory of music},
	publisher = {Dover Publications},
	author = {Helmholtz, Hermann L. F.},
	year = {1954},
}

@article{elliott_modulation_2009,
	title = {The {Modulation} {Transfer} {Function} for {Speech} {Intelligibility}},
	volume = {5},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1000302},
	doi = {10.1371/journal.pcbi.1000302},
	number = {3},
	urldate = {2022-08-04},
	journal = {PLoS Computational Biology},
	author = {Elliott, Taffeta M. and Theunissen, Frédéric E.},
	editor = {Friston, Karl J.},
	month = mar,
	year = {2009},
	pages = {e1000302},
}

@book{schaeffer_treatise_2017,
	address = {Oakland, California},
	series = {California studies in 20th-century music},
	title = {Treatise on musical objects: essays across disciplines},
	isbn = {978-0-520-29429-5 978-0-520-29430-1},
	shorttitle = {Treatise on musical objects},
	abstract = {"The Treatise on musical objects by Pierre Schaeffer is regarded as his most important work on music and its relationship with technology. Schaeffer refers to his earlier research in musique concrète and expands this to suggest a methodology of working with sounds resulting from the recording process. Drawing on acoustics, physics, and physiology, but also philosophy and the relationship between subject and object, Schaeffer's book summarizes his theoretical and practical work in music composition. North and Dack present an important book in the history of ideas in Europe that will resonate far beyond electroacoustic music."–Provided by publisher},
	publisher = {University of California Press},
	author = {Schaeffer, Pierre and North, Christine and Dack, John},
	year = {2017},
	note = {Issue: 20},
	keywords = {Music, Philosophy and aesthetics},
}

@article{smalley_spectromorphology_1997,
	title = {Spectromorphology: explaining sound-shapes},
	volume = {2},
	issn = {13557718},
	shorttitle = {Spectromorphology},
	url = {http://www.journals.cambridge.org/abstract%5FS1355771897009059},
	doi = {10.1017/S1355771897009059},
	number = {2},
	urldate = {2022-08-04},
	journal = {Organised Sound},
	author = {Smalley, Denis},
	month = aug,
	year = {1997},
	pages = {107--126},
}

@inproceedings{cartwright_social-eq_2013,
	title = {Social-eq: {Crowdsourcing} an equalization descriptor map.},
	booktitle = {Proceedings of the 14th international society for music information retrieval conference},
	author = {Cartwright, Mark Brozier and Pardo, Bryan},
	month = jan,
	year = {2013},
	pages = {395--400},
}

@inproceedings{gounaropoulos_synthesising_2006,
	address = {Berlin, Heidelberg},
	title = {Synthesising timbres and timbre-changes from {Adjectives}/{Adverbs}},
	doi = {10.1007/11732242_63},
	booktitle = {Applications of evolutionary computing. {EvoWorkshops} 2006},
	author = {Gounaropoulos, Alex and Johnson, Colin},
	month = apr,
	year = {2006},
	pages = {664--675},
}

@inproceedings{stables_semantic_2016,
	title = {Semantic description of timbral transformations in music production},
	doi = {10.1145/2964284.2967238},
	booktitle = {{ACM} multimedia, oct. 15-19, amsterdam, netherlands},
	author = {Stables, R and De Man, B and Enderby, S and Reiss, JD and Wilmering, T. and Fazekas, G.},
	month = oct,
	year = {2016},
	pages = {337--341},
}

@inproceedings{stables_safe_2014,
	address = {Taipei, Taiwan},
	title = {{SAFE}: {A} system for extraction and retrieval of semantic audio descriptors},
	booktitle = {Proceedings of the 15th international society for music information retrieval conference},
	author = {Stables, Ryan and Enderby, Sean and De Man, Brecht and Fazekas, György and Reiss, Joshua D},
	month = oct,
	year = {2014},
}

@article{gomez_accelerated_2008,
	title = {Accelerated neural evolution through cooperatively coevolved synapses},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/gomez08a.html},
	abstract = {Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artificial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difficult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be significantly more efficient and powerful than the other methods on these tasks.},
	number = {31},
	urldate = {2022-08-19},
	journal = {Journal of Machine Learning Research},
	author = {Gomez, Faustino and Schmidhuber, Jürgen and Miikkulainen, Risto},
	year = {2008},
	keywords = {⛔ No DOI found},
	pages = {937--965},
}

@misc{huang_addressing_2019,
	title = {Addressing the loss-{Metric} mismatch with adaptive loss alignment},
	url = {http://arxiv.org/abs/1905.05895},
	abstract = {In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Huang, Chen and Zhai, Shuangfei and Talbott, Walter and Bautista, Miguel Angel and Sun, Shih-Yu and Guestrin, Carlos and Susskind, Josh},
	month = may,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{whitelam_correspondence_2021,
	title = {Correspondence between neuroevolution and gradient descent},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26568-2},
	doi = {10.1038/s41467-021-26568-2},
	abstract = {We show analytically that training a neural network by conditioned stochastic mutation or neuroevolution of its weights is equivalent, in the limit of small mutations, to gradient descent on the loss function in the presence of Gaussian white noise. Averaged over independent realizations of the learning process, neuroevolution is equivalent to gradient descent on the loss function. We use numerical simulation to show that this correspondence can be observed for finite mutations, for shallow and deep neural networks. Our results provide a connection between two families of neural-network training methods that are usually considered to be fundamentally different.},
	number = {1},
	urldate = {2022-08-19},
	journal = {Nature Communications},
	author = {Whitelam, Stephen and Selin, Viktor and Park, Sang-Won and Tamblyn, Isaac},
	month = nov,
	year = {2021},
	note = {tex.copyright: 2021 The Author(s)},
	keywords = {Applied mathematics, Statistical physics},
	pages = {6317},
}

@article{galvan_neuroevolution_2021,
	title = {Neuroevolution in deep neural networks: {Current} trends and future challenges},
	volume = {2},
	issn = {2691-4581},
	shorttitle = {Neuroevolution in {Deep} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9383028/},
	doi = {10.1109/TAI.2021.3067574},
	number = {6},
	urldate = {2022-08-19},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Galvan, Edgar and Mooney, Peter},
	month = dec,
	year = {2021},
	pages = {476--493},
}

@incollection{miikkulainen_neuroevolution_2014,
	address = {Boston, MA},
	title = {Neuroevolution},
	isbn = {978-1-4899-7502-7},
	url = {https://doi.org/10.1007/978-1-4899-7502-7%5F594-1},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	publisher = {Springer US},
	author = {Miikkulainen, Risto},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2014},
	doi = {10.1007/978-1-4899-7502-7_594-1},
	keywords = {Baldwin Effect, Full Network, Hebbian Learning, Partially Observable Markov Decision Process, Reinforcement Learning Method},
	pages = {1--7},
}

@misc{wu_learning_2018,
	title = {Learning to {Teach} with {Dynamic} {Loss} {Functions}},
	url = {http://arxiv.org/abs/1810.12081},
	abstract = {Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as "learning to teach with dynamic loss functions" (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Wu, Lijun and Tian, Fei and Xia, Yingce and Fan, Yang and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gao_searching_2021,
	title = {Searching for robustness: {Loss} learning for noisy classification tasks},
	shorttitle = {Searching for {Robustness}},
	url = {http://arxiv.org/abs/2103.00243},
	abstract = {We present a “learning to learn” approach for automatically constructing white-box classiﬁcation loss functions that are robust to label noise in the training data. We parameterize a ﬂexible family of loss functions using Taylor polynomials, and apply evolutionary strategies to search for noise-robust losses in this space. To learn re-usable loss functions that can apply to new tasks, our ﬁtness function scores their performance in aggregate across a range of training dataset and architecture combinations. The resulting white-box loss provides a simple and fast “plug-andplay” module that enables effective noise-robust learning in diverse downstream tasks, without requiring a special training procedure or network architecture. The efﬁcacy of our method is demonstrated on a variety of datasets with both synthetic and real label noise, where we compare favorably to previous work.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Gao, Boyan and Gouk, Henry and Hospedales, Timothy M.},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_feature-critic_2019,
	title = {Feature-{Critic} {Networks} for {Heterogeneous} {Domain} {Generalization}},
	url = {http://arxiv.org/abs/1901.11448},
	abstract = {The well known domain shift issue causes model performance to degrade when deployed to a new target domain with different statistics to training. Domain adaptation techniques alleviate this, but need some instances from the target domain to drive adaptation. Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box, and various approaches aim to train a domain-invariant feature extractor, typically by adding some manually designed losses. In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label space with the seen ones, and the goal is to train a feature representation that is useful off-the-shelf for novel data and novel categories. Experimental evaluation demonstrates that our method outperforms state-of-the-art solutions in both settings.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Li, Yiying and Yang, Yongxin and Zhou, Wei and Hospedales, Timothy M.},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gonzalez_improved_2020,
	title = {Improved training speed, accuracy, and data utilization through loss function optimization},
	url = {http://arxiv.org/abs/1905.11528},
	abstract = {As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss-function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Gonzalez, Santiago and Miikkulainen, Risto},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{gonzalez_optimizing_2020,
	title = {Optimizing loss functions through multivariate taylor polynomial parameterization},
	url = {http://arxiv.org/abs/2002.00059},
	abstract = {Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Gonzalez, Santiago and Miikkulainen, Risto},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2002.00059},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{jenni_deep_2018,
	title = {Deep {Bilevel} {Learning}},
	url = {http://arxiv.org/abs/1809.01465},
	abstract = {We present a novel regularization approach to train neural networks that enjoys better generalization and test error than standard stochastic gradient descent. Our approach is based on the principles of cross-validation, where a validation set is used to limit the model overfitting. We formulate such principles as a bilevel optimization problem. This formulation allows us to define the optimization of a cost on the validation set subject to another optimization on the training set. The overfitting is controlled by introducing weights on each mini-batch in the training set and by choosing their values so that they minimize the error on the validation set. In practice, these weights define mini-batch learning rates in a gradient descent update equation that favor gradients with better generalization capabilities. Because of its simplicity, this approach can be integrated with other regularization methods and training schemes. We evaluate extensively our proposed algorithm on several neural network architectures and datasets, and find that it consistently improves the generalization of the model, especially when labels are noisy.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Jenni, Simon and Favaro, Paolo},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{venkatesh_word_2022,
	title = {Word {Embeddings} for {Automatic} {Equalization} in {Audio} {Mixing}},
	volume = {70},
	number = {9},
	journal = {Journal of The Audio Engineering Society},
	author = {Venkatesh, Satvik and Moffat, David},
	year = {2022},
	keywords = {⛔ No DOI found},
	pages = {11},
}

@book{noauthor_derivative-free_nodate,
	title = {Derivative-{Free} and {Blackbox} {Optimization}},
	url = {https://link.springer.com/book/10.1007/978-3-319-68913-5},
	abstract = {This textbook is suitable for self-learning, or for teaching an upper-year university course on derivative-free and blackbox optimization.},
	urldate = {2022-09-17},
}

@article{stoica_maximum_1989,
	title = {Maximum likelihood estimation of the parameters of multiple sinusoids from noisy measurements},
	volume = {37},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/21705/},
	doi = {10.1109/29.21705},
	number = {3},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Stoica, P. and Moses, R.L. and Friedlander, B. and Soderstrom, T.},
	month = mar,
	year = {1989},
	pages = {378--392},
}

@article{rife_multiple_1976,
	title = {Multiple tone parameter estimation from discrete-{Time} observations},
	volume = {55},
	issn = {00058580},
	url = {https://ieeexplore.ieee.org/document/6768893},
	doi = {10.1002/j.1538-7305.1976.tb02941.x},
	number = {9},
	urldate = {2022-09-18},
	journal = {Bell System Technical Journal},
	author = {Rife, D. C. and Boorstyn, R. R.},
	month = nov,
	year = {1976},
	pages = {1389--1410},
}

@techreport{xu_autoloss_2018,
	title = {{AutoLoss}: {Learning} discrete schedules for alternate optimization},
	shorttitle = {{AutoLoss}},
	url = {http://arxiv.org/abs/1810.02442},
	abstract = {Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable – it can guide and improve the learning of a new task model with different specifications, or on different datasets.},
	urldate = {2022-08-23},
	author = {Xu, Haowen and Zhang, Hao and Hu, Zhiting and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{song_training_2016,
	address = {New York, NY, USA},
	series = {{ICML}'16},
	title = {Training deep neural networks via direct loss minimization},
	abstract = {Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on metrics specific to the application. In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function. This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise.},
	urldate = {2022-08-23},
	publisher = {JMLR.org},
	author = {Song, Yang and Schwing, Alexander G. and Zemel, Richard S. and Urtasun, Raquel},
	month = jun,
	year = {2016},
	pages = {2169--2177},
}

@techreport{yeh_exploiting_2022,
	title = {Exploiting pre-trained feature networks for generative adversarial networks in audio-domain loop generation},
	url = {http://arxiv.org/abs/2209.01751},
	abstract = {While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.},
	urldate = {2022-09-08},
	author = {Yeh, Yen-Tung and Chen, Bo-Yu and Yang, Yi-Hsuan},
	month = sep,
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{colonel_direct_2022-1,
	title = {Direct design of biquad filter cascades with deep learning by sampling random polynomials},
	url = {http://arxiv.org/abs/2110.03691},
	abstract = {Designing infinite impulse response filters to match an arbitrary magnitude response requires specialized techniques. Methods like modified Yule-Walker are relatively efficient, but may not be sufficiently accurate in matching high order responses. On the other hand, iterative optimization techniques often enable superior performance, but come at the cost of longer run-times and are sensitive to initial conditions, requiring manual tuning. In this work, we address some of these limitations by learning a direct mapping from the target magnitude response to the filter coefficient space with a neural network trained on millions of random filters. We demonstrate our approach enables both fast and accurate estimation of filter coefficients given a desired response. We investigate training with different families of random filters, and find training with a variety of filter families enables better generalization when estimating real-world filters, using head-related transfer functions and guitar cabinets as case studies. We compare our method against existing methods including modified Yule-Walker and gradient descent and show our approach is, on average, both faster and more accurate.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Colonel, Joseph T. and Steinmetz, Christian J. and Michelen, Marcus and Reiss, Joshua D.},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2110.03691},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@article{abatzoglou_fast_1985,
	title = {A fast maximum likelihood algorithm for frequency estimation of a sinusoid based on {Newton}'s method},
	volume = {33},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164541/},
	doi = {10.1109/TASSP.1985.1164541},
	number = {1},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Abatzoglou, T.},
	month = feb,
	year = {1985},
	pages = {77--89},
}

@article{parthasarathy_maximum-likelihood_1985,
	title = {Maximum-likelihood estimation of parameters of exponentially damped sinusoids},
	volume = {73},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1457596/},
	doi = {10.1109/PROC.1985.13328},
	number = {10},
	urldate = {2022-09-18},
	journal = {Proceedings of the IEEE},
	author = {Parthasarathy, S. and Tufts, D.W.},
	year = {1985},
	pages = {1528--1530},
}

@article{kumaresan_algorithm_1986,
	title = {An algorithm for pole-zero modeling and spectral analysis},
	volume = {34},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164843/},
	doi = {10.1109/TASSP.1986.1164843},
	number = {3},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Kumaresan, R. and Scharf, L. and Shaw, A.},
	month = jun,
	year = {1986},
	pages = {637--640},
}

@article{evans_optimal_1973,
	title = {Optimal least squares time-domain synthesis of recursive digital filters},
	volume = {21},
	issn = {0018-9278},
	url = {http://ieeexplore.ieee.org/document/1162433/},
	doi = {10.1109/TAU.1973.1162433},
	number = {1},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Audio and Electroacoustics},
	author = {Evans, A. and Fischl, R.},
	month = feb,
	year = {1973},
	pages = {61--65},
}

@article{ortega_dynamic_2018,
	title = {On dynamic regressor extension and mixing parameter estimators: {Two} {Luenberger} observers interpretations},
	volume = {95},
	issn = {0005-1098},
	shorttitle = {On dynamic regressor extension and mixing parameter estimators},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109818303030},
	doi = {10.1016/j.automatica.2018.06.011},
	abstract = {Dynamic regressor extension and mixing is a new technique for parameter estimation with guaranteed performance improvement – with respect to classical gradient or least-squares estimators – that has proven instrumental in the solution of several open problems in system identification and adaptive control. In this brief note we give two interpretations of this parameter estimator in terms of the recent extensions, to the cases of nonlinear systems and observation of linear functionals for time-varying systems, of the classical Luenberger’s state observers.},
	urldate = {2022-09-22},
	journal = {Automatica},
	author = {Ortega, Romeo and Praly, Laurent and Aranovskiy, Stanislav and Yi, Bowen and Zhang, Weidong},
	month = sep,
	year = {2018},
	keywords = {Adaptive systems, Nonlinear systems, Observer design theory, Parameter estimation},
	pages = {548--551},
}

@article{rife_single_1974,
	title = {Single tone parameter estimation from discrete-time observations},
	volume = {20},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1055282/},
	doi = {10.1109/TIT.1974.1055282},
	number = {5},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Information Theory},
	author = {Rife, D. and Boorstyn, R.},
	month = sep,
	year = {1974},
	pages = {591--598},
}

@article{bresler_exact_1986,
	title = {Exact maximum likelihood parameter estimation of superimposed exponential signals in noise},
	volume = {34},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164949/},
	doi = {10.1109/TASSP.1986.1164949},
	number = {5},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Bresler, Y. and Macovski, A.},
	month = oct,
	year = {1986},
	pages = {1081--1089},
}

@book{kay_fundamentals_1993,
	edition = {1},
	title = {Fundamentals of {Statistical} {Signal} {Processing}: {Estimation} {Theory}},
	volume = {1},
	isbn = {978-0-13-345711-7},
	publisher = {Pearson},
	author = {Kay, Steven},
	month = may,
	year = {1993},
}

@misc{kreutz-delgado_complex_2009,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	url = {http://arxiv.org/abs/0906.4835},
	abstract = {A thorough discussion and development of the calculus of real-valued functions of complex-valued vectors is given using the framework of the Wirtinger Calculus. The presented material is suitable for exposition in an introductory Electrical Engineering graduate level course on the use of complex gradients and complex Hessian matrices, and has been successfully used in teaching at UC San Diego. Going beyond the commonly encountered treatments of the first-order complex vector calculus, second-order considerations are examined in some detail filling a gap in the pedagogic literature.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Kreutz-Delgado, Ken},
	month = jun,
	year = {2009},
	doi = {10.48550/arXiv.0906.4835},
	keywords = {Mathematics - Complex Variables, Mathematics - Optimization and Control},
}

@inproceedings{vishnu_improved_2020,
	address = {Bangalore, India},
	title = {An improved {LSF}-based algorithm for sinusoidal frequency estimation that achieves maximum likelihood performance},
	isbn = {978-1-72818-895-9},
	url = {https://ieeexplore.ieee.org/document/9179546/},
	doi = {10.1109/SPCOM50965.2020.9179546},
	abstract = {In this paper we propose a method for sinusoidal frequency estimation that improves upon our previously proposed LSF-based algorithm that used at most 5p candidate points, where p is the number of sinusoids present. In this paper we propose the following improvements: (i) reduced the number of candidate frequencies to at most 2p points, (ii) reduced the method’s threshold to equal that of ML, and (iii) reduced the computational burden by switching to methods like ESPRIT when the SNR is above threshold. Since neither the SNR nor the threshold is known, we estimate them from the data. The proposed reduction-in-threshold step can be applied to EPUMA (proposed Qian et al.), with which we compare our results. For the well-known two-sinusoid example the proposed method has the same threshold as that of ML; ML performance is also achieved when tested on a new, three-sinusoid example.},
	urldate = {2022-09-22},
	booktitle = {2020 international conference on signal processing and communications ({SPCOM})},
	publisher = {IEEE},
	author = {Vishnu, P. and Ramalingam, C.S.},
	month = jul,
	year = {2020},
	pages = {1--5},
}

@inproceedings{gromov_first-order_2017,
	title = {First-order frequency estimator for a pure sinusoidal signal},
	doi = {10.1109/MED.2017.7984087},
	abstract = {This paper is devoted to frequency estimation of a pure sinusoidal signal. The new parameterization approach based on applying delay operators to a measurable signal is proposed. The result is the first order linear regression model with one parameter, which depends on the signal frequency. The estimation algorithm is basing on standard gradient approach. It is shown that the frequency estimation error converges to zero exponentially fast. The described method does not require measuring or calculating derivatives of the input signal and uses only one integrator. The efficiency of the proposed approach is demonstrated through the set of numerical simulations.},
	booktitle = {2017 25th mediterranean conference on control and automation ({MED})},
	author = {Gromov, Vladislav S. and Vedyakov, Alexey A. and Vediakova, Anastasiia O. and Bobtsov, Alexey A. and Pyrkin, Anton A.},
	month = jul,
	year = {2017},
	keywords = {Control systems, Convergence, Delays, Estimation error, Frequency estimation, Linear regression},
	pages = {7--11},
}

@misc{vediakova_frequency_2020,
	title = {Frequency estimation of multi-{Sinusoidal} signals in finite-{Time}},
	url = {http://arxiv.org/abs/2009.06400},
	abstract = {This paper considers the problem of frequency estimation for a multi-sinusoidal signal consisting of n sinuses in finite-time. The parameterization approach based on applying delay operators to a measurable signal is used. The result is the nth order linear regression model with n parameters, which depends on the signals frequencies. We propose to use Dynamic Regressor Extension and Mixing method to replace nth order regression model with n first-order regression models. Then the standard gradient descent method is used to estimate separately for each the regression model parameter. On the next step using algebraic equations finite-time frequency estimate is found. The described method does not require measuring or calculating derivatives of the input signal, and uses only the signal measurement. The efficiency of the proposed approach is demonstrated through the set of numerical simulations.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Vediakova, Anastasiia and Vedyakov, Alexey and Pyrkin, Anton and Bobtsov, Alexey and Gromov, Vladislav},
	month = sep,
	year = {2020},
	keywords = {93C40, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{guzman_gradient_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Gradient} {Ascent} {Approach} for {Multiple} {Frequency} {Estimation}},
	isbn = {978-3-030-45096-0},
	doi = {10.1007/978-3-030-45096-0_3},
	abstract = {This work investigates a new approach for frequency estimation of multiple complex sinusoids in the presence of noise. The algorithm is based on the optimization of the least squares (LS) cost function using a gradient ascent algorithm. The paper studies the performance of the proposed method and compares it to other estimation techniques such as root-multiple signal classification (root-MUSIC) and the discrete-time Fourier transform (DTFT). Simulation results show the performance gains provided by the proposed algorithm in different scenarios.},
	booktitle = {Computer {Aided} {Systems} {Theory} – {EUROCAST} 2019},
	publisher = {Springer International Publishing},
	author = {Guzman, Yuneisy E. Garcia and Lunglmayr, Michael and Huemer, Mario},
	editor = {Moreno-Díaz, Roberto and Pichler, Franz and Quesada-Arencibia, Alexis},
	year = {2020},
	keywords = {Cramer-Rao lower bound, DTFT, Frequency estimation, Gradient ascent, root-MUSIC},
	pages = {20--27},
}

@article{tseng_low-complexity_2022,
	title = {Low-complexity high-{Resolution} frequency estimation of multi-{Sinusoidal} signals},
	volume = {71},
	issn = {1557-9662},
	doi = {10.1109/TIM.2022.3187728},
	abstract = {High-resolution frequency estimation is crucial for some applications. Accordingly, this article proposes three high-performance computationally efficient (HPCE) methods for high-resolution frequency estimators, which are designed based on a modified likelihood function. Traditional maximum likelihood-based approaches for high-resolution frequency estimation are inefficient since the associated optimization problem is nonconvex. Accordingly, in the first estimator proposed in this study, the amplitudes and frequencies of the multi-sinusoidal signals are estimated iteratively based on a simple linear Taylor approximation and a low-dimensional closed-form solution in every iteration. In the second estimator, the frequencies are determined directly using a primal decomposition approach and a gradient descent search method. Finally, a novel low-complexity parallel interference cancellation (PIC)-based frequency estimation approach is developed. The simulation results show that the proposed designs not only meet the Cramér–Rao lower bound (CRLB) in most cases of the conducted examples but also possess lower computational complexity than existing state-of-the-art approaches.},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Tseng, Fan-Shuo and Sanpayao, Mantsawee and Wang, Tsang-Yi and Zhong, Ming-Xian},
	year = {2022},
	keywords = {Computational complexity, Cramér–Rao lower bound (CRLB), Detectors, Frequency estimation, Interference cancellation, Maximum likelihood estimation, Multiple signal classification, Signal resolution, frequency estimation, low complexity, multi-sinusoidal signals, primal decomposition},
	pages = {1--12},
}

@misc{zhang_deep_2020,
	title = {Deep {Set} {Prediction} {Networks}},
	url = {http://arxiv.org/abs/1906.06565},
	abstract = {Current approaches for predicting sets from feature vectors ignore the unordered nature of sets and suffer from discontinuity issues as a result. We propose a general model for predicting sets that properly respects the structure of sets and avoids this problem. With a single feature vector as input, we show that our model is able to auto-encode point sets, predict the set of bounding boxes of objects in an image, and predict the set of attributes of these objects.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Zhang, Yan and Hare, Jonathon and Prügel-Bennett, Adam},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{abeysekera_least-squares_2018,
	title = {Least-squares multiple frequency estimation using recursive regression sum of squares},
	doi = {10.1109/ISCAS.2018.8351138},
	abstract = {An accurate frequency estimation method from a signal consisting of multiple complex sinusoids is presented. The method avoids inversion of data dependent matrices of large dimensions, and hence do not suffer from rank deficiency of the data covariance matrix. Using recursion, a procedure to reduce mw/t/-dimensional maximum likelihood estimation to a series of one-dimensional optimization while achieving Cramer-Rao bounds for estimation is proposed.},
	booktitle = {2018 {IEEE} international symposium on circuits and systems ({ISCAS})},
	author = {Abeysekera, Saman S.},
	month = may,
	year = {2018},
	keywords = {Covariance matrices, Cramer-Rao bound, Cramer-Rao bounds, Decorrelation, Discrete-time Fourier transform, Frequency estimation, Optimization, Regression sum of squares, Spectral decorrelation},
	pages = {1--5},
}

@article{vediakova_finite_2021,
	title = {Finite {Time} {Frequency} {Estimation} for {Multi}-{Sinusoidal} {Signals}},
	volume = {59},
	url = {https://www.sciencedirect.com/science/article/pii/S0947358021000121},
	doi = {10.1016/j.ejcon.2021.01.004},
	abstract = {The paper presents a method to estimate frequencies of a multi-sinusoidal signal in finite time. Expressing the signal via delayed measurements, we transform this nonlinear problem into a linear regression, where unknown parameters depend on signal frequencies. Dynamic Regressor Extension and Mixing method is used to replace nth order regression model with scalar regression. After, the parameters are estimated separately with the standard gradient descent method. On the last step, the finite time frequency estimate is found algebraically. In contrast to the standard gradient descent method, there is not a trade-off between estimation duration and sensitivity to measurement noise.},
	urldate = {2022-09-22},
	journal = {European Journal of Control},
	author = {Vediakova, Anastasiia O. and Vedyakov, Alexey A. and Pyrkin, Anton A. and Bobtsov, Alexey A. and Gromov, Vladislav S.},
	month = may,
	year = {2021},
	keywords = {continuous-time estimation, finite time estimator, online frequency estimation},
	pages = {38--46},
}

@inproceedings{gajecki_end--end_2022,
	title = {An end-to-end deep learning speech coding and denoising strategy for cochlear implants},
	doi = {10.1109/ICASSP43922.2022.9746963},
	abstract = {Cochlear implant (CI) users struggle to understand speech in noisy conditions. To address this problem, we propose a deep learning speech denoising sound coding strategy that estimates the CI electric stimulation patterns out of the raw audio data captured by the microphone, performing end-to-end CI processing. To estimate the relative denoising performance differences between various approaches, we compared this technique to a classic Wiener filter and to a convTasNet. Speech enhancement performance was assessed by means of signal-to-noise-ratio improvement and the short-time objective speech intelligibility measure. Additionally, 5 CI users were evaluated for speech intelligibility in noise to assess the potential benefits of each algorithm. Our results show that the proposed method is capable of replacing a CI sound coding strategy while preserving its general use for every listener and performing speech enhancement in noisy environments, without sacrificing algorithmic latency.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Gajecki, Tom and Nogueira, Waldo},
	month = may,
	year = {2022},
	keywords = {Cochlear Implant, Cochlear implants, Deep Learning, Deep learning, Signal processing, Signal processing algorithms, Sound Coding Strategy, Speech Enhancement, Speech coding, Speech enhancement, Wiener filters},
	pages = {3109--3113},
}

@inproceedings{hayes_timbrefun_2022,
	address = {Gyeongju, South Korea},
	title = {timbre.fun: {A} gamified interactive system for crowdsourcing a timbre semantic vocabulary},
	booktitle = {Proceedings of the 24th {International} {Congress} on {Acousics}},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = oct,
	year = {2022},
	note = {tex.copyright: All rights reserved},
}

@article{zhang_set_2021,
	title = {Set prediction without imposing structure as conditional density estimation},
	abstract = {Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdeﬁned cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper, we propose an alternative to training via set losses by viewing learning as conditional density estimation. Our learning framework ﬁts deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reﬂecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.},
	author = {Zhang, David W and Burghouts, Gertjan J and Snoek, Cees G M},
	year = {2021},
	keywords = {⛔ No DOI found},
	pages = {18},
}

@misc{rezatofighi_deep_2018,
	title = {Deep {Perm}-{Set} {Net}: {Learn} to predict sets with unknown permutation and cardinality using deep neural networks},
	shorttitle = {Deep {Perm}-{Set} {Net}},
	url = {http://arxiv.org/abs/1805.00613},
	abstract = {Many real-world problems, e.g. object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Specifically, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this new formulation on two relevant vision problems: object detection, for which our formulation outperforms state-of-the-art detectors such as Faster R-CNN and YOLO, and a complex CAPTCHA test, where we observe that, surprisingly, our set based network acquired the ability of mimicking arithmetics without any rules being coded.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Rezatofighi, S. Hamid and Kaskman, Roman and Motlagh, Farbod T. and Shi, Qinfeng and Cremers, Daniel and Leal-Taixé, Laura and Reid, Ian},
	month = oct,
	year = {2018},
	doi = {10.48550/arXiv.1805.00613},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{colonel_approximating_2022,
	title = {Approximating ballistics in a differentiable dynamic range compressor},
	url = {https://www.aes.org/e-lib/browse.cfm?elib=21915},
	abstract = {We present a dynamic range compressor with ballistics implemented in a differentiable framework that can be used for differentiable digital signal processing tasks. This compressor can update the values of its threshold, compression ratio, knee width, makeup gain, attack time, and release time using stochastic gradient descent and backpropagation techniques. The performance of this technique is evaluated on a reverse engineering of audio effects task, in which the parameter settings of a...},
	urldate = {2022-10-25},
	publisher = {Audio Engineering Society},
	author = {Colonel, Joseph and Reiss, Joshua D.},
	month = oct,
	year = {2022},
}

@misc{kim_setvae_2021,
	title = {{SetVAE}: {Learning} hierarchical composition for generative modeling of set-{Structured} data},
	shorttitle = {{SetVAE}},
	url = {http://arxiv.org/abs/2103.15619},
	abstract = {Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/jw9730/setvae.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Kim, Jinwoo and Yoo, Jaehoon and Lee, Juho and Hong, Seunghoon},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2103.15619},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{tang_sequence--set_2022,
	title = {Sequence-to-{Set} {Generative} {Models}},
	url = {http://arxiv.org/abs/2209.08801},
	abstract = {In this paper, we propose a sequence-to-set method that can transform any sequence generative model based on maximum likelihood to a set generative model where we can evaluate the utility/probability of any set. An efficient importance sampling algorithm is devised to tackle the computational challenge of learning our sequence-to-set model. We present GRU2Set, which is an instance of our sequence-to-set method and employs the famous GRU model as the sequence generative model. To further obtain permutation invariant representation of sets, we devise the SetNN model which is also an instance of the sequence-to-set model. A direct application of our models is to learn an order/set distribution from a collection of e-commerce orders, which is an essential step in many important operational decisions such as inventory arrangement for fast delivery. Based on the intuition that small-sized sets are usually easier to learn than large sets, we propose a size-bias trick that can help learn better set distributions with respect to the \${\textbackslash}ell\_1\$-distance evaluation metric. Two e-commerce order datasets, TMALL and HKTVMALL, are used to conduct extensive experiments to show the effectiveness of our models. The experimental results demonstrate that our models can learn better set/order distributions from order data than the baselines. Moreover, no matter what model we use, applying the size-bias trick can always improve the quality of the set distribution learned from data.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Tang, Longtao and Zhou, Ying and Yang, Yu},
	month = sep,
	year = {2022},
	doi = {10.48550/arXiv.2209.08801},
	keywords = {Computer Science - Machine Learning},
}

@misc{vignac_top-n_2022,
	title = {Top-{N}: {Equivariant} set and graph generation without exchangeability},
	shorttitle = {Top-{N}},
	url = {http://arxiv.org/abs/2110.02096},
	abstract = {This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15\% at SetMNIST reconstruction, by 33\% at object detection on CLEVR, generates sets that are 74\% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Vignac, Clement and Frossard, Pascal},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2110.02096},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{madaan_conditional_2022,
	title = {Conditional set generation using {Seq2seq} models},
	url = {http://arxiv.org/abs/2205.12485},
	abstract = {Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. We jointly model the set cardinality and output by prepending the set size and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this augmented data (without any additional annotations) gets an average relative improvement of 20\% on four benchmark datasets across various models: BART, T5, and GPT-3. Code to use SETAUG available at: https://setgen.structgen.com.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Madaan, Aman and Rajagopal, Dheeraj and Tandon, Niket and Yang, Yiming and Bosselut, Antoine},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2205.12485},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kosiorek_conditional_2020,
	title = {Conditional {Set} {Generation} with {Transformers}},
	url = {http://arxiv.org/abs/2006.16841},
	abstract = {A set is an unordered collection of unique elements–and yet many machine learning models that generate sets impose an implicit or explicit ordering. Since model performance can depend on the choice of order, any particular ordering can lead to sub-optimal results. An alternative solution is to use a permutation-equivariant set generator, which does not specify an order-ing. An example of such a generator is the DeepSet Prediction Network (DSPN). We introduce the Transformer Set Prediction Network (TSPN), a flexible permutation-equivariant model for set prediction based on the transformer, that builds upon and outperforms DSPN in the quality of predicted set elements and in the accuracy of their predicted sizes. We test our model on MNIST-as-point-clouds (SET-MNIST) for point-cloud generation and on CLEVR for object detection.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Kosiorek, Adam R. and Kim, Hyunjik and Rezende, Danilo J.},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{preechakul_set_2021,
	title = {Set {Prediction} in the {Latent} {Space}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d61e9e58ae1058322bc169943b39f1d8-Abstract.html},
	abstract = {Set prediction tasks require the matching between predicted set and ground truth set in order to propagate the gradient signal. Recent works have performed this matching in the original feature space thus requiring predefined distance functions. We propose a method for learning the distance function by performing the matching in the latent space learned from encoding networks. This method enables the use of teacher forcing which was not possible previously since matching in the feature space must be computed after the entire output sequence is generated. Nonetheless, a naive implementation of latent set prediction might not converge due to permutation instability. To address this problem, we provide sufficient conditions for permutation stability which begets an algorithm to improve the overall model convergence. Experiments on several set prediction tasks, including image captioning and object detection, demonstrate the effectiveness of our method.},
	urldate = {2022-11-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Preechakul, Konpat and Piansaddhayanon, Chawan and Naowarat, Burin and Khandhawit, Tirasan and Sriswasdi, Sira and Chuangsuwanich, Ekapol},
	year = {2021},
	keywords = {⛔ No DOI found},
	pages = {25516--25527},
}

@misc{niu_permutation_2020,
	title = {Permutation invariant graph generation via score-{Based} generative modeling},
	url = {http://arxiv.org/abs/2003.00638},
	abstract = {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.},
	urldate = {2022-11-26},
	author = {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
	month = mar,
	year = {2020},
	doi = {10.48550/arXiv.2003.00638},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{caetano_adaptive_2015,
	title = {Adaptive {Modeling} of {Synthetic} {Nonstationary} {Sinusoids}},
	abstract = {Nonstationary oscillations are ubiquitous in music and speech, ranging from the fast transients in the attack of musical instruments and consonants to amplitude and frequency modulations in expressive variations present in vibrato and prosodic contours. Modeling nonstationary oscillations with sinusoids remains one of the most challenging problems in signal processing because the ﬁt also depends on the nature of the underlying sinusoidal model. For example, frequency modulated sinusoids are more appropriate to model vibrato than fast transitions. In this paper, we propose to model nonstationary oscillations with adaptive sinusoids from the extended adaptive quasi-harmonic model (eaQHM). We generated synthetic nonstationary sinusoids with different amplitude and frequency modulations and compared the modeling performance of adaptive sinusoids estimated with eaQHM, exponentially damped sinusoids estimated with ESPRIT, and log-linear-amplitude quadratic-phase sinusoids estimated with frequency reassignment. The adaptive sinusoids from eaQHM outperformed frequency reassignment for all nonstationary sinusoids tested and presented performance comparable to exponentially damped sinusoids.},
	author = {Caetano, Marcelo and Kafentzis, George and Mouchtaris, Athanasios and Shannon, Claude},
	year = {2015},
	keywords = {⛔ No DOI found},
	pages = {7},
}

@article{caetano_full-band_2016,
	title = {Full-band quasi-{Harmonic} analysis and synthesis of musical instrument sounds with adaptive sinusoids},
	volume = {6},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/6/5/127},
	doi = {10.3390/app6050127},
	abstract = {Sinusoids are widely used to represent the oscillatory modes of musical instrument sounds in both analysis and synthesis. However, musical instrument sounds feature transients and instrumental noise that are poorly modeled with quasi-stationary sinusoids, requiring spectral decomposition and further dedicated modeling. In this work, we propose a full-band representation that fits sinusoids across the entire spectrum. We use the extended adaptive Quasi-Harmonic Model (eaQHM) to iteratively estimate amplitude- and frequency-modulated (AM–FM) sinusoids able to capture challenging features such as sharp attacks, transients, and instrumental noise. We use the signal-to-reconstruction-error ratio (SRER) as the objective measure for the analysis and synthesis of 89 musical instrument sounds from different instrumental families. We compare against quasi-stationary sinusoids and exponentially damped sinusoids. First, we show that the SRER increases with adaptation in eaQHM. Then, we show that full-band modeling with eaQHM captures partials at the higher frequency end of the spectrum that are neglected by spectral decomposition. Finally, we demonstrate that a frame size equal to three periods of the fundamental frequency results in the highest SRER with AM–FM sinusoids from eaQHM. A listening test confirmed that the musical instrument sounds resynthesized from full-band analysis with eaQHM are virtually perceptually indistinguishable from the original recordings.},
	number = {5},
	urldate = {2022-12-01},
	journal = {Applied Sciences},
	author = {Caetano, Marcelo and Kafentzis, George P. and Mouchtaris, Athanasios and Stylianou, Yannis},
	month = may,
	year = {2016},
	keywords = {AM–FM sinusoids, adaptive modeling, analysis and synthesis, full-band modeling, musical instruments, nonstationary sinusoids, sinusoidal modeling},
	pages = {127},
}

@inproceedings{garcia_satorras_en_2021,
	title = {E(n) {Equivariant} {Normalizing} {Flows}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/21b5680d80f75a616096f2e791affac6-Abstract.html},
	abstract = {This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.},
	urldate = {2022-11-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garcia Satorras, Victor and Hoogeboom, Emiel and Fuchs, Fabian and Posner, Ingmar and Welling, Max},
	year = {2021},
	keywords = {⛔ No DOI found},
	pages = {4181--4192},
}

@inproceedings{goodwin_overlap-add_1995,
	address = {Banff, AB, Canada},
	title = {Overlap-add synthesis of {NonStationary} sinusoids},
	url = {https://hdl.handle.net/2027/spo.bbp2372.1995.103},
	booktitle = {Proceedings of the 1995 {International} {Computer} {Music} {Conference}},
	publisher = {Michigan Publishing},
	author = {Goodwin, Michael and Kogon, Alex},
	month = sep,
	year = {1995},
	keywords = {⛔ No DOI found},
}

@misc{pariente_asteroid_2020,
	title = {Asteroid: the {PyTorch}-based audio source separation toolkit for researchers},
	shorttitle = {Asteroid},
	url = {http://arxiv.org/abs/2005.04132},
	abstract = {This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of Asteroid and its most important features. By showing experimental results obtained with Asteroid's recipes, we show that our implementations are at least on par with most results reported in reference papers. The toolkit is publicly available at https://github.com/mpariente/asteroid .},
	urldate = {2022-12-11},
	author = {Pariente, Manuel and Cornell, Samuele and Cosentino, Joris and Sivasankaran, Sunit and Tzinis, Efthymios and Heitkaemper, Jens and Olvera, Michel and Stöter, Fabian-Robert and Hu, Mathieu and Martín-Doñas, Juan M. and Ditter, David and Frank, Ariel and Deleforge, Antoine and Vincent, Emmanuel},
	month = may,
	year = {2020},
	doi = {10.48550/arXiv.2005.04132},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{defossez_hybrid_2022,
	title = {Hybrid {Spectrogram} and {Waveform} {Source} {Separation}},
	url = {http://arxiv.org/abs/2111.03600},
	abstract = {Source separation models either work on the spectrogram or waveform domain. In this work, we show how to perform end-to-end hybrid source separation, letting the model decide which domain is best suited for each source, and even combining both. The proposed hybrid version of the Demucs architecture won the Music Demixing Challenge 2021 organized by Sony. This architecture also comes with additional improvements, such as compressed residual branches, local attention or singular value regularization. Overall, a 1.4 dB improvement of the Signal-To-Distortion (SDR) was observed across all sources as measured on the MusDB HQ dataset, an improvement confirmed by human subjective evaluation, with an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid Demucs), and absence of contamination at 3.04 (against 2.37 for the non hybrid Demucs and 2.44 for the second ranking model submitted at the competition).},
	urldate = {2022-12-11},
	author = {Défossez, Alexandre},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2111.03600},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{luo_conv-tasnet_2019,
	title = {Conv-{TasNet}: {Surpassing} ideal time–{Frequency} magnitude masking for speech separation},
	volume = {27},
	issn = {2329-9304},
	shorttitle = {Conv-{TasNet}},
	doi = {10.1109/TASLP.2019.2915167},
	abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time–frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time–frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network consisting of stacked one-dimensional dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time–frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time–frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study, therefore, represents a major step toward the realization of speech separation systems for real-world speech processing technologies.},
	number = {8},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Luo, Yi and Mesgarani, Nima},
	month = aug,
	year = {2019},
	keywords = {Convolution, Decoding, Deep learning, Source separation, Spectrogram, Speech processing, Time-domain analysis, Time-frequency analysis, deep learning, real-time, single-channel, time-domain},
	pages = {1256--1266},
}

@inproceedings{wisdom_unsupervised_2020,
	title = {Unsupervised {Sound} {Separation} {Using} {Mixture} {Invariant} {Training}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/28538c394c36e4d5ea8ff5ad60562a93-Abstract.html},
	abstract = {In recent years, rapid progress has been made on the problem of single-channel sound separation using supervised training of deep neural networks. In such supervised approaches, a model is trained to predict the component sources from synthetic mixtures created by adding up isolated ground-truth sources. Reliance on this synthetic training data is problematic because good performance depends upon the degree of match between the training data and real-world audio, especially in terms of the acoustic conditions and distribution of sources. The acoustic properties can be challenging to accurately simulate, and the distribution of sound types may be hard to replicate. In this paper, we propose a completely unsupervised method, mixture invariant training (MixIT), that requires only single-channel acoustic mixtures. In MixIT, training examples are constructed by mixing together existing mixtures, and the model separates them into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures. We show that MixIT can achieve competitive performance compared to supervised methods on speech separation. Using MixIT in a semi-supervised learning setting enables unsupervised domain adaptation and learning from large amounts of real-world data without ground-truth source waveforms. In particular, we significantly improve reverberant speech separation performance by incorporating reverberant mixtures, train a speech enhancement system from noisy mixtures, and improve universal sound separation by incorporating a large amount of in-the-wild data.},
	urldate = {2022-12-11},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Tzinis, Efthymios and Erdogan, Hakan and Weiss, Ron and Wilson, Kevin and Hershey, John},
	year = {2020},
	pages = {3846--3857},
}

@inproceedings{wisdom_sparse_2021,
	title = {Sparse, efficient, and semantic mixture invariant training: {Taming} in-the-wild unsupervised sound separation},
	shorttitle = {Sparse, {Efficient}, and {Semantic} {Mixture} {Invariant} {Training}},
	doi = {10.1109/WASPAA52581.2021.9632714},
	abstract = {Supervised neural network training has led to significant progress on single-channel sound separation. This approach relies on ground truth isolated sources, which precludes scaling to widely available mixture data and limits progress on open-domain tasks. The recent mixture invariant training (MixIT) method enables training on in-the-wild data; however, it suffers from two outstanding problems. First, it produces models which tend to over-separate, producing more output sources than are present in the input. Second, the exponential computational complexity of the MixIT loss limits the number of feasible output sources. In this paper we address both issues. To combat over-separation we introduce new losses: sparsity losses that favor fewer output sources and a covariance loss that discourages correlated outputs. We also experiment with a semantic classification loss by predicting weak class labels for each mixture. To handle larger numbers of sources, we introduce an efficient approximation using a fast least-squares solution, projected onto the MixIT constraint set. Our experiments show that the proposed losses curtail over-separation and improve overall performance. The best performance is achieved using larger numbers of output sources, enabled by our efficient MixIT loss, combined with sparsity losses to prevent over-separation. On the FUSS test set, we achieve over 13 dB in multi-source SI-SNR improvement, while boosting single-source reconstruction SI-SNR by over 17 dB.},
	booktitle = {2021 {IEEE} workshop on applications of signal processing to audio and acoustics ({WASPAA})},
	author = {Wisdom, Scott and Jansen, Aren and Weiss, Ron J. and Erdogan, Hakan and Hershey, John R.},
	month = oct,
	year = {2021},
	keywords = {Computational modeling, Conferences, Neural networks, Semantics, Signal processing, Speech enhancement, Training, mixture invariant training (MixIT), sparsity, universal sound separation, unsupervised learning},
	pages = {51--55},
}

@misc{subakan_using_2022,
	title = {On {Using} {Transformers} for {Speech}-{Separation}},
	url = {http://arxiv.org/abs/2202.02884},
	abstract = {Transformers have enabled major improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we have proposed SepFormer, which uses self-attention and obtains state-of-the art results on WSJ0-2/3 Mix datasets for speech separation. In this paper, we extend our previous work by providing results on more datasets including LibriMix, and WHAM!, WHAMR! which include noisy and noisy-reverberant conditions. Moreover we provide denoising, and denoising+dereverberation results in the context of speech enhancement, respectively on WHAM! and WHAMR! datasets. We also investigate incorporating recently proposed efficient self-attention mechanisms inside the SepFormer model, and show that by using efficient self-attention mechanisms it is possible to reduce the memory requirements significantly while performing better than the popular convtasnet model on WSJ0-2Mix dataset.},
	urldate = {2022-12-11},
	author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Grondin, Francois and Bronzi, Mirko},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.02884},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{zeghidour_wavesplit_2020,
	title = {Wavesplit: {End}-to-{End} {Speech} {Separation} by {Speaker} {Clustering}},
	shorttitle = {Wavesplit},
	url = {http://arxiv.org/abs/2002.08933},
	abstract = {We introduce Wavesplit, an end-to-end source separation system. From a single mixture, the model infers a representation for each source and then estimates each source signal given the inferred representations. The model is trained to jointly perform both tasks from the raw waveform. Wavesplit infers a set of source representations via clustering, which addresses the fundamental permutation problem of separation. For speech separation, our sequence-wide speaker representations provide a more robust separation of long, challenging recordings compared to prior work. Wavesplit redefines the state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and reverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent LibriMix dataset. Finally, we show that Wavesplit is also applicable to other domains, by separating fetal and maternal heart rates from a single abdominal electrocardiogram.},
	urldate = {2022-12-11},
	author = {Zeghidour, Neil and Grangier, David},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2002.08933},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{balazs_adapted_2013,
	title = {Adapted and adaptive linear time-{Frequency} representations: {A} synthesis point of view},
	volume = {30},
	issn = {1053-5888},
	shorttitle = {Adapted and {Adaptive} {Linear} {Time}-{Frequency} {Representations}},
	url = {http://ieeexplore.ieee.org/document/6633031/},
	doi = {10.1109/MSP.2013.2266075},
	number = {6},
	urldate = {2023-01-19},
	journal = {IEEE Signal Processing Magazine},
	author = {Balazs, Peter and Doerfler, Monika and Kowalski, Matthieu and Torresani, Bruno},
	month = nov,
	year = {2013},
	pages = {20--31},
}

@misc{yang_multiresolution_2019,
	title = {Multiresolution mode decomposition for adaptive time series analysis},
	url = {http://arxiv.org/abs/1709.06880},
	abstract = {This paper proposes the {\textbackslash}emph\{multiresolution mode decomposition\} as a novel model for adaptive time series analysis. The main conceptual innovation is the introduction of the {\textbackslash}emph\{multiresolution intrinsic mode function\} (MIMF) of the form {\textbackslash}[ {\textbackslash}sum\_\{n=-N/2\}ˆ\{N/2-1\} a\_n{\textbackslash}cos(2{\textbackslash}pi n{\textbackslash}phi(t))s\_\{cn\}(2{\textbackslash}pi N{\textbackslash}phi(t))+{\textbackslash}sum\_\{n=-N/2\}ˆ\{N/2-1\}b\_n {\textbackslash}sin(2{\textbackslash}pi n{\textbackslash}phi(t))s\_\{sn\}(2{\textbackslash}pi N{\textbackslash}phi(t)){\textbackslash}] to model nonlinear and non-stationary data with time-dependent amplitudes, frequencies, and waveforms. \%The MIMF explains the intrinsic difficulty in concentrating time-frequency representation of nonlinear and non-stationary data and provides a new direction for mode decomposition. The multiresolution expansion coefficients \${\textbackslash}\{a\_n{\textbackslash}\}\$, \${\textbackslash}\{b\_n{\textbackslash}\}\$, and the shape function series \${\textbackslash}\{s\_\{cn\}(t){\textbackslash}\}\$ and \${\textbackslash}\{s\_\{sn\}(t){\textbackslash}\}\$ provide innovative features for adaptive time series analysis. For complex signals that are a superposition of several MIMFs with well-differentiated phase functions \${\textbackslash}phi(t)\$, a new recursive scheme based on Gauss-Seidel iteration and diffeomorphisms is proposed to identify these MIMFs, their multiresolution expansion coefficients, and shape function series. Numerical examples from synthetic data and natural phenomena are given to demonstrate the power of this new method.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Yang, Haizhao},
	month = aug,
	year = {2019},
	keywords = {Mathematics - Numerical Analysis},
}

@inproceedings{guso_loss_2022,
	title = {On loss functions and evaluation metrics for music source separation},
	doi = {10.1109/ICASSP43922.2022.9746530},
	abstract = {We investigate which loss functions provide better separations via benchmarking an extensive set of those for music source separation. To that end, we first survey the most representative audio source separation losses we identified, to later consistently benchmark them in a controlled experimental setup. We also explore using such losses as evaluation metrics, via cross-correlating them with the results of a subjective test. Based on the observation that the standard signal-to-distortion ratio metric can be misleading in some scenarios, we study alternative evaluation metrics based on the considered losses.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Gusó, Enric and Pons, Jordi and Pascual, Santiago and Serrà, Joan},
	month = may,
	year = {2022},
	keywords = {Benchmark testing, Conferences, Measurement, Multiple signal classification, Music, Source separation, Speech processing, evaluation, loss functions, source separation},
	pages = {306--310},
}

@misc{postolache_adversarial_2022,
	title = {Adversarial permutation invariant training for universal sound separation},
	url = {http://arxiv.org/abs/2210.12108},
	abstract = {Universal sound separation consists of separating mixes with arbitrary sounds of different types, and permutation invariant training (PIT) is used to train source agnostic models that do so. In this work, we complement PIT with adversarial losses but find it challenging with the standard formulation used in speech source separation. We overcome this challenge with a novel I-replacement context-based adversarial loss, and by training with multiple discriminators. Our experiments show that by simply improving the loss (keeping the same model and dataset) we obtain a non-negligible improvement of 1.4 dB SI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be effective at reducing spectral holes, ubiquitous in mask-based separation models, which highlights the potential relevance of adversarial losses for source separation.},
	urldate = {2022-12-11},
	author = {Postolache, Emilian and Pons, Jordi and Pascual, Santiago and Serrà, Joan},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2210.12108},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{jones_high_1990,
	title = {A high resolution data-adaptive time-frequency representation},
	volume = {38},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/61539/},
	doi = {10.1109/29.61539},
	number = {12},
	urldate = {2023-01-19},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Jones, D.L. and Parks, T.W.},
	month = dec,
	year = {1990},
	pages = {2127--2135},
}

@article{coifman_entropy-based_1992,
	title = {Entropy-based algorithms for best basis selection},
	volume = {38},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/119732/},
	doi = {10.1109/18.119732},
	number = {2},
	urldate = {2023-01-19},
	journal = {IEEE Transactions on Information Theory},
	author = {Coifman, R.R. and Wickerhauser, M.V.},
	month = mar,
	year = {1992},
	pages = {713--718},
}

@incollection{davis_adaptive_1994,
	title = {Adaptive {Time}-{Frequency} {Approximations} with {Matching} {Pursuits}},
	volume = {5},
	isbn = {978-0-08-052084-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080520841500181},
	urldate = {2023-01-19},
	booktitle = {Wavelet {Analysis} and {Its} {Applications}},
	publisher = {Elsevier},
	author = {Davis, Geoffrey and Mallat, Stéphane and Zhang, Zhifeng},
	year = {1994},
	doi = {10.1016/B978-0-08-052084-1.50018-1},
	pages = {271--293},
}

@misc{peifer_sparse_2019,
	title = {Sparse multiresolution representations with adaptive kernels},
	url = {http://arxiv.org/abs/1905.02797},
	abstract = {Reproducing kernel Hilbert spaces (RKHSs) are key elements of many non-parametric tools successfully used in signal processing, statistics, and machine learning. In this work, we aim to address three issues of the classical RKHS based techniques. First, they require the RKHS to be known a priori, which is unrealistic in many applications. Furthermore, the choice of RKHS affects the shape and smoothness of the solution, thus impacting its performance. Second, RKHSs are ill-equipped to deal with heterogeneous degrees of smoothness, i.e., with functions that are smooth in some parts of their domain but vary rapidly in others. Finally, the computational complexity of evaluating the solution of these methods grows with the number of data points, rendering these techniques infeasible for many applications. Though kernel learning, local kernel adaptation, and sparsity have been used to address these issues, many of these approaches are computationally intensive or forgo optimality guarantees. We tackle these problems by leveraging a novel integral representation of functions in RKHSs that allows for arbitrary centers and different kernels at each center. To address the complexity issues, we then write the function estimation problem as a sparse functional program that explicitly minimizes the support of the representation leading to low complexity solutions. Despite their non-convexity and infinite dimensionality, we show these problems can be solved exactly and efficiently by leveraging duality, and we illustrate this new approach in simulated and real data.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Peifer, Maria and Chamon, Luiz F. O. and Paternain, Santiago and Ribeiro, Alejandro},
	month = may,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{raymond_online_2023,
	title = {Online {Loss} {Function} {Learning}},
	url = {http://arxiv.org/abs/2301.13247},
	abstract = {Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently outperforms the cross-entropy loss and offline loss function learning techniques on a diverse range of neural network architectures and datasets.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Raymond, Christian and Chen, Qi and Xue, Bing and Zhang, Mengjie},
	month = jan,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lample_fader_2018,
	title = {Fader {Networks}: {Manipulating} {Images} by {Sliding} {Attributes}},
	shorttitle = {Fader {Networks}},
	url = {http://arxiv.org/abs/1706.00409},
	abstract = {This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
	month = jan,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{schmidhuber_learning_1992,
	title = {Learning {Factorial} {Codes} by {Predictability} {Minimization}},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/4/6/863-879/5678},
	doi = {10.1162/neco.1992.4.6.863},
	abstract = {I propose a novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor, which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter "abstract concepts" out of the environmental input such that these concepts are statistically independent of those on which the other units focus. I discuss various simple yet potentially powerful implementations of the principle that aim at finding binary factorial codes (Barlow et al. 1989), i.e., codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, and (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also nonlinear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences.},
	number = {6},
	urldate = {2023-02-08},
	journal = {Neural Computation},
	author = {Schmidhuber, Jürgen},
	month = nov,
	year = {1992},
	pages = {863--879},
}

@misc{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	month = may,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{zhang_fspool_2020,
	title = {{FSPool}: {Learning} set representations with featurewise sort pooling},
	shorttitle = {{FSPool}},
	url = {http://arxiv.org/abs/1906.02795},
	abstract = {Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on a variety of datasets.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Zhang, Yan and Hare, Jonathon and Prügel-Bennett, Adam},
	month = may,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ismailov_three_2022,
	title = {A three layer neural network can represent any multivariate function},
	url = {http://arxiv.org/abs/2012.03016},
	abstract = {In 1987, Hecht-Nielsen showed that any continuous multivariate function can be implemented by a certain type three-layer neural network. This result was very much discussed in neural network literature. In this paper we prove that not only continuous functions but also all discontinuous functions can be implemented by such neural networks.},
	urldate = {2023-02-19},
	publisher = {arXiv},
	author = {Ismailov, Vugar},
	month = jan,
	year = {2022},
	keywords = {26B40, 46A22, 46E10, 46N60, 68T05, 92B20, Computer Science - Machine Learning, Mathematics - Functional Analysis, Statistics - Machine Learning},
}

@article{kratsios_learning_2022,
	title = {Learning sub-patterns in piecewise continuous functions},
	volume = {480},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122200056X},
	doi = {10.1016/j.neucom.2022.01.036},
	urldate = {2023-02-19},
	journal = {Neurocomputing},
	author = {Kratsios, Anastasis and Zamanlooy, Behnoosh},
	month = apr,
	year = {2022},
	pages = {192--211},
}

@misc{louppe_learning_2017,
	title = {Learning to {Pivot} with {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.01046},
	abstract = {Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot – a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Physics - Data Analysis, Statistics - Machine Learning, Statistics - Methodology, Statistics and Probability},
}

@article{llanas_constructive_2008,
	title = {Constructive approximation of discontinuous functions by neural networks},
	volume = {27},
	issn = {1370-4621, 1573-773X},
	url = {http://link.springer.com/10.1007/s11063-007-9070-9},
	doi = {10.1007/s11063-007-9070-9},
	number = {3},
	urldate = {2023-02-19},
	journal = {Neural Processing Letters},
	author = {Llanas, B. and Lantarón, S. and Sáinz, F. J.},
	month = jun,
	year = {2008},
	pages = {209--226},
}

@article{selmic_neural-network_2002,
	title = {Neural-network approximation of piecewise continuous functions: application to friction compensation},
	volume = {13},
	issn = {1045-9227},
	shorttitle = {Neural-network approximation of piecewise continuous functions},
	url = {http://ieeexplore.ieee.org/document/1000141/},
	doi = {10.1109/TNN.2002.1000141},
	number = {3},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Neural Networks},
	author = {Selmic, R.R. and Lewis, F.L.},
	month = may,
	year = {2002},
	pages = {745--751},
}

@article{hemasinha_ordered_2015,
	title = {Ordered vector spaces},
	volume = {468},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002437951400319X},
	doi = {10.1016/j.laa.2014.05.028},
	urldate = {2023-02-19},
	journal = {Linear Algebra and its Applications},
	author = {Hemasinha, Rohan and Weaver, James R.},
	month = mar,
	year = {2015},
	pages = {171--183},
}

@misc{chen_dual-path_2020,
	title = {Dual-path transformer network: {Direct} context-{Aware} modeling for end-to-end monaural speech separation},
	shorttitle = {Dual-{Path} {Transformer} {Network}},
	url = {http://arxiv.org/abs/2007.13975},
	abstract = {The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPTNet) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).},
	urldate = {2023-02-20},
	publisher = {arXiv},
	author = {Chen, Jingjing and Mao, Qirong and Liu, Dong},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{steinmetz_deep_2022,
	title = {Deep learning for automatic mixing},
	author = {Steinmetz, Christian J and Sai Vanka, Soumya and Bromham, Gary and Martinez Ramirez, Marco A.},
	year = {2022},
	note = {Place: Bengaluru, India
Type: Tutorial},
}

@misc{executable_books_community_jupyter_2020,
	title = {Jupyter {Book}},
	url = {https://zenodo.org/record/2561065},
	abstract = {{\textless}strong{\textgreater}Jupyter Book{\textless}/strong{\textgreater} is an open source project for building beautiful, publication-quality books and documents from computational material. Here are some of the features of Jupyter Book: ✔ Write publication-quality content in Markdown. You can write in either Jupyter Markdown, or an extended flavor of Markdown with publishing features. This includes support for rich syntax such as citations and cross-references, math and equations, and figures. ✔ Write content in Jupyter Notebook. This allows you to include your code and outputs in your book. You can also write notebooks entirely in Markdown that get executed when you build your book. ✔ Execute and cache your book’s content. For {\textless}code{\textgreater}.ipynb{\textless}/code{\textgreater} and Markdown notebooks, execute code and insert the latest outputs into your book. In addition, cache and re-use outputs to be used later. ✔ Insert notebook outputs into your content. Generate outputs as you build your documentation, and insert them in-line with your content across pages. ✔ Add interactivity to your book. You can toggle cell visibility, include interactive outputs from Jupyter, and connect with online services like Binder. ✔ Generate a variety of outputs. This includes single- and multi-page websites, as well as PDF outputs. ✔ Build books with a simple command-line interface. You can quickly generate your books with one command, like so: {\textless}code{\textgreater}jupyter-book build mybook/{\textless}/code{\textgreater}},
	urldate = {2023-03-26},
	publisher = {Zenodo},
	author = {Community, Executable Books},
	month = feb,
	year = {2020},
	doi = {10.5281/ZENODO.2561065},
	note = {tex.copyright: Open Access},
	keywords = {data science, jupyter, publishing, scholarship},
}

@misc{lee_bigvgan_2023,
	title = {{BigVGAN}: {A} universal neural vocoder with large-{Scale} training},
	shorttitle = {{BigVGAN}},
	url = {http://arxiv.org/abs/2206.04658},
	abstract = {Despite recent progress in generative adversarial network (GAN)-based vocoders, where the model generates raw waveform conditioned on acoustic features, it is challenging to synthesize high-fidelity audio for numerous speakers across various recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning. We introduce periodic activation function and anti-aliased representation into the GAN generator, which brings the desired inductive bias for audio synthesis and significantly improves audio quality. In addition, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. We identify and address the failure modes in large-scale GAN training for audio, while maintaining high-fidelity output without over-regularization. Our BigVGAN, trained only on clean speech (LibriTTS), achieves the state-of-the-art performance for various zero-shot (out-of-distribution) conditions, including unseen speakers, languages, recording environments, singing voices, music, and instrumental audio. We release our code and model at: https://github.com/NVIDIA/BigVGAN},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Lee, Sang-gil and Ping, Wei and Ginsburg, Boris and Catanzaro, Bryan and Yoon, Sungroh},
	month = feb,
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zen_libritts_2019,
	title = {{LibriTTS}: {A} corpus derived from {LibriSpeech} for text-to-speech},
	shorttitle = {{LibriTTS}},
	url = {http://arxiv.org/abs/1904.02882},
	abstract = {This paper introduces a new speech corpus called "LibriTTS" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from http://www.openslr.org/60/.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J. and Jia, Ye and Chen, Zhifeng and Wu, Yonghui},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@book{won_minz_music_2021,
	title = {Music classification: {Beyond} supervised learning, towards real-world applications},
	shorttitle = {Music {Classification}},
	url = {https://music-classification.github.io/tutorial},
	abstract = {NOTE: We strongly recommend visiting https://music-classification.github.io/tutorial/ and use a web version of the book. This is a book written for a tutorial session of the 22nd International Society for Music Information Retrieval Conference, Nov 8-12, 2021 in an online format. In this book, we focus on the more modern history of music classification since the popularization of deep learning in mid 2010s.},
	urldate = {2023-03-26},
	publisher = {https://music-classification.github.io/tutorial},
	author = {Won, Minz and Spijkervet, Janne and Choi, Keunwoo},
	month = nov,
	year = {2021},
	note = {tex.copyright: Creative Commons Attribution 4.0 International, Open Access},
	keywords = {genre classification, ismir, music classification, music tagging},
}

@article{gordon_you_1996,
	title = {You can't hear the shape of a drum},
	volume = {84},
	issn = {00030996},
	url = {http://www.jstor.org/stable/29775597},
	number = {1},
	urldate = {2023-03-30},
	journal = {American Scientist},
	author = {Gordon, Carolyn and Webb, David},
	year = {1996},
	keywords = {⛔ No DOI found},
	pages = {46--55},
}

@misc{ping_waveflow_2020,
	title = {{WaveFlow}: {A} {Compact} {Flow}-based {Model} for {Raw} {Audio}},
	shorttitle = {{WaveFlow}},
	url = {http://arxiv.org/abs/1912.01219},
	abstract = {In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15\${\textbackslash}times\$ smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6\${\textbackslash}times\$ faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Ping, Wei and Peng, Kainan and Zhao, Kexin and Song, Zhao},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{jacovi_neural_2018,
	title = {Neural network gradient-based learning of black-box function interfaces},
	url = {https://openreview.net/forum?id=r1e13s05YX},
	abstract = {Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this “Estimate and Replace” paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.},
	urldate = {2023-06-05},
	author = {Jacovi, Alon and Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Berant, Jonathan},
	month = dec,
	year = {2018},
}

@inproceedings{hayes_sinusoidal_2023,
	title = {Sinusoidal {Frequency} {Estimation} by {Gradient} {Descent}},
	doi = {10.1109/ICASSP49357.2023.10095188},
	abstract = {Sinusoidal parameter estimation is a fundamental task in applications from spectral analysis to time-series forecasting. Estimating the sinusoidal frequency parameter by gradient descent is, however, often impossible as the error function is non-convex and densely populated with local minima. The growing family of differentiable signal processing methods has therefore been unable to tune the frequency of oscillatory components, preventing their use in a broad range of applications. This work presents a technique for joint sinusoidal frequency and amplitude estimation using the Wirtinger derivatives of a complex exponential surrogate and any first order gradient-based optimizer, enabling end-to-end training of neural network controllers for unconstrained sinusoidal models.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = jun,
	year = {2023},
	keywords = {Frequency estimation, Neural networks, Signal processing, Speech processing, Standards, Task analysis, Training, differentiable signal processing, machine learning, sinusoidal parameter estimation},
	pages = {1--5},
}

@article{andersen_conversations_2016,
	title = {Conversations with expert users in music retrieval and research challenges for creative mir},
	abstract = {Sample retrieval remains a central problem in the creative process of making electronic dance music. This paper describes the ﬁndings from a series of interview sessions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most participants mentioned very practical requirements of storing and retrieving ﬁles. A central aspect of the desired systems is the need to provide increased ﬂow and unbroken periods of concentration and creativity.},
	journal = {New York City},
	author = {Andersen, Kristina and Knees, Peter},
	year = {2016},
	keywords = {⛔ No DOI found},
}

@inproceedings{wells_real-time_2002,
	title = {Real-time partial tracking in an augmented additive synthesis system},
	url = {https://www.dafx.de/paper-archive/details.php?id=bbdq0n169VTtZvKa4ZnFlw},
	abstract = {Wells, J. J.; Murphy, D. T.: Real-Time Partial Tracking in an Augmented Additive Synthesis System, 2002},
	urldate = {2023-04-13},
	author = {Wells, Jeremy J. and Murphy, Damian T.},
	year = {2002},
	keywords = {⛔ No DOI found},
}

@inproceedings{you_gan_2021,
	title = {{GAN} vocoder: {Multi}-resolution discriminator is all you need},
	doi = {10.21437/Interspeech.2021-41},
	booktitle = {Interspeech},
	author = {You, Jaeseong and Kim, Dalhyun and Nam, Gyuhyeon and Hwang, Geumbyeol and Chae, Gyeongsu},
	year = {2021},
}

@inproceedings{grathwohl_backpropagation_2022,
	title = {Backpropagation through the {Void}: {Optimizing} control variates for black-box gradient estimation},
	shorttitle = {Backpropagation through the {Void}},
	url = {https://openreview.net/forum?id=SyzKd1bCW},
	abstract = {Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.},
	urldate = {2023-06-05},
	author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoff and Duvenaud, David},
	month = feb,
	year = {2022},
}

@inproceedings{chaudhari_learning_2023,
	address = {Rhodes Island, Greece},
	title = {Learning gradients of convex functions with monotone gradient networks},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10097266/},
	doi = {10.1109/ICASSP49357.2023.10097266},
	urldate = {2023-06-09},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Chaudhari, Shreyas and Pranav, Srinivasa and Moura, José M.F.},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@misc{bamler_improving_2018,
	title = {Improving optimization for models with continuous symmetry breaking},
	url = {http://arxiv.org/abs/1803.03234},
	abstract = {Many loss functions in representation learning are invariant under a continuous symmetry transformation. For example, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that representation learning models for time series possess an approximate continuous symmetry that leads to slow convergence of gradient descent. We propose a new optimization algorithm that speeds up convergence using ideas from gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence and to more interpretable representations, as we show for dynamic extensions of matrix factorization and word embedding models. We further present an example application of our proposed algorithm that translates modern words into their historic equivalents.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Bamler, Robert and Mandt, Stephan},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{richter-powell_input_2021,
	title = {Input {Convex} {Gradient} {Networks}},
	url = {http://arxiv.org/abs/2111.12187},
	abstract = {The gradients of convex functions are expressive models of non-trivial vector fields. For example, Brenier's theorem yields that the optimal transport map between any two measures on Euclidean space under the squared distance is realized as a convex gradient, which is a key insight used in recent generative flow models. In this paper, we study how to model convex gradients by integrating a Jacobian-vector product parameterized by a neural network, which we call the Input Convex Gradient Network (ICGN). We theoretically study ICGNs and compare them to taking the gradient of an Input-Convex Neural Network (ICNN), empirically demonstrating that a single layer ICGN can fit a toy example better than a single layer ICNN. Lastly, we explore extensions to deeper networks and connections to constructions from Riemannian geometry.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Richter-Powell, Jack and Lorraine, Jonathan and Amos, Brandon},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{saremi_approximating_2019,
	title = {On approximating \${\textbackslash}nabla f\$ with neural networks},
	url = {http://arxiv.org/abs/1910.12744},
	abstract = {Consider a feedforward neural network \${\textbackslash}psi: {\textbackslash}mathbb\{R\}ˆd{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}ˆd\$ such that \${\textbackslash}psi{\textbackslash}approx {\textbackslash}nabla f\$, where \$f:{\textbackslash}mathbb\{R\}ˆd {\textbackslash}rightarrow {\textbackslash}mathbb\{R\}\$ is a smooth function, therefore \${\textbackslash}psi\$ must satisfy \${\textbackslash}partial\_j {\textbackslash}psi\_i = {\textbackslash}partial\_i {\textbackslash}psi\_j\$ pointwise. We prove a theorem that a \${\textbackslash}psi\$ network with more than one hidden layer can only represent one feature in its first hidden layer; this is a dramatic departure from the well-known results for one hidden layer. The proof of the theorem is straightforward, where two backward paths and a weight-tying matrix play the key roles. We then present the alternative, the implicit parametrization, where the neural network is \${\textbackslash}phi: {\textbackslash}mathbb\{R\}ˆd {\textbackslash}rightarrow {\textbackslash}mathbb\{R\}\$ and \${\textbackslash}nabla {\textbackslash}phi {\textbackslash}approx {\textbackslash}nabla f\$; in addition, a "soft analysis" of \${\textbackslash}nabla {\textbackslash}phi\$ gives a dual perspective on the theorem. Throughout, we come back to recent probabilistic models that are formulated as \${\textbackslash}nabla {\textbackslash}phi {\textbackslash}approx {\textbackslash}nabla f\$, and conclude with a critique of denoising autoencoders.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Saremi, Saeed},
	month = nov,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{tayal_inverse_2020,
	title = {Inverse {Problems}, {Deep} {Learning}, and {Symmetry} {Breaking}},
	url = {http://arxiv.org/abs/2003.09077},
	abstract = {In many physical systems, inputs related by intrinsic system symmetries are mapped to the same output. When inverting such systems, i.e., solving the associated inverse problems, there is no unique solution. This causes fundamental difficulties for deploying the emerging end-to-end deep learning approach. Using the generalized phase retrieval problem as an illustrative example, we show that careful symmetry breaking on the training data can help get rid of the difficulties and significantly improve the learning performance. We also extract and highlight the underlying mathematical principle of the proposed solution, which is directly applicable to other inverse problems.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Tayal, Kshitij and Lai, Chieh-Hsin and Kumar, Vipin and Sun, Ju},
	month = mar,
	year = {2020},
	doi = {10.48550/arXiv.2003.09077},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{kuznetsov_differentiable_nodate,
	title = {Differentiable {IIR} filters for machine learning applications},
	abstract = {In this paper we present an approach to using traditional digital IIR filter structures inside deep-learning networks trained using backpropagation. We establish the link between such structures and recurrent neural networks. Three different differentiable IIR filter topologies are presented and compared against each other and an established baseline. Additionally, a simple Wiener-Hammerstein model using differentiable IIRs as its filtering component is presented and trained on a guitar signal played through a Boss DS-1 guitar pedal.},
	author = {Kuznetsov, Boris and Parker, Julian D and Esqueda, Fabián},
	keywords = {⛔ No DOI found},
}

@article{hawley_synthesis_2020,
	title = {Synthesis of musical instrument sounds: {Physics}-{Based} modeling or machine learning?},
	volume = {16},
	issn = {15570215},
	shorttitle = {Synthesis of {Musical} {Instrument} {Sounds}},
	url = {https://acousticstoday.org/issues/2020AT/Spring2020/index.html#p=20},
	doi = {10.1121/AT.2020.16.1.20},
	number = {1},
	urldate = {2023-08-11},
	journal = {Acoustics Today},
	author = {Hawley, Scott H.},
	year = {2020},
	pages = {20},
}

@inproceedings{gebauer_symmetry-adapted_2019,
	title = {Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a4d8e2a7e0d0c102339f97716d2fdfb6-Abstract.html},
	abstract = {Deep learning has proven to yield fast and accurate predictions of quantum-chemical properties to accelerate the discovery of novel molecules and materials. As an exhaustive exploration of the vast chemical space is still infeasible, we require generative models that guide our search towards systems with desired properties. While graph-based models have previously been proposed, they are restricted by a lack of spatial information such that they are unable to recognize spatial isomerism and non-bonded interactions. Here, we introduce a generative neural network for 3d point sets that respects the rotational invariance of the targeted structures. We apply it to the generation of molecules and demonstrate its ability to approximate the distribution of equilibrium structures using spatial metrics as well as established measures from chemoinformatics. As our model is able to capture the complex relationship between 3d geometry and electronic properties, we bias the distribution of the generator towards molecules with a small HOMO-LUMO gap - an important property for the design of organic solar cells.},
	urldate = {2023-08-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gebauer, Niklas and Gastegger, Michael and Schütt, Kristof},
	year = {2019},
	keywords = {⛔ No DOI found},
}

@inproceedings{xie_crystal_2021,
	title = {Crystal diffusion variational autoencoder for periodic material generation},
	url = {https://openreview.net/forum?id=03RLpj-tc%5F},
	abstract = {Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community.},
	urldate = {2023-08-16},
	author = {Xie, Tian and Fu, Xiang and Ganea, Octavian-Eugen and Barzilay, Regina and Jaakkola, Tommi S.},
	month = oct,
	year = {2021},
}

@misc{dehmamy_automatic_2021,
	title = {Automatic symmetry discovery with lie algebra convolutional network},
	url = {http://arxiv.org/abs/2109.07103},
	abstract = {Existing equivariant neural networks require prior knowledge of the symmetry group and discretization for continuous groups. We propose to work with Lie algebras (infinitesimal generators) instead of Lie groups. Our model, the Lie algebra convolutional network (L-conv) can automatically discover symmetries and does not require discretization of the group. We show that L-conv can serve as a building block to construct any group equivariant feedforward architecture. Both CNNs and Graph Convolutional Networks can be expressed as L-conv with appropriate groups. We discover direct connections between L-conv and physics: (1) group invariant loss generalizes field theory (2) Euler-Lagrange equation measures the robustness, and (3) equivariance leads to conservation laws and Noether current.These connections open up new avenues for designing more general equivariant networks and applying them to important problems in physical sciences},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Dehmamy, Nima and Walters, Robin and Liu, Yanchen and Wang, Dashun and Yu, Rose},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2109.07103},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Group Theory},
}

@inproceedings{kicki_new_2021,
	title = {A {New} {Approach} to {Design} {Symmetry} {Invariant} {Neural} {Networks}},
	doi = {10.1109/IJCNN52387.2021.9533541},
	abstract = {We investigate a new method to design G-invariant neural networks that approximate functions invariant to the action of a given permutation subgroup G of the symmetric group on input data. The key element of the new network architecture is a G-invariant transformation module, which produces a G-invariant latent representation of the input data. This latent representation is then processed with a multi-layer perceptron in the network. We prove the universality of the new architecture, discuss its properties and highlight its computational and memory efficiency. Theoretical considerations are supported by numerical experiments involving different network configurations, which demonstrate the efficiency and strong generalization properties of the new approach to design symmetry invariant neural networks, in comparison to other G-invariant neural architectures.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Kicki, Piotr and Skrzypczyński, Piotr and Ozay, Mete},
	month = jul,
	year = {2021},
	keywords = {Computational efficiency, Deep learning, Design methodology, G-invariance, Machine learning, Memory management, Network architecture, Neural networks, geometric deep learning, group invariance, neural networks},
	pages = {1--8},
}

@misc{bloem-reddy_probabilistic_2020,
	title = {Probabilistic symmetries and invariant neural networks},
	url = {http://arxiv.org/abs/1901.06082},
	abstract = {Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.},
	urldate = {2023-08-13},
	publisher = {arXiv},
	author = {Bloem-Reddy, Benjamin and Teh, Yee Whye},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{allingham_learning_2022,
	title = {Learning {Generative} {Models} with {Invariance} to {Symmetries}},
	url = {https://openreview.net/forum?id=Ff1N3et1IV},
	abstract = {While imbuing a model with invariance under symmetry transformations can improve data efficiency and predictive performance, most methods require specialised architectures and, thus, prior knowledge of the symmetries. Unfortunately, we don’t always know what symmetries are present in the data. Recent work has solved this problem by jointly learning the invariance (or the degree of invariance) with the model from the data alone. But, this work has focused on discriminative models. We describe a method for learning invariant generative models. We demonstrate that our method can learn a generative model of handwritten digits that is invariant to rotation.},
	urldate = {2023-08-15},
	author = {Allingham, James Urquhart and Antoran, Javier and Padhy, Shreyas and Nalisnick, Eric and Hernández-Lobato, José Miguel},
	month = nov,
	year = {2022},
}

@article{yang_generative_2023,
	title = {Generative {Adversarial} {Symmetry} {Discovery}},
	url = {https://openreview.net/forum?id=6bBla9LAJ2},
	abstract = {Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to *automatically discover equivariances* from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group \${\textbackslash}mathrm\{SO\}(n)\$, restricted Lorentz group \${\textbackslash}mathrm\{SO\}(1,3)ˆ+\$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.},
	urldate = {2023-08-18},
	author = {Yang, Jianke and Walters, Robin and Dehmamy, Nima and Yu, Rose},
	month = jun,
	year = {2023},
	keywords = {⛔ No DOI found},
}

@patent{cahill_art_1897,
	title = {Art of and apparatus for generating and distributing music electrically},
	url = {https://patents.google.com/patent/US580035A/en},
	nationality = {US},
	number = {US580035A},
	urldate = {2023-08-18},
	author = {Cahill, Thaddeus},
	month = apr,
	year = {1897},
	keywords = {electrical, key, pitch, rheotome, vibrations},
}

@patent{ssergejewitsch_method_1928-1,
	title = {Method of and apparatus for the generation of sounds},
	url = {https://patents.google.com/patent/US1661058A/en},
	nationality = {US},
	assignee = {FIRM OF M J GOLDBERG und SOHNE},
	number = {US1661058A},
	urldate = {2023-08-18},
	author = {Ssergejewitsch, Theremin Leo},
	month = feb,
	year = {1928},
	keywords = {circuit, electrode, reproducer, sound, tones},
}

@misc{luo_towards_2023,
	title = {Towards {Symmetry}-{Aware} {Generation} of {Periodic} {Materials}},
	url = {http://arxiv.org/abs/2307.02707},
	abstract = {We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Luo, Youzhi and Liu, Chengkai and Ji, Shuiwang},
	month = jul,
	year = {2023},
	doi = {10.48550/arXiv.2307.02707},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science},
}

@article{yang_latent_2023,
	title = {Latent {Space} {Symmetry} {Discovery}},
	url = {https://openreview.net/forum?id=z3SHey9hK1},
	abstract = {Existing equivariant neural networks require explicit knowledge of the symmetry group before model implementation. Various symmetry discovery methods have been developed to learn invariance and equivariance from data, but their search spaces are limited to linear symmetries. We propose to discover arbitrary nonlinear symmetries by factorizing the group action into nonlinear transformations parameterized by an autoencoder network and linear symmetries generated by an existing symmetry discovery framework, LieGAN. Our method can capture the intrinsic symmetry in high-dimensional observations, which also results in a well-structured latent space that is useful for other downstream tasks, including long-term prediction and latent space equation discovery.},
	urldate = {2023-08-18},
	author = {Yang, Jianke and Dehmamy, Nima and Walters, Robin and Yu, Rose},
	month = jun,
	year = {2023},
	keywords = {⛔ No DOI found},
}

@patent{ssergejewitsch_method_1928,
	title = {Method of and apparatus for the generation of sounds},
	url = {https://patents.google.com/patent/US1661058A/en},
	nationality = {US},
	assignee = {FIRM OF M J GOLDBERG und SOHNE},
	number = {US1661058A},
	urldate = {2023-08-18},
	author = {Ssergejewitsch, Theremin Leo},
	month = feb,
	year = {1928},
	keywords = {circuit, electrode, reproducer, sound, tones},
}

@inproceedings{pons_upsampling_2021,
	address = {Toronto, ON, Canada},
	title = {Upsampling {Artifacts} in {Neural} {Audio} {Synthesis}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414913/},
	doi = {10.1109/ICASSP39728.2021.9414913},
	urldate = {2023-08-21},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Pons, Jordi and Pascual, Santiago and Cengarle, Giulio and Serra, Joan},
	month = jun,
	year = {2021},
	pages = {3005--3009},
}

@misc{kato_differentiable_2020,
	title = {Differentiable {Rendering}: {A} {Survey}},
	shorttitle = {Differentiable {Rendering}},
	url = {http://arxiv.org/abs/2006.12057},
	abstract = {Deep neural networks (DNNs) have shown remarkable performance improvements on vision-related tasks such as object detection or image segmentation. Despite their success, they generally lack the understanding of 3D objects which form the image, as it is not always possible to collect 3D information about the scene or to easily annotate it. Differentiable rendering is a novel field which allows the gradients of 3D objects to be calculated and propagated through images. It also reduces the requirement of 3D data collection and annotation, while enabling higher success rate in various applications. This paper reviews existing literature and discusses the current state of differentiable rendering, its applications and open research problems.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2006.12057},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{schlecht_allpass_2021,
	title = {Allpass {Feedback} {Delay} {Networks}},
	volume = {69},
	issn = {1053-587X, 1941-0476},
	url = {https://ieeexplore.ieee.org/document/9332286/},
	doi = {10.1109/TSP.2021.3053507},
	urldate = {2023-08-24},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schlecht, Sebastian J.},
	year = {2021},
	pages = {1028--1038},
}

@article{jack_action-sound_2018,
	title = {Action-sound latency and the perceived quality of digital musical instruments},
	volume = {36},
	issn = {0730-7829, 1533-8312},
	url = {https://online.ucpress.edu/mp/article/36/1/109/92063/Actionsound-Latency-and-the-Perceived-Quality-of},
	doi = {10.1525/mp.2018.36.1.109},
	abstract = {Asynchrony between tactile and auditory feedback (action-sound latency) when playing a musical instrument is widely recognized as disruptive to musical performance. In this paper we present a study that assesses the effects of delayed auditory feedback on the timing accuracy and judgments of instrument quality for two groups of participants: professional percussionists and non-percussionist amateur musicians. The amounts of delay tested in this study are relatively small in comparison to similar studies of auditory delays in a musical context (0 ms, 10 ms, 10 ms ± 3 ms, 20 ms). We found that both groups rated the zero latency condition as higher quality for a series of quality measures in comparison to 10 ms ± 3 ms and 20 ms latency, but did not show a significant difference in rating between 10 ms latency and zero latency. Professional percussionists were more aware of the latency conditions and showed less variation of timing under the latency conditions, although this ability decreased as the temporal demands of the task increased. We compare our findings from each group and discuss them in relation to latency in interactive digital systems more generally and experimentally similar work on sensorimotor control and rhythmic performance.},
	number = {1},
	urldate = {2023-08-26},
	journal = {Music Perception},
	author = {Jack, Robert H. and Mehrabi, Adib and Stockman, Tony and McPherson, Andrew},
	month = sep,
	year = {2018},
	pages = {109--128},
}

@inproceedings{devis_continuous_2023,
	address = {Rhodes Island, Greece},
	title = {Continuous {Descriptor}-{Based} {Control} for {Deep} {Audio} {Synthesis}},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10096670/},
	doi = {10.1109/ICASSP49357.2023.10096670},
	urldate = {2023-08-26},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Devis, Ninon and Demerlé, Nils and Nabi, Sarah and Genova, David and Esling, Philippe},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@book{holmes_electronic_2008,
	address = {New York},
	edition = {3rd ed},
	title = {Electronic and experimental music: technology, music, and culture},
	isbn = {978-0-415-95781-6 978-0-415-95782-3 978-0-203-92959-9},
	shorttitle = {Electronic and experimental music},
	publisher = {Routledge},
	author = {Holmes, Thom},
	year = {2008},
	keywords = {Computer music, Electronic music, History and criticism},
}

@inproceedings{huang_counterpoint_2017,
	title = {Counterpoint by {Convolution}},
	url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/187%5FPaper.pdf},
	booktitle = {Proceedings of {ISMIR} 2017},
	author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@book{bregman_auditory_1990,
	title = {Auditory {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}},
	isbn = {978-0-262-26920-9},
	shorttitle = {Auditory {Scene} {Analysis}},
	url = {https://direct.mit.edu/books/book/3887/auditory-scene-analysisthe-perceptual-organization},
	urldate = {2023-08-23},
	publisher = {The MIT Press},
	author = {Bregman, Albert S.},
	year = {1990},
	doi = {10.7551/mitpress/1486.001.0001},
}

@article{valimaki_late_2017,
	title = {Late {Reverberation} {Synthesis} {Using} {Filtered} {Velvet} {Noise}},
	volume = {7},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/7/5/483},
	doi = {10.3390/app7050483},
	number = {5},
	urldate = {2023-08-24},
	journal = {Applied Sciences},
	author = {Välimäki, Vesa and Holm-Rasmussen, Bo and Alary, Benoit and Lehtonen, Heidi-Maria},
	month = may,
	year = {2017},
	pages = {483},
}

@inproceedings{lee_blind_2023,
	address = {Rhodes Island, Greece},
	title = {Blind {Estimation} of {Audio} {Processing} {Graph}},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10096581/},
	doi = {10.1109/ICASSP49357.2023.10096581},
	urldate = {2023-08-27},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Lee, Sungho and Park, Jaehyun and Paik, Seungryeol and Lee, Kyogu},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@misc{ziyin_neural_2020,
	title = {Neural networks {Fail} to learn {Periodic} {Functions} and how to fix {It}},
	abstract = {Previous literature offers limited clues on how to learn a periodic function using modern neural networks. We start with a study of the extrapolation properties of neural networks; we prove and demonstrate experimentally that the standard activations functions, such as ReLU, tanh, sigmoid, along with their variants, all fail to learn to extrapolate simple periodic functions. We hypothesize that this is due to their lack of a "periodic" inductive bias. As a fix of this problem, we propose a new activation, namely, \$x + {\textbackslash}sin2̂(x)\$, which achieves the desired periodic inductive bias to learn a periodic function while maintaining a favorable optimization property of the ReLU-based activations. Experimentally, we apply the proposed method to temperature and financial data prediction.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Ziyin, Liu and Hartwig, Tilman and Ueda, Masahito},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.08195 [cs, stat]
Issue: arXiv:2006.08195},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhao_transferring_2019,
	title = {Transferring neural speech waveform synthesizers to musical instrument sounds generation},
	abstract = {Recent neural waveform synthesizers such as WaveNet, WaveGlow, and the neural-source-filter (NSF) model have shown good performance in speech synthesis despite their different methods of waveform generation. The similarity between speech and music audio synthesis techniques suggests interesting avenues to explore in terms of the best way to apply speech synthesizers in the music domain. This work compares three neural synthesizers used for musical instrument sounds generation under three scenarios: training from scratch on music data, zero-shot learning from the speech domain, and fine-tuning-based adaptation from the speech to the music domain. The results of a large-scale perceptual test demonstrated that the performance of three synthesizers improved when they were pre-trained on speech data and fine-tuned on music data, which indicates the usefulness of knowledge from speech data for music audio generation. Among the synthesizers, WaveGlow showed the best potential in zero-shot learning while NSF performed best in the other scenarios and could generate samples that were perceptually close to natural audio.},
	urldate = {2023-08-14},
	publisher = {arXiv},
	author = {Zhao, Yi and Wang, Xin and Juvela, Lauri and Yamagishi, Junichi},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.12381 [cs, eess, stat]
Issue: arXiv:1910.12381},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{yu_singing_2023,
	title = {Singing voice {Synthesis} {Using} {Differentiable} {LPC} and glottal-{Flow}-{Inspired} {Wavetables}},
	abstract = {This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis. Audio samples are available at https://yoyololicon.github.io/golf-demo/.},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Yu, Chin-Yun and Fazekas, György},
	month = jun,
	year = {2023},
	note = {arXiv: 2306.17252 [cs, eess]
Issue: arXiv:2306.17252},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{yu_durian_2020,
	title = {{DurIAN}: {Duration} {Informed} {Attention} {Network} for speech {Synthesis}},
	shorttitle = {{DurIAN}},
	doi = {10.21437/Interspeech.2020-2968},
	language = {english},
	urldate = {2023-08-27},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Yu, Chengzhu and Lu, Heng and Hu, Na and Yu, Meng and Weng, Chao and Xu, Kun and Liu, Peng and Tuo, Deyi and Kang, Shiyin and Lei, Guangzhi and Su, Dan and Yu, Dong},
	month = oct,
	year = {2020},
	pages = {2027--2031},
}

@article{zuiderveld_towards_2021,
	title = {Towards lightweight {Controllable} {Audio} {Synthesis} with conditional {Implicit} {Neural} {Representations}},
	abstract = {The high temporal resolution of audio and our perceptual sensitivity to small irregularities in waveforms make synthesizing at high sampling rates a complex and computationally intensive task, prohibiting real-time, controllable synthesis within many approaches. In this work we aim to shed light on the potential of Conditional Implicit Neural Representations (CINRs) as lightweight backbones in generative frameworks for audio synthesis. Our experiments show that small Periodic Conditional INRs (PCINRs) learn faster and generally produce quantitatively better audio reconstructions than Transposed Convolutional Neural Networks with equal parameter counts. However, their performance is very sensitive to activation scaling hyperparameters. When learning to represent more uniform sets, PCINRs tend to introduce artificial high-frequency components in reconstructions. We validate this noise can be minimized by applying standard weight regularization during training or decreasing the compositional depth of PCINRs, and suggest directions for future research.},
	urldate = {2022-03-12},
	journal = {arXiv:2111.08462 [cs]},
	author = {Zuiderveld, Jan and Federici, Marco and Bekkers, Erik J.},
	month = dec,
	year = {2021},
	note = {arXiv: 2111.08462 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, ⛔ No DOI found},
}

@misc{zhou_permutation_2023,
	title = {Permutation {Equivariant} {Neural} {Functionals}},
	abstract = {This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate parameter sharing scheme. In our experiments, we find that permutation equivariant neural functionals are effective on a diverse set of tasks that require processing the weights of MLPs and CNNs, such as predicting classifier generalization, producing "winning ticket" sparsity masks for initializations, and editing the weights of implicit neural representations (INRs). In addition, we provide code for our models and experiments at https://github.com/AllanYangZhou/nfn.},
	urldate = {2023-08-25},
	publisher = {arXiv},
	author = {Zhou, Allan and Yang, Kaien and Burns, Kaylee and Jiang, Yiding and Sokota, Samuel and Kolter, J. Zico and Finn, Chelsea},
	month = feb,
	year = {2023},
	note = {arXiv: 2302.14040 [cs]
Issue: arXiv:2302.14040},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{zhou_hifi-svc_2022,
	title = {{HiFi}-{SVC}: {Fast} {High} {Fidelity} {Cross}-{Domain} {Singing} {Voice} {Conversion}},
	shorttitle = {{HiFi}-{SVC}},
	doi = {10.1109/ICASSP43922.2022.9746812},
	abstract = {This paper presents HiFi-SVC, a small cross-domain singing voice conversion model for generating high-fidelity 22.05 kHz singing voices. Building on state-of-the-art neural vocoder HiFi-GAN and a convolution-based module for modeling F0, HiFi-SVC can be trained end-to-end with either speech or singing data, achieving better voice similarity on two of the datasets than FastSVC while using slightly smaller number of parameters. We also propose a pitch adjustment method for improving conversion quality.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Zhou, Yong and Lu, Xiangju},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Buildings, Conferences, Convolution, Data models, Singing voice conversion, Speech processing, Vocoders, phonetic posteriorgrams, pitch modelling},
	pages = {6667--6671},
}

@article{zen_statistical_2009,
	title = {Statistical parametric speech synthesis},
	volume = {51},
	issn = {01676393},
	doi = {10.1016/j.specom.2009.04.004},
	abstract = {This review gives a general overview of techniques used in statistical parametric speech synthesis. One instance of these techniques, called hidden Markov model (HMM)-based speech synthesis, has recently been demonstrated to be very effective in synthesizing acceptable speech. This review also contrasts these techniques with the more conventional technique of unit-selection synthesis that has dominated speech synthesis over the last decade. The advantages and drawbacks of statistical parametric synthesis are highlighted and we identify where we expect key developments to appear in the immediate future.},
	number = {11},
	urldate = {2023-08-15},
	journal = {Speech Communication},
	author = {Zen, Heiga and Tokuda, Keiichi and Black, Alan W.},
	month = nov,
	year = {2009},
	pages = {1039--1064},
}

@misc{yu_durian_2019,
	title = {{DurIAN}: {Duration} {Informed} {Attention} {Network} {For} {Multimodal} {Synthesis}},
	shorttitle = {{DurIAN}},
	abstract = {In this paper, we present a generic and robust multimodal synthesis system that produces highly natural speech and facial expression simultaneously. The key component of this system is the Duration Informed Attention Network (DurIAN), an autoregressive model in which the alignments between the input text and the output acoustic features are inferred from a duration model. This is different from the end-to-end attention mechanism used, and accounts for various unavoidable artifacts, in existing end-to-end speech synthesis systems such as Tacotron. Furthermore, DurIAN can be used to generate high quality facial expression which can be synchronized with generated speech with/without parallel speech and face data. To improve the efficiency of speech generation, we also propose a multi-band parallel generation strategy on top of the WaveRNN model. The proposed Multi-band WaveRNN effectively reduces the total computational complexity from 9.8 to 5.5 GFLOPS, and is able to generate audio that is 6 times faster than real time on a single CPU core. We show that DurIAN could generate highly natural speech that is on par with current state of the art end-to-end systems, while at the same time avoid word skipping/repeating errors in those systems. Finally, a simple yet effective approach for fine-grained control of expressiveness of speech and facial expression is introduced.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Yu, Chengzhu and Lu, Heng and Hu, Na and Yu, Meng and Weng, Chao and Xu, Kun and Liu, Peng and Tuo, Deyi and Kang, Shiyin and Lei, Guangzhi and Su, Dan and Yu, Dong},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01700 [cs, eess]
Issue: arXiv:1909.01700},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{yee-king_studio_nodate,
	title = {Studio report: {Sound} synthesis with {DDSP} and network bending techniques},
	abstract = {This paper reports on our experiences synthesizing sounds and building network bending functionality onto the Differentiable Digital Signal Processing (DDSP) system. DDSP is an extension to the TensorFlow API with which we can embed trainable signal processing nodes in neural networks. Comparing DDSP sound synthesis networks to preset finding networks and sample level synthesis networks, we argue that it offers a third mode of working, providing continuous control in real-time of high fidelity synthesizers using low numbers of control parameters. We describe two phases of our experimentation. Firstly we worked with a composer to explore different training datasets and parameters. Secondly, we extended DDSP models with network bending functionality, which allows us to feed additional control data into the network's hidden layers and achieve new timbral effects. We describe several possible network bending techniques and how they affect the sound.},
	author = {Yee-King, Matthew and McCallum, Louis},
	keywords = {⛔ No DOI found},
}

@misc{yamamoto_probability_2019,
	title = {Probability density distillation with generative adversarial networks for high-quality parallel waveform generation},
	abstract = {This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
	month = aug,
	year = {2019},
	note = {arXiv: 1904.04472 [cs, eess]
Issue: arXiv:1904.04472},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{yamamoto_parallel_2020,
	title = {Parallel wavegan: {A} {Fast} {Waveform} {Generation} {Model} {Based} on generative {Adversarial} {Networks} with multi-{Resolution} {Spectrogram}},
	shorttitle = {Parallel {Wavegan}},
	doi = {10.1109/ICASSP40776.2020.9053795},
	abstract = {We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high-fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Generative adversarial networks, Graphics processing units, Neural networks, Neural vocoder, Parallel WaveNet, Real-time systems, Spectrogram, Speech processing, Time-frequency analysis, Transformer, generative adversarial networks, text-to-speech},
	pages = {6199--6203},
}

@article{wu_quasi-periodic_2021,
	title = {Quasi-periodic {WaveNet}: {An} {Autoregressive} {Raw} {Waveform} {Generative} {Model} {With} {Pitch}-{Dependent} {Dilated} {Convolution} {Neural} {Network}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Quasi-{Periodic} {WaveNet}},
	doi = {10.1109/TASLP.2021.3061245},
	urldate = {2023-06-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Yi-Chiao and Hayashi, Tomoki and Tobing, Patrick Lumban and Kobayashi, Kazuhiro and Toda, Tomoki},
	year = {2021},
	pages = {1134--1148},
}

@inproceedings{yoshimura_simultaneous_1999,
	title = {Simultaneous modeling of spectrum, pitch and duration in {HMM}-based speech synthesis},
	doi = {10.21437/Eurospeech.1999-513},
	abstract = {In this paper, we describe an HMM-based speech synthesis system in which spectrum, pitch and state duration are modeled simultaneously in a unified framework of HMM. In the system, pitch and state duration are modeled by multi-space probability distribution HMMs and multi-dimensional Gaussian distributions, respectively. The distributions for spectral parameter, pitch parameter and the state duration are clustered independently by using a decision-tree based context clustering technique. Synthetic speech is generated by using an speech parameter generation algorithm from HMM and a mel-cepstrum based vocoding technique. Through informal listening tests, we have confirmed that the proposed system successfully synthesizes natural-sounding speech which resembles the speaker in the training database.},
	urldate = {2023-07-24},
	booktitle = {6th european {Conference} on speech {Communication} and technology ({Eurospeech} 1999)},
	publisher = {ISCA},
	author = {Yoshimura, Takayoshi and Tokuda, Keiichi and Masuko, Takashi and Kobayashi, Takao and Kitamura, Tadashi},
	month = sep,
	year = {1999},
	pages = {2347--2350},
}

@inproceedings{yoshimura_embedding_2023,
	title = {Embedding a differentiable {Mel}-{Cepstral} {Synthesis} {Filter} to a neural {Speech} {Synthesis} {System}},
	doi = {10.1109/ICASSP49357.2023.10094872},
	abstract = {This paper integrates a classic mel-cepstral synthesis filter into a modern neural speech synthesis system towards end-to-end controllable speech synthesis. Since the mel-cepstral synthesis filter is explicitly embedded in neural waveform models in the proposed system, both voice characteristics and the pitch of synthesized speech are highly controlled via a frequency warping parameter and fundamental frequency, respectively. We implement the mel-cepstral synthesis filter as a differentiable and GPU-friendly module to enable the acoustic and waveform models in the proposed system to be simultaneously optimized in an end-to-end manner. Experiments show that the proposed system improves speech quality from a baseline system maintaining controllability. The core PyTorch modules used in the experiments are publicly available on GitHub1.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Yoshimura, Takenori and Takaki, Shinji and Nakamura, Kazuhiro and Oura, Keiichiro and Hono, Yukiya and Hashimoto, Kei and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = jun,
	year = {2023},
	keywords = {Acoustics, Controllability, Frequency synthesizers, Generative adversarial networks, MLSA filter, Network architecture, Signal processing, Speech synthesis, Training, end-to-end training, mel-cepstrum, neural networks},
	pages = {1--5},
}

@article{yee-king_automatic_2018,
	title = {Automatic programming of {VST} {Sound} {Synthesizers} {Using} {Deep} {Networks} and other {Techniques}},
	volume = {2},
	issn = {2471-285X},
	doi = {10.1109/TETCI.2017.2783885},
	abstract = {Programming sound synthesizers is a complex and time-consuming task. Automatic synthesizer programming involves finding parameters for sound synthesizers using algorithmic methods. Sound matching is one application of automatic programming, where the aim is to find the parameters for a synthesizer that cause it to emit as close a sound as possible to a target sound. We describe and compare several sound matching techniques that can be used to automatically program the Dexed synthesizer, which is a virtual model of a Yamaha DX7. The techniques are a hill climber, a genetic algorithm and three deep neural networks that have not been applied to the problem before. We define a sound matching task based on six sets of sounds, which we derived from increasingly complex configurations of the Dexed synthesis algorithm. A bidirectional, long short-term memory network (LSTM) with highway layers performed better than any other technique and was able to match sounds closely in 25\% of the test cases. This network was also able to match sounds in near real time, once trained, which provides a significant speed advantage over previously reported techniques that are based on search heuristics. We also describe our open source framework which makes it possible to repeat our study, and to adapt it to different synthesizers and algorithmic programming techniques.},
	number = {2},
	urldate = {2020-07-04},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Yee-King, Matthew John and Fedden, Leon and d'Inverno, Mark},
	month = apr,
	year = {2018},
	pages = {150--159},
}

@misc{ye_nas-fm_2023,
	title = {{NAS}-{FM}: {Neural} {Architecture} {Search} for tunable and interpretable {Sound} {Synthesis} based on frequency {Modulation}},
	shorttitle = {{NAS}-{FM}},
	abstract = {Developing digital sound synthesizers is crucial to the music industry as it provides a low-cost way to produce high-quality sounds with rich timbres. Existing traditional synthesizers often require substantial expertise to determine the overall framework of a synthesizer and the parameters of submodules. Since expert knowledge is hard to acquire, it hinders the flexibility to quickly design and tune digital synthesizers for diverse sounds. In this paper, we propose “NAS-FM”, which adopts neural architecture search (NAS) to build a differentiable frequency modulation (FM) synthesizer. Tunable synthesizers with interpretable controls can be developed automatically from sounds without any prior expert knowledge and manual operating costs. In detail, we train a supernet with a specifically designed search space, including predicting the envelopes of carriers and modulators with different frequency ratios. An evolutionary search algorithm with adaptive oscillator size is then developed to find the optimal relationship between oscillators and the frequency ratio of FM. Extensive experiments on recordings of different instrument sounds show that our algorithm can build a synthesizer fully automatically, achieving better results than handcrafted synthesizers. Audio samples are available at https://nas-fm.github.io/.},
	urldate = {2023-06-03},
	author = {Ye, Zhen and Xue, Wei and Tan, Xu and Liu, Qifeng and Guo, Yike},
	month = may,
	year = {2023},
	doi = {10.48550/arXiv.2305.12868},
	note = {arXiv: 2305.12868 [cs, eess]
Issue: arXiv:2305.12868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wu_midi-ddsp_2022,
	title = {{MIDI}-{DDSP}: {Detailed} control of musical performance via hierarchical modeling},
	booktitle = {International conference on learning representations},
	author = {Wu, Yusong and Manilow, Ethan and Deng, Yi and Swavely, Rigel and Kastner, Kyle and Cooijmans, Tim and Courville, Aaron and Huang, Cheng-Zhi Anna and Engel, Jesse},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@misc{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.10135 [cs]
Issue: arXiv:1703.10135},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@inproceedings{wang_synthetic_2023,
	title = {A {Synthetic} {Corpus} {Generation} {Method} for {Neural} {Vocoder} {Training}},
	doi = {10.1109/ICASSP49357.2023.10094786},
	abstract = {Nowadays, neural vocoders are preferred for their ability to synthesize high-fidelity audio. However, training a neural vocoder requires a massive corpus of high-quality real audio, and the audio recording process is often labor-intensive. In this work, we propose a synthetic corpus generation method for neural vocoder training, which can easily generate synthetic audio with an unlimited number at nearly no cost. We explicitly model the prior characteristics of audio from multiple target domains simultaneously (e.g., speeches, singing voices, and instrumental pieces) to equip the generated audio data with these characteristics. And we show that our synthetic corpus allows the neural vocoder to achieve competitive results without any real audio in the training process. To validate the effectiveness of our proposed method, we performed empirical experiments on both speech and music utterances in subjective and objective metrics. The experimental results show that the neural vocoder trained with the synthetic corpus produced by our method can generalize to multiple target scenarios and has excellent singing voice (MOS: 4.20) and instrumental piece (MOS: 4.00) synthesis results.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Wang, Zilin and Liu, Peng and Chen, Jun and Li, Sipan and Bai, Jinfeng and He, Gang and Wu, Zhiyong and Meng, Helen},
	month = jun,
	year = {2023},
	keywords = {Audio recording, Costs, Instruments, Measurement, Signal processing, Training, Vocoders, neural vocoder, speech synthesis, synthetic corpus},
	pages = {1--5},
}

@inproceedings{wang_style_2018,
	title = {Style tokens: {Unsupervised} {Style} {Modeling}, control and transfer in end-to-{End} {Speech} {Synthesis}},
	shorttitle = {Style {Tokens}},
	abstract = {In this work, we propose “global style tokens” (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable “labels” they generate can be used to control synthesis in novel ways, such as varying speed and speaking style – independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 35th international {Conference} on machine {Learning}},
	publisher = {PMLR},
	author = {Wang, Yuxuan and Stanton, Daisy and Zhang, Yu and Ryan, RJ-Skerry and Battenberg, Eric and Shor, Joel and Xiao, Ying and Jia, Ye and Ren, Fei and Saurous, Rif A.},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5180--5189},
}

@inproceedings{wu_generating_2022,
	address = {Baltimore, Maryland, USA},
	title = {Generating {Detailed} {Music} {Datasets} with {Neural} {Audio} {Synthesis}},
	volume = {162},
	abstract = {Generative models are increasingly able to generate realistic high-quality data with systematic control over conditional attributes. These models are ideally suited for creating voluminous and detailed synthetic datasets, which has led to large improvements in low-resource tasks in language and vision. However, such generative data amplification has yet to be demonstrated for the domain of music (symbolic music (i.e. MIDI) or raw audio). In this work, we address this gap by using a generative model of MIDI (Coconet trained on Bach Chorales) with a structured audio synthesis model (MIDI-DDSP trained on URMP). We demonstrate a system capable of producing unlimited amounts of realistic chorale music with rich annotations through controlled synthesis of MIDI through generative models. We call this system the Chamber Ensemble Generator (CEG), and use it to generate a large dataset of chorales (CocoChorales). We demonstrate that data generated using our approach improves state-of-the-art models for music transcription and source separation, and we release both the system and the dataset as an open-source foundation for future work.},
	booktitle = {Proceedings of the 39th international {Conference} on machine {Learning}},
	author = {Wu, Yusong and Gardner, Joshua and Manilow, Ethan and Simon, Ian and Hawthorne, Curtis and Engel, Jesse},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@misc{wu_ddsp-based_2022,
	title = {{DDSP}-based {Singing} {Vocoders}: {A} {New} {Subtractive}-based {Synthesizer} and a {Comprehensive} {Evaluation}},
	shorttitle = {{DDSP}-based {Singing} {Vocoders}},
	abstract = {A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our experiments show that SawSing converges much faster and outperforms state-of-the-art generative adversarial network and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Wu, Da-Yi and Hsiao, Wen-Yi and Yang, Fu-Rong and Friedman, Oscar and Jackson, Warren and Bruzenak, Scott and Liu, Yi-Wen and Yang, Yi-Hsuan},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2208.04756},
	note = {arXiv: 2208.04756 [cs, eess]
Issue: arXiv:2208.04756},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wright_real-time_2019,
	title = {Real-time black-box modelling with recurrent neural networks},
	booktitle = {22nd international conference on digital audio effects ({DAFx}-19)},
	author = {Wright, Alec and Damskägg, Eero-Pekka and Välimäki, Vesa and {others}},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {1--8},
}

@article{wilmering_history_2020,
	title = {A {History} of {Audio} {Effects}},
	volume = {10},
	issn = {2076-3417},
	doi = {10.3390/app10030791},
	abstract = {Audio effects are an essential tool that the field of music production relies upon. The ability to intentionally manipulate and modify a piece of sound has opened up considerable opportunities for music making. The evolution of technology has often driven new audio tools and effects, from early architectural acoustics through electromechanical and electronic devices to the digitisation of music production studios. Throughout time, music has constantly borrowed ideas and technological advancements from all other fields and contributed back to the innovative technology. This is defined as transsectorial innovation and fundamentally underpins the technological developments of audio effects. The development and evolution of audio effect technology is discussed, highlighting major technical breakthroughs and the impact of available audio effects.},
	number = {3},
	urldate = {2023-07-28},
	journal = {Applied Sciences},
	author = {Wilmering, Thomas and Moffat, David and Milo, Alessia and Sandler, Mark B.},
	month = jan,
	year = {2020},
	pages = {791},
}

@inproceedings{webber_autovocoder_2023,
	title = {Autovocoder: {Fast} {Waveform} {Generation} from a learned {Speech} {Representation} {Using} {Differentiable} {Digital} {Signal} {Processing}},
	shorttitle = {Autovocoder},
	doi = {10.1109/ICASSP49357.2023.10095729},
	abstract = {Most state-of-the-art Text-to-Speech systems use the mel-spectrogram as an intermediate representation, to decompose the task into acoustic modelling and waveform generation.A mel-spectrogram is extracted from the waveform by a simple, fast DSP operation, but generating a high-quality waveform from a mel-spectrogram requires computationally expensive machine learning: a neural vocoder. Our proposed "autovocoder" reverses this arrangement. We use machine learning to obtain a representation that replaces the mel-spectrogram, and that can be inverted back to a waveform using simple, fast operations including a differentiable implementation of the inverse STFT.The autovocoder generates a waveform 5 times faster than the DSP-based Griffin-Lim algorithm, and 14 times faster than the neural vocoder HiFi-GAN. We provide perceptual listening test results to confirm that the speech is of comparable quality to HiFi-GAN in the copy synthesis task.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Webber, Jacob J and Valentini-Botinhao, Cassia and Williams, Evelyn and Henter, Gustav Eje and King, Simon},
	month = jun,
	year = {2023},
	keywords = {Machine learning, Machine learning algorithms, Signal processing algorithms, Speech coding, Synthesizers, Training, Vocoders, differentiable DSP, neural vocoder, representation learning, speech synthesis},
	pages = {1--5},
}

@inproceedings{watts_puffin_2023,
	address = {Rhodes Island, Greece},
	title = {{PUFFIN}: {Pitch}-{Synchronous} {Neural} {Waveform} {Generation} for fullband {Speech} on modest {Devices}},
	isbn = {978-1-72816-327-7},
	shorttitle = {{PUFFIN}},
	doi = {10.1109/ICASSP49357.2023.10094729},
	urldate = {2023-06-21},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Watts, Oliver and Wihlborg, Lovisa and Valentini-Botinhao, Cassia},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@misc{wang_using_2020,
	title = {Using cyclic {Noise} as the source {Signal} for neural {Source}-{Filter}-based {Speech} {Waveform} {Model}},
	abstract = {Neural source-filter (NSF) waveform models generate speech waveforms by morphing sine-based source signals through dilated convolution in the time domain. Although the sine-based source signals help the NSF models to produce voiced sounds with specified pitch, the sine shape may constrain the generated waveform when the target voiced sounds are less periodic. In this paper, we propose a more flexible source signal called cyclic noise, a quasi-periodic noise sequence given by the convolution of a pulse train and a static random noise with a trainable decaying rate that controls the signal shape. We further propose a masked spectral loss to guide the NSF models to produce periodic voiced sounds from the cyclic noise-based source signal. Results from a large-scale listening test demonstrated the effectiveness of the cyclic noise and the masked spectral loss on speaker-independent NSF models in copy-synthesis experiments on the CMU ARCTIC database.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Wang, Xin and Yamagishi, Junichi},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.02191 [eess]
Issue: arXiv:2004.02191},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wang_neural_2019,
	title = {Neural harmonic-plus-{Noise} {Waveform} {Model} with trainable {Maximum} {Voice} {Frequency} for text-to-{Speech} {Synthesis}},
	doi = {10.21437/SSW.2019-1},
	abstract = {Neural source-filter (NSF) models are deep neural networks that produce waveforms given input acoustic features. They use dilated-convolution-based neural filter modules to filter sinebased excitation for waveform generation, which is different from WaveNet and flow-based models. One of the NSF models, called harmonic-plus-noise NSF (h-NSF) model, uses separate pairs of source and neural filters to generate harmonic and noise waveform components. It is close to WaveNet in terms of speech quality while being superior in generation speed.},
	urldate = {2023-07-04},
	booktitle = {10th {ISCA} {Workshop} on {Speech} {Synthesis} ({SSW} 10)},
	publisher = {ISCA},
	author = {Wang, Xin and Yamagishi, Junichi},
	month = sep,
	year = {2019},
	pages = {1--6},
}

@book{wang_computational_2006,
	address = {Piscataway, N.J. : Hoboken, N.J},
	title = {Computational auditory scene analysis: {Principles}, algorithms, and applications},
	isbn = {978-0-471-74109-1},
	shorttitle = {Computational auditory scene analysis},
	publisher = {IEEE Press ; Wiley Interscience},
	editor = {Wang, DeLiang and Brown, Guy J.},
	year = {2006},
	note = {tex.lccn: QP461 .C645 2006},
	keywords = {Auditory perception, Computational auditory scene analysis, Computer simulation},
}

@inproceedings{wang_comparison_2018,
	title = {A comparison of recent {Waveform} {Generation} and acoustic {Modeling} {Methods} for neural-{Network}-{Based} {Speech} {Synthesis}},
	doi = {10.1109/ICASSP.2018.8461452},
	abstract = {Recent advances in speech synthesis suggest that limitations such as the lossy nature of the amplitude spectrum with minimum phase approximation and the over-smoothing effect in acoustic modeling can be overcome by using advanced machine learning approaches. In this paper, we build a framework in which we can fairly compare new vocoding and acoustic modeling techniques with conventional approaches by means of a large scale crowdsourced evaluation. Results on acoustic models showed that generative adversarial networks and an autoregressive (AR) model performed better than a normal recurrent network and the AR model performed best. Evaluation on vocoders by using the same AR acoustic model demonstrated that a Wavenet vocoder outperformed classical source-filter-based vocoders. Particularly, generated speech waveforms from the combination of AR acoustic model and Wavenet vocoder achieved a similar score of speech quality to vocoded speech.},
	booktitle = {2018 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Wang, Xin and Lorenzo-Trueba, Jaime and Takaki, Shinji and Juvela, Lauri and Yamagishi, Junichi},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Artificial neural networks, Feature extraction, Gallium nitride, Linguistics, Speech synthesis, Vocoders, Wavenet, autoregressive neural network, deep learning, general adversarial network, speech synthesis},
	pages = {4804--4808},
}

@misc{wang_opencpop_2022,
	title = {Opencpop: {A} {High}-{Quality} {Open} {Source} {Chinese} {Popular} {Song} {Corpus} for singing {Voice} {Synthesis}},
	shorttitle = {Opencpop},
	abstract = {This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Wang, Yu and Wang, Xinsheng and Zhu, Pengcheng and Wu, Jie and Li, Hanzhao and Xue, Heyang and Zhang, Yongmao and Xie, Lei and Bi, Mengxiao},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.07429},
	note = {arXiv: 2201.07429 [cs, eess]
Issue: arXiv:2201.07429},
	keywords = {Computer Science - Databases, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{wang_neural_2019-2,
	title = {Neural source-{Filter} {Waveform} {Models} for statistical {Parametric} {Speech} {Synthesis}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	doi = {10.1109/TASLP.2019.2956145},
	urldate = {2021-03-29},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = nov,
	year = {2019},
	pages = {402--415},
}

@inproceedings{wang_neural_2019-1,
	address = {Brighton, United Kingdom},
	title = {Neural source-filter-based {Waveform} {Model} for statistical {Parametric} {Speech} {Synthesis}},
	isbn = {978-1-4799-8131-1},
	doi = {10.1109/ICASSP.2019.8682298},
	urldate = {2023-08-18},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = may,
	year = {2019},
	pages = {5916--5920},
}

@article{wang_autoregressive_2018,
	title = {Autoregressive neural {F0} {Model} for statistical {Parametric} {Speech} {Synthesis}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2018.2828650},
	abstract = {Recurrent neural networks (RNNs) have been successfully used as fundamental frequency (F0) models for text-to-speech synthesis. However, this paper showed that a normal RNN may not take into account the statistical dependency of the F0 data across frames and consequently only generate noisy F0 contours when F0 values are sampled from the model. A better model may take into account the causal dependency of the current F0 datum on the previous frames' F0 data. One such model is the shallow autoregressive (AR) recurrent mixture density network (SAR) that we recently proposed. However, as this study showed, an SAR is equivalent to the combination of trainable linear filters and a conventional RNN. It is still weak for F0 modeling. To better model the temporal dependency in F0 contours, we propose a deep AR model (DAR). On the basis of an RNN, this DAR propagates the previous frame's F0 value through the RNN, which allows nonlinear AR dependency to be achieved. We also propose F0 quantization and data dropout strategies for the DAR. Experiments on a Japanese corpus demonstrated that this DAR can generate appropriate F0 contours by using the random-sampling-based generation method, which is impossible for the baseline RNN and SAR. When a conventional mean-based generation method was used in the proposed DAR and other experimental models, the DAR generated accurate and less oversmoothed F0 contours and achieved a better mean-opinion-score in a subjective evaluation test.},
	number = {8},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = aug,
	year = {2018},
	keywords = {Artificial neural networks, Data models, F0, Feature extraction, Fundamental frequency, Hidden Markov models, Linguistics, Speech, autoregressive model, neural network, pitch, speech synthesis},
	pages = {1406--1419},
}

@article{walczyna_overview_2023,
	title = {Overview of {Voice} {Conversion} {Methods} {Based} on {Deep} {Learning}},
	volume = {13},
	issn = {2076-3417},
	doi = {10.3390/app13053100},
	abstract = {Voice conversion is a process where the essence of a speaker's identity is seamlessly transferred to another speaker, all while preserving the content of their speech. This usage is accomplished using algorithms that blend speech processing techniques, such as speech analysis, speaker classification, and vocoding. The cutting-edge voice conversion technology is characterized by deep neural networks that effectively separate a speaker's voice from their linguistic content. This article offers a comprehensive overview of the development status of this area of science based on the current state-of-the-art voice conversion methods.},
	number = {5},
	urldate = {2023-08-15},
	journal = {Applied Sciences},
	author = {Walczyna, Tomasz and Piotrowski, Zbigniew},
	month = jan,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {voice conversion, voice disentangling, voice synthesis},
	pages = {3100},
}

@misc{vipperla_bunched_2020,
	title = {Bunched {LPCNet} : {Vocoder} for low-cost {Neural} {Text}-{To}-{Speech} {Systems}},
	shorttitle = {Bunched {LPCNet}},
	abstract = {LPCNet is an efficient vocoder that combines linear prediction and deep neural network modules to keep the computational complexity low. In this work, we present two techniques to further reduce it's complexity, aiming for a low-cost LPCNet vocoder-based neural Text-to-Speech (TTS) System. These techniques are: 1) Sample-bunching, which allows LPCNet to generate more than one audio sample per inference; and 2) Bit-bunching, which reduces the computations in the final layer of LPCNet. With the proposed bunching techniques, LPCNet, in conjunction with a Deep Convolutional TTS (DCTTS) acoustic model, shows a 2.19x improvement over the baseline run-time when running on a mobile device, with a less than 0.1 decrease in TTS mean opinion score (MOS).},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Vipperla, Ravichander and Park, Sangjun and Choo, Kihyun and Ishtiaq, Samin and Min, Kyoungbo and Bhattacharya, Sourav and Mehrotra, Abhinav and Ramos, Alberto Gil C. P. and Lane, Nicholas D.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.04574 [cs, eess]
Issue: arXiv:2008.04574},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{vincent_extracting_2008,
	address = {Helsinki, Finland},
	title = {Extracting and composing robust features with denoising autoencoders},
	isbn = {978-1-60558-205-4},
	doi = {10.1145/1390156.1390294},
	urldate = {2023-08-21},
	booktitle = {Proceedings of the 25th international conference on machine learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2008},
	pages = {1096--1103},
}

@inproceedings{villavicencio_applying_2010,
	title = {Applying voice conversion to concatenative singing-voice synthesis},
	doi = {10.21437/Interspeech.2010-596},
	abstract = {This work address the application of Voice Conversion to singing-voice. The GMM-based approach was applied to VOCALOID, a concatenative singing synthesizer, to perform singer timbre conversion. The conversion framework was applied to full-quality singing databases, achieving a satisfactory conversion effect on the synthesized utterances. We report in this paper the results of our experimentation focused to study the spectral conversion performance when applied to specific pitch-range data.},
	urldate = {2023-07-25},
	booktitle = {Interspeech 2010},
	publisher = {ISCA},
	author = {Villavicencio, Fernando and Bonada, Jordi},
	month = sep,
	year = {2010},
	pages = {2162--2165},
}

@article{valimaki_all_2016,
	title = {All {About} {Audio} {Equalization}: {Solutions} and {Frontiers}},
	volume = {6},
	issn = {2076-3417},
	shorttitle = {All {About} {Audio} {Equalization}},
	doi = {10.3390/app6050129},
	abstract = {Audio equalization is a vast and active research area. The extent of research means that one often cannot identify the preferred technique for a particular problem. This review paper bridges those gaps, systemically providing a deep understanding of the problems and approaches in audio equalization, their relative merits and applications. Digital signal processing techniques for modifying the spectral balance in audio signals and applications of these techniques are reviewed, ranging from classic equalizers to emerging designs based on new advances in signal processing and machine learning. Emphasis is placed on putting the range of approaches within a common mathematical and conceptual framework. The application areas discussed herein are diverse, and include well-defined, solvable problems of filter design subject to constraints, as well as newly emerging challenges that touch on problems in semantics, perception and human computer interaction. Case studies are given in order to illustrate key concepts and how they are applied in practice. We also recommend preferred signal processing approaches for important audio equalization problems. Finally, we discuss current challenges and the uncharted frontiers in this field. The source code for methods discussed in this paper is made available at https://code.soundsoftware.ac.uk/projects/allaboutaudioeq.},
	number = {5},
	urldate = {2023-07-31},
	journal = {Applied Sciences},
	author = {Välimäki, Vesa and Reiss, Joshua D.},
	month = may,
	year = {2016},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {acoustic signal processing, audio systems, digital filters, digital signal processing, equalizers, infinite impulse response filters, music technology},
	pages = {129},
}

@article{turian_one_2021,
	title = {One {Billion} {Audio} {Sounds} from {GPU}-enabled {Modular} {Synthesis}},
	abstract = {We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, which is 100x larger than any audio dataset in the literature. Each sound is paired with the corresponding latent parameters used to generate it. synth1B1 samples are deterministically generated on-the-fly 16200x faster than real-time (714MHz) on a single GPU using torchsynth (https://github.com/torchsynth/torchsynth), an open-source modular synthesizer we release. Additionally, we release two new audio datasets: FM synth timbre (https://zenodo.org/record/4677102) and subtractive synth pitch (https://zenodo.org/record/4677097). Using these datasets, we demonstrate new rank-based synthesizer-motivated evaluation criteria for existing audio representations. Finally, we propose novel approaches to synthesizer hyperparameter optimization, and demonstrate how perceptually-correlated auditory distances could enable new applications in synthesizer design.},
	urldate = {2021-05-05},
	journal = {arXiv:2104.12922 [cs, eess]},
	author = {Turian, Joseph and Shier, Jordie and Tzanetakis, George and McNally, Kirk and Henry, Max},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12922 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, ⛔ No DOI found},
}

@article{turian_im_2020,
	title = {I'm sorry for your {Loss}: {Spectrally}-{Based} {Audio} {Distances} {Are} {Bad} at pitch},
	shorttitle = {I'm {Sorry} for {Your} {Loss}},
	abstract = {Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.},
	urldate = {2021-05-05},
	journal = {arXiv:2012.04572 [cs, eess]},
	author = {Turian, Joseph and Henry, Max},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.04572 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{valin_lpcnet_2019,
	title = {{LPCNET}: {Improving} {Neural} {Speech} {Synthesis} through linear {Prediction}},
	shorttitle = {{LPCNET}},
	doi = {10.1109/ICASSP.2019.8682804},
	abstract = {Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Valin, Jean-Marc and Skoglund, Jan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Cepstrum, Complexity theory, Logic gates, Neural networks, Predictive models, Sparse matrices, Speech synthesis, WaveRNN, neural audio synthesis, parametric coding},
	pages = {5891--5895},
}

@article{valimaki_neurally_2019,
	title = {Neurally {Controlled} {Graphic} {Equalizer}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2935809},
	abstract = {This paper describes a neural network based method to simplify the design of a graphic equalizer without sacrificing the accuracy of approximation. The key idea is to train a neural network to predict the mapping from target gains to the optimized band filter gains at specified center frequencies. The prediction is implemented with a feedforward neural network having a hidden layer with 20 neurons in the case of the ten-octave graphic equalizer. The band filter coefficients can then be quickly and easily computed using closed-form formulas. This work turns, for the first time, the accurate graphic equalization design into a feedforward calculation without matrix inversion or iterations. The filter gain control using the neural network reduces the computing time by 99.6\% in comparison to the least-squares design method it is imitating and contributes an approximation error of less than 0.1 dB. The resulting neurally controlled graphic equalizer will be highly useful in various audio and music processing applications, which require time-varying equalization.},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Välimäki, Vesa and Rämö, Jussi},
	month = dec,
	year = {2019},
	keywords = {Audio systems, Bandwidth, Equalizers, Gain, Graphics, IIR filters, Optimization, Speech processing, equalizers, feedforward neural networks, supervised learning},
	pages = {2140--2149},
}

@article{valimaki_fifty_2012,
	title = {Fifty years of artificial reverberation},
	volume = {20},
	issn = {15587916},
	doi = {10.1109/TASL.2012.2189567},
	abstract = {The first artificial reverberation algorithms were proposed in the early 1960s, and new, improved algorithms are published regularly. These algorithms have been widely used in music production since the 1970s, and now find applications in new fields, such as game audio. This overview article provides a unified review of the various approaches to digital artificial reverberation. The three main categories have been delay networks, convolutionbased algorithms, and physical room models. Delay-network and convolution techniques have been competing in popularity in the music technology field, and are often employed to produce a desired perceptual or artistic effect. In applications including virtual reality, predictive acoustic modeling, and computer-aided design of acoustic spaces, accuracy is desired, and physical models have been mainly used, although, due to their computational complexity, they are currently mainly used for simplified geometries or to generate reverberation impulse responses for use with a convolution method. With the increase of computing power, all these approaches will be available in real time. A recent trend in audio technology is the emulation of analog artificial reverberation units, such as spring reverberators, using signal processing algorithms. As a case study we present an improved parametric model for a spring reverberation unit.},
	number = {5},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Välimäki, Vesa and Parker, Julian D. and Savioja, Lauri and Smith, Julius O. and Abel, Jonathan S.},
	year = {2012},
	note = {Publisher: IEEE},
	keywords = {Acoustic scattering, Acoustic signal processing, Acoustics, Architectural acoustics, Convolution, Infinite impule response (IIR) digital filters},
	pages = {1421--1448},
}

@inproceedings{tokuday_directly_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis},
	isbn = {978-1-4673-6997-8},
	doi = {10.1109/ICASSP.2015.7178765},
	urldate = {2023-06-22},
	booktitle = {2015 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Tokuday, Keiichi and Zen, Heiga},
	month = apr,
	year = {2015},
	pages = {4215--4219},
}

@misc{tian_featherwave_2020,
	title = {{FeatherWave}: {An} efficient high-fidelity neural vocoder with multi-band linear prediction},
	shorttitle = {{FeatherWave}},
	abstract = {In this paper, we propose the FeatherWave, yet another variant of WaveRNN vocoder combining the multi-band signal processing and the linear predictive coding. The LPCNet, a recently proposed neural vocoder which utilized the linear predictive characteristic of speech signal in the WaveRNN architecture, can generate high quality speech with a speed faster than real-time on a single CPU core. However, LPCNet is still not efficient enough for online speech generation tasks. To address this issue, we adopt the multi-band linear predictive coding for WaveRNN vocoder. The multi-band method enables the model to generate several speech samples in parallel at one step. Therefore, it can significantly improve the efficiency of speech synthesis. The proposed model with 4 sub-bands needs less than 1.6 GFLOPS for speech generation. In our experiments, it can generate 24 kHz high-fidelity audio 9x faster than real-time on a single CPU, which is much faster than the LPCNet vocoder. Furthermore, our subjective listening test shows that the FeatherWave can generate speech with better quality than LPCNet.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Tian, Qiao and Zhang, Zewang and Lu, Heng and Chen, Ling-Hui and Liu, Shan},
	month = sep,
	year = {2020},
	note = {arXiv: 2005.05551 [cs, eess]
Issue: arXiv:2005.05551},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{tewari_state_2020,
	title = {State of the {Art} on {Neural} {Rendering}},
	volume = {39},
	issn = {1467-8659},
	doi = {10.1111/cgf.14022},
	abstract = {Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.},
	number = {2},
	urldate = {2023-06-21},
	journal = {Computer Graphics Forum},
	author = {Tewari, A. and Fried, O. and Thies, J. and Sitzmann, V. and Lombardi, S. and Sunkavalli, K. and Martin-Brualla, R. and Simon, T. and Saragih, J. and Nießner, M. and Pandey, R. and Fanello, S. and Wetzstein, G. and Zhu, J.-Y. and Theobalt, C. and Agrawala, M. and Shechtman, E. and Goldman, D. B and Zollhöfer, M.},
	year = {2020},
	pages = {701--727},
}

@misc{tan_survey_2021,
	title = {A {Survey} on {Neural} {Speech} {Synthesis}},
	abstract = {Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models, and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.15561 [cs, eess]
Issue: arXiv:2106.15561},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{sudholt_vocal_2023,
	title = {Vocal {Tract} {Area} {Estimation} by {Gradient} {Descent}},
	abstract = {Articulatory features can provide interpretable and flexible controls for the synthesis of human vocalizations by allowing the user to directly modify parameters like vocal strain or lip position. To make this manipulation through resynthesis possible, we need to estimate the features that result in a desired vocalization directly from audio recordings. In this work, we propose a white-box optimization technique for estimating glottal source parameters and vocal tract shapes from audio recordings of human vowels. The approach is based on inverse filtering and optimizing the frequency response of a wave{\textbackslash}-guide model of the vocal tract with gradient descent, propagating error gradients through the mapping of articulatory features to the vocal tract area function. We apply this method to the task of matching the sound of the Pink Trombone, an interactive articulatory synthesizer, to a given vocalization. We find that our method accurately recovers control functions for audio generated by the Pink Trombone itself. We then compare our technique against evolutionary optimization algorithms and a neural network trained to predict control parameters from audio. A subjective evaluation finds that our approach outperforms these black-box optimization baselines on the task of reproducing human vocalizations.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Südholt, David and Cámara, Mateo and Xu, Zhiyuan and Reiss, Joshua D.},
	month = jul,
	year = {2023},
	note = {arXiv: 2307.04702 [cs, eess]
Issue: arXiv:2307.04702},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{subramani_end--end_2022,
	title = {End-to-end {LPCNet}: {A} {Neural} {Vocoder} {With} {Fully}-{Differentiable} {LPC} {Estimation}},
	shorttitle = {End-to-end {LPCNet}},
	doi = {10.21437/Interspeech.2022-912},
	urldate = {2023-06-21},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Subramani, Krishna and Valin, Jean-Marc and Isik, Umut and Smaragdis, Paris and Krishnaswamy, Arvindh},
	month = sep,
	year = {2022},
	pages = {818--822},
}

@inproceedings{sturm_artificial_2019,
	title = {Artificial intelligence and music: {Open} {Questions} of copyright {Law} and engineering {Praxis}},
	volume = {8},
	doi = {10.3390/arts8030115},
	booktitle = {Arts},
	publisher = {Multidisciplinary Digital Publishing Institute},
	author = {Sturm, Bob L T and Iglesias, Maria and Ben-Tal, Oded and Miron, Marius and Gómez, Emilia},
	year = {2019},
	keywords = {artificial intelligence, copyright, engineering, ethics, music},
	pages = {115},
}

@inproceedings{tancik_fourier_2020,
	title = {Fourier features let networks learn high frequency functions in low dimensional domains},
	volume = {33},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {7537--7547},
}

@inproceedings{tamamori_speaker-dependent_2017,
	title = {Speaker-dependent wavenet vocoder.},
	volume = {2017},
	doi = {10.21437/Interspeech.2017-314},
	booktitle = {Interspeech},
	author = {Tamamori, Akira and Hayashi, Tomoki and Kobayashi, Kazuhiro and Takeda, Kazuya and Toda, Tomoki},
	year = {2017},
	pages = {1118--1122},
}

@inproceedings{stylianou_voice_2009,
	title = {Voice {Transformation}: {A} survey},
	shorttitle = {Voice {Transformation}},
	doi = {10.1109/ICASSP.2009.4960401},
	abstract = {Voice transformation refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice transformation is usually seen as an add-on or an external system in speech synthesis systems since it may create virtual voices in a simple and flexible way. In this paper we review the state-of-the-art Voice transformation methodology showing its limitations in producing good speech quality and its current challenges. Addressing quality issues of current voice transformation algorithms in conjunction with properties of the speech production and speech perception systems we try to pave the way for more natural Voice Transformation algorithms in the future. Facing the challenges, will allow Voice transformation systems to be applied in important and versatile areas of speech technology; applications that are far beyond speech synthesis.},
	booktitle = {2009 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing}},
	author = {Stylianou, Yannis},
	month = apr,
	year = {2009},
	note = {ISSN: 2379-190X},
	keywords = {Computer science, Informatics, Production systems, Signal processing, Speaker recognition, Speech coding, Speech enhancement, Speech processing, Speech production, Speech recognition, Speech synthesis, Voice Transformation, speaking style, speech perception, voice quality},
	pages = {3585--3588},
}

@inproceedings{stilson_analyzing_1996,
	title = {Analyzing the {Moog} {VCF} with considerations for digital implementation},
	booktitle = {Proceedings of the 1996 international computer music conference, hong kong, computer music association},
	author = {Stilson, Tim and Smith, Julius O},
	year = {1996},
	keywords = {⛔ No DOI found},
}

@article{stewart_electrical_1922,
	title = {An electrical analogue of the vocal organs},
	volume = {110},
	doi = {10.1038/110311a0},
	number = {2757},
	journal = {Nature},
	author = {Stewart, John Q},
	year = {1922},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {311--312},
}

@article{steinmetz_style_2022,
	title = {Style transfer of audio {Effects} with differentiable {Signal} {Processing}},
	volume = {70},
	doi = {10.17743/jaes.2022.0025},
	number = {9},
	journal = {Journal of The Audio Engineering Society},
	author = {Steinmetz, Christian J and Bryan, Nicholas J and Reiss, Joshua D},
	year = {2022},
	pages = {14},
}

@book{smith_spectral_2011,
	address = {Stanford, Calif},
	title = {Spectral audio signal processing},
	isbn = {978-0-9745607-3-1},
	publisher = {Stanford University, CCRMA},
	author = {Smith, Julius O.},
	collaborator = {University, Stanford},
	year = {2011},
}

@book{smith_physical_2010,
	address = {Stanford, Calif},
	title = {Physical audio {Signal} {Processing}: {For} virtual {Musical} {Instruments} and audio {Effects}},
	isbn = {978-0-9745607-2-4},
	shorttitle = {Physical audio signal processing},
	publisher = {Stanford University, CCRMA},
	author = {Smith, Julius O.},
	collaborator = {University, Stanford},
	year = {2010},
}

@inproceedings{steinmetz_efficient_2022,
	title = {Efficient neural networks for real-time modeling of analog dynamic range compression},
	booktitle = {Audio {Engineering} {Society} {Convention} 152},
	author = {Steinmetz, Christian J. and Reiss, Joshua D.},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@inproceedings{steinmetz_automatic_2021,
	title = {Automatic multitrack {Mixing} {With} {A} {Differentiable} {Mixing} {Console} {Of} {Neural} {Audio} {Effects}},
	doi = {10.1109/ICASSP39728.2021.9414364},
	abstract = {Applications of deep learning to automatic multitrack mixing are largely unexplored. This is partly due to the limited available data, coupled with the fact that such data is relatively unstructured and variable. To address these challenges, we propose a domain-inspired model with a strong inductive bias for the mixing task. We achieve this with the application of pre-trained sub-networks and weight sharing, as well as with a sum/difference stereo loss function. The proposed model can be trained with a limited number of examples, is permutation invariant with respect to the input ordering, and places no limit on the number of input sources. Furthermore, it produces human-readable mixing parameters, allowing users to manually adjust or refine the generated mix. Results from a perceptual evaluation involving audio engineers indicate that our approach generates mixes that outperform baseline approaches. To the best of our knowledge, this work demonstrates the first approach in learning multitrack mixing conventions from real-world data at the waveform level, without knowledge of the underlying mixing parameters.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Steinmetz, Christian J. and Pons, Jordi and Pascual, Santiago and Serrà, Joan},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Conferences, Convolution, Deep learning, Intelligent music production, Multiple signal classification, Music, Speech processing, Task analysis, automatic mixing, deep learning, temporal convolutional network},
	pages = {71--75},
}

@inproceedings{stanton_speaker_2022,
	title = {Speaker {Generation}},
	doi = {10.1109/ICASSP43922.2022.9747345},
	abstract = {This work explores the task of synthesizing speech in non-existent human-sounding voices. We call this task "speaker generation", and present TacoSpawn, a system that performs competitively at this task. TacoSpawn is a recurrent attention-based text-to-speech model that learns a distribution over a speaker embedding space, which enables sampling of novel and diverse speakers. Our method is easy to implement, and does not require transfer learning from speaker ID systems. We present objective and subjective metrics for evaluating performance on this task, and demonstrate that our proposed objective metrics correlate with human perception of speaker similarity. Audio samples are available on our demo page1.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Stanton, Daisy and Shannon, Matt and Mariooryad, Soroosh and Skerry-Ryan, RJ and Battenberg, Eric and Bagby, Tom and Kao, David},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Conferences, Measurement, Signal processing, Speech processing, Task analysis, Transfer learning, speaker generation, text-to-speech synthesis},
	pages = {7897--7901},
}

@article{spall_overview_1998,
	title = {An overview of the simultaneous perturbation method for efficient optimization},
	volume = {19},
	number = {4},
	journal = {Johns Hopkins apl technical digest},
	author = {Spall, James C},
	year = {1998},
	note = {Publisher: Citeseer},
	keywords = {⛔ No DOI found},
	pages = {482--492},
}

@inproceedings{song_dspgan_2023,
	title = {{DSPGAN}: {A} {Gan}-{Based} {Universal} {Vocoder} for high-{Fidelity} {TTS} by time-{Frequency} {Domain} {Supervision} from {DSP}},
	shorttitle = {{DSPGAN}},
	doi = {10.1109/ICASSP49357.2023.10095105},
	abstract = {Recent development of neural vocoders based on the generative adversarial neural network (GAN) has shown obvious advantages of generating raw waveform conditioned on mel-spectrogram with fast inference speed and lightweight networks. Whereas, it is still challenging to train a universal neural vocoder that can synthesize high-fidelity speech from various scenarios with unseen speakers, languages, and speaking styles. In this paper, we propose DSP- GAN, a GAN-based universal vocoder for high-fidelity speech synthesis by applying the time-frequency domain supervision from digital signal processing (DSP). To eliminate the mismatch problem caused by the ground-truth spectrograms in the training phase and the predicted spectrograms in the inference phase, we leverage the mel-spectrogram extracted from the waveform generated by a DSP module, rather than the predicted mel-spectrogram from the Text-to-Speech (TTS) acoustic model, as the time-frequency domain supervision to the GAN-based vocoder. We also utilize sine excitation as the time-domain supervision to improve the harmonic modeling and eliminate various artifacts of the GAN-based vocoder. Experiments show that DSPGAN significantly outperforms the compared approaches and it can generate high-fidelity speech for various TTS models trained using diverse data. 1},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Song, Kun and Zhang, Yongmao and Lei, Yi and Cong, Jian and Li, Hanzhao and Xie, Lei and He, Gang and Bai, Jinfeng},
	month = jun,
	year = {2023},
	keywords = {Acoustics, Digital signal processing, Generative adversarial networks, Speech, Time-frequency analysis, Training, Vocoders, digital signal processing, generative adversarial network, source-filter model, universal vocoder},
	pages = {1--5},
}

@article{smith_physical_1992,
	title = {Physical {Modeling} {Using} {Digital} {Waveguides}},
	volume = {16},
	issn = {01489267},
	doi = {10.2307/3680470},
	number = {4},
	urldate = {2023-08-10},
	journal = {Computer Music Journal},
	author = {Smith, Julius O.},
	year = {1992},
	note = {JSTOR: 3680470},
	pages = {74},
}

@article{shynk_adaptive_1989,
	title = {Adaptive {IIR} filtering},
	volume = {6},
	issn = {0740-7467},
	doi = {10.1109/53.29644},
	number = {2},
	urldate = {2023-07-05},
	journal = {IEEE ASSP Magazine},
	author = {Shynk, J.J.},
	month = apr,
	year = {1989},
	pages = {4--21},
}

@inproceedings{sisman_singan_2019,
	title = {{SINGAN}: {Singing} {Voice} {Conversion} with generative {Adversarial} {Networks}},
	shorttitle = {{SINGAN}},
	doi = {10.1109/APSIPAASC47483.2019.9023162},
	abstract = {Singing voice conversion (SVC) is a task to convert the source singer's voice to sound like that of the target singer, without changing the lyrical content. So far, most of the voice conversion studies mainly focus only on the speech voice conversion that is different from singing voice conversion. We note that singing conveys both lexical and emotional information through words and tones. It is one of the most expressive components in music and a means of entertainment as well as self expression. In this paper, we propose a novel singing voice conversion framework, that is based on Generative Adversarial Networks (GANs). The proposed GAN-based conversion framework, that we call SINGAN, consists of two neural networks: a discriminator to distinguish natural and converted singing voice, and a generator to deceive the discriminator. With GAN, we minimize the differences of the distributions between the original target parameters and the generated singing parameters. To our best knowledge, this is the first framework that uses generative adversarial networks for singing voice conversion. In experiments, we show that the proposed method effectively converts singing voices and outperforms the baseline approach.},
	booktitle = {2019 asia-{Pacific} {Signal} and information {Processing} {Association} {Annual} {Summit} and conference ({APSIPA} {ASC})},
	author = {Sisman, Berrak and Vijayan, Karthika and Dong, Minghui and Li, Haizhou},
	month = nov,
	year = {2019},
	note = {ISSN: 2640-0103},
	keywords = {Gallium nitride, Generative adversarial networks, Singing voice conversion, Speech recognition, Static VAr compensators, Training, Training data, Vocoders, generative adversarial networks, singing voice},
	pages = {112--118},
}

@article{sisman_overview_2021,
	title = {An overview of voice {Conversion} and its {Challenges}: {From} {Statistical} {Modeling} to deep {Learning}},
	volume = {29},
	issn = {2329-9304},
	shorttitle = {An {Overview} of {Voice} {Conversion} and {Its} {Challenges}},
	doi = {10.1109/TASLP.2020.3038524},
	abstract = {Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Sisman, Berrak and Yamagishi, Junichi and King, Simon and Li, Haizhou},
	year = {2021},
	keywords = {Deep learning, Pipelines, Speech analysis, Speech synthesis, Training, Training data, Vocoders, Voice conversion, speaker characterization, speech analysis, vocoding, voice conversion challenges, voice conversion evaluation},
	pages = {132--157},
}

@incollection{siedenburg_perceptual_2019,
	address = {Cham},
	title = {The {Perceptual} {Representation} of {Timbre}},
	volume = {69},
	isbn = {978-3-030-14831-7 978-3-030-14832-4},
	abstract = {Timbre is a complex auditory attribute that is extracted from a fused auditory event. Its perceptual representation has been explored as a multidimensional attribute whose different dimensions can be related to abstract spectral, temporal, and spectrotemporal properties of the audio signal, although previous knowledge of the sound source itself also plays a role. Perceptual dimensions can also be related to acoustic properties that directly carry information about the mechanical processes of a sound source, including its geometry (size, shape), its material composition, and the way it is set into vibration. Another conception of timbre is as a spectromorphology encompassing time-varying frequency and amplitude behaviors, as well as spectral and temporal modulations. In all musical sound sources, timbre covaries with fundamental frequency (pitch) and playing effort (loudness, dynamic level) and displays strong interactions with these parameters.},
	urldate = {2023-08-10},
	booktitle = {Timbre: {Acoustics}, {Perception}, and {Cognition}},
	publisher = {Springer International Publishing},
	author = {McAdams, Stephen},
	editor = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	year = {2019},
	doi = {10.1007/978-3-030-14832-4_2},
	pages = {23--57},
}

@article{siedenburg_four_2017,
	title = {Four distinctions for the auditory “{Wastebasket}” of timbre},
	volume = {8},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2017.01747},
	urldate = {2023-08-10},
	journal = {Frontiers in Psychology},
	author = {Siedenburg, Kai and McAdams, Stephen},
	month = oct,
	year = {2017},
	pages = {1747},
}

@article{siddhi_survey_2017,
	title = {Survey on {Various} {Methods} of {Text} to {Speech} {Synthesis}},
	volume = {165},
	issn = {09758887},
	doi = {10.5120/ijca2017913891},
	abstract = {The primary objective of this paper is to provide an overview of existing methods Text-To-Speech synthesis techniques. Text to speech synthesis can be broadly categorized into three categories, formant Based, Concatenative based and Articulatory. Formant based speech synthesis relies on different techniques such as cascade, parallel, klatt and PARCAS Model etc. Concatenative speech synthesis can be broadly categorized into three categories, Diphones Based, Corpus based and Hybrid whereas Articulatory synthesis involves Vocal Tract Models, Acoustic Models, Glottis Models , Noise Source Models . In this paper, all text to speech synthesis methods are explained with their pros and cones.},
	number = {6},
	urldate = {2023-08-15},
	journal = {International Journal of Computer Applications},
	author = {Siddhi, Desai and M., Jashin and Bhavik, Desai},
	month = may,
	year = {2017},
	pages = {26--30},
}

@inproceedings{sheng_feature_2019,
	title = {A feature {Learning} {Siamese} {Model} for intelligent {Control} of the dynamic {Range} {Compressor}},
	doi = {10.1109/IJCNN.2019.8851950},
	abstract = {In this paper, a siamese DNN model is proposed to learn the characteristics of the audio dynamic range compressor (DRC). This facilitates an intelligent control system that uses audio examples to configure the DRC, a widely used nonlinear audio signal conditioning technique in the areas of music production, speech communication and broadcasting. Several alternative siamese DNN architectures are proposed to learn feature embeddings that can characterise subtle effects due to dynamic range compression. These models are compared with each other as well as handcrafted features proposed in previous work. The evaluation of the relations between the hyperparameters of DNN and DRC parameters are also provided. The best model is able to produce a universal feature embedding that is capable of predicting multiple DRC parameters simultaneously, which is a significant improvement from our previous research. The feature embedding shows better performance than handcrafted audio features when predicting DRC parameters for both mono-instrument audio loops and polyphonic music pieces.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sheng, Di and Fazekas, György},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Dynamic range, Intelligent control, Neural networks, Predictive models, Signal processing, Task analysis, Training},
	pages = {1--8},
}

@inproceedings{shen_natural_2018,
	title = {Natural {TTS} {Synthesis} by conditioning {Wavenet} on {MEL} {Spectrogram} {Predictions}},
	doi = {10.1109/ICASSP.2018.8461368},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
	booktitle = {2018 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and Saurous, Rif A. and Agiomvrgiannakis, Yannis and Wu, Yonghui},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Decoding, Linguistics, Spectrogram, Tacotron 2, Time-domain analysis, Training, Vocoders, WaveNet, text-to-speech},
	pages = {4779--4783},
}

@inproceedings{shan_differentiable_2022,
	title = {Differentiable {Wavetable} {Synthesis}},
	doi = {10.1109/ICASSP43922.2022.9746940},
	abstract = {Differentiable Wavetable Synthesis (DWTS) is a technique for neural audio synthesis which learns a dictionary of one-period waveforms i.e. wavetables, through end-to-end training. We achieve high-fidelity audio synthesis with as little as 10 to 20 wavetables and demonstrate how a data-driven dictionary of waveforms opens up unprecedented one-shot learning paradigms on short audio clips. Notably, we show audio manipulations, such as high quality pitch-shifting, using only a few seconds of input audio. Lastly, we investigate performance gains from using learned wavetables for realtime and interactive audio synthesis.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Shan, Siyuan and Hantrakul, Lamtharn and Chen, Jitong and Avent, Matt and Trevelyan, David},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Conferences, Dictionaries, Differentiable Dictionaries, Differentiable Digital Signal Processing, Performance gain, Signal processing, Synthesizers, Training, Wavetable Synthesis},
	pages = {4598--4602},
}

@inproceedings{shadle_prospects_2001,
	title = {Prospects for articulatory synthesis: {A} position paper},
	booktitle = {4th {ISCA} tutorial and research workshop ({ITRW}) on speech synthesis},
	author = {Shadle, Christine H and Damper, Robert I},
	year = {2001},
	keywords = {⛔ No DOI found},
}

@article{schwarz_corpus-based_2007,
	title = {Corpus-{Based} {Concatenative} {Synthesis}},
	volume = {24},
	issn = {1558-0792},
	doi = {10.1109/MSP.2007.323274},
	abstract = {Corpus-based concatenative methods for musical sound synthesis have attracted much attention recently. They make use of a variety of sound snippets in a database to assemble a desired sound or phrase according to a target specification given in sound descriptors or by an example sound. With ever-larger sound databases easily available, together with a pertinent description of their contents, they are increasingly used for composition, high-level instrument synthesis, and interactive exploration of a sound corpus. This article gives an overview of the components needed for corpus-based concatenative synthesis and details of some realizations. Signal processing methods are crucial for all parts of analysis, (segmentation; and descriptor analysis), for synthesis, and can intervene in the selection part, e.g., for spectral matching},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Schwarz, Diemo},
	month = mar,
	year = {2007},
	keywords = {Assembly, Context awareness, Databases, Instruments, Signal analysis, Signal processing, Signal processing algorithms, Signal synthesis, Speech recognition, Speech synthesis},
	pages = {92--104},
}

@misc{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498 [cs]
Issue: arXiv:1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{sagisaka_speech_1988,
	address = {New York, NY, USA},
	title = {Speech synthesis by rule using an optimal selection of non-uniform synthesis units},
	doi = {10.1109/ICASSP.1988.196677},
	urldate = {2023-07-22},
	booktitle = {{ICASSP}-88., international {Conference} on acoustics, speech, and signal {Processing}},
	publisher = {IEEE},
	author = {Sagisaka, Y.},
	year = {1988},
	pages = {679--682},
}

@article{serra_spectral_1990,
	title = {Spectral modeling {Synthesis}: {A} {Sound} {Analysis}/{Synthesis} {System} {Based} on a deterministic {Plus} {Stochastic} {Decomposition}},
	volume = {14},
	issn = {0148-9267},
	shorttitle = {Spectral {Modeling} {Synthesis}},
	doi = {10.2307/3680788},
	number = {4},
	urldate = {2019-12-21},
	journal = {Computer Music Journal},
	author = {Serra, Xavier and Smith, Julius},
	year = {1990},
	keywords = {unread},
	pages = {12--24},
}

@inproceedings{seeviour_automatic_1976,
	title = {Automatic generation of control signals for a parallel formant speech synthesizer},
	volume = {1},
	doi = {10.1109/ICASSP.1976.1169987},
	abstract = {An algorithm to estimate formant frequencies and amplitudes has been developed to provide control signals for a parallel formant speech synthesizer. During voiced sounds power spectra are derived by Fourier analysis of speech samples in the closed glottis regions but for unvoiced sounds spectra are derived with appropriate time averaging. Each power spectrum is matched trying several different allocations of formants to spectral peaks. An analysis-by-synthesis procedure iteratively updates the formant frequencies and amplitudes for each allocation. The final choice from these formant parameters is made every 10 ms, and depends on both spectral match and speech-like constraints.},
	booktitle = {{ICASSP} '76. {IEEE} {International} {Conference} on acoustics, speech, and signal {Processing}},
	author = {Seeviour, P. and Holmes, J. and Judd, M.},
	month = apr,
	year = {1976},
	keywords = {Acoustic noise, Amplitude estimation, Automatic generation control, Equations, Frequency estimation, Frequency synthesizers, Signal generators, Speech analysis, Speech synthesis, Testing},
	pages = {690--693},
}

@inproceedings{seago_critical_2004,
	address = {Bristol, UK},
	title = {A {Critical} {Analysis} of {Synthesizer} {User} {Interfaces} for {Timbre}},
	volume = {2},
	abstract = {In this paper, we review and analyse categories of user interface used in hardware and software electronic music synthesizers. Problems with the user specification and modification of timbre are discussed. Three principal types of user interface for controlling timbre are distinguished. A problem common to all three categories is identified: that the core language of each category has no well-defined mapping onto the task languages of subjective timbre categories as used by musicians.},
	booktitle = {Proceedings of the {XVIII} {British} {HCI} {Group} {Annual} {Conference} {HCI} 2004},
	publisher = {Research Press International},
	author = {Seago, Allan and Holland, Simon and Mulholland, Paul},
	month = sep,
	year = {2004},
	keywords = {User interface},
	pages = {105--108},
}

@article{schwarz_concatenative_2006,
	title = {Concatenative sound synthesis: {The} early years},
	volume = {35},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Concatenative sound synthesis},
	doi = {10.1080/09298210600696857},
	number = {1},
	urldate = {2023-08-08},
	journal = {Journal of New Music Research},
	author = {Schwarz, Diemo},
	month = mar,
	year = {2006},
	pages = {3--22},
}

@article{schulze-forster_unsupervised_2023,
	title = {Unsupervised music {Source} {Separation} {Using} {Differentiable} {Parametric} {Source} {Models}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2023.3252272},
	abstract = {Supervised deep learning approaches to underdetermined audio source separation achieve state-of-the-art performance but require a dataset of mixtures along with their corresponding isolated source signals. Such datasets can be extremely costly to obtain for musical mixtures. This raises a need for unsupervised methods. We propose a novel unsupervised model-based deep learning approach to musical source separation. Each source is modelled with a differentiable parametric source-filter model. A neural network is trained to reconstruct the observed mixture as a sum of the sources by estimating the source models' parameters given their fundamental frequencies. At test time, soft masks are obtained from the synthesized source signals. The experimental evaluation on a vocal ensemble separation task shows that the proposed method outperforms learning-free methods based on nonnegative matrix factorization and a supervised deep learning baseline. Integrating domain knowledge in the form of source models into a data-driven method leads to high data efficiency: the proposed approach achieves good separation quality even when trained on less than three minutes of audio. This work makes powerful deep learning based separation usable in scenarios where training data with ground truth is expensive or nonexistent.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Schulze-Forster, Kilian and Richard, Gaël and Kelley, Liam and Doire, Clement S. J. and Badeau, Roland},
	year = {2023},
	keywords = {Deep learning, Music, Recording, Source separation, Speech processing, Task analysis, Training, Unsupervised learning, audio source separation, deep learning, model-based, signal processing},
	pages = {1276--1289},
}

@inproceedings{saino_hmm-based_2006,
	title = {An {HMM}-based singing voice synthesis system},
	doi = {10.21437/Interspeech.2006-584},
	booktitle = {Ninth international conference on spoken language processing},
	author = {Saino, Keijiro and Zen, Heiga and Nankaku, Yoshihiko and Lee, Akinobu and Tokuda, Keiichi},
	year = {2006},
}

@article{rouard_crash_2021,
	title = {{CRASH}: {Raw} {Audio} {Score}-based {Generative} {Modeling} for controllable {High}-resolution {Drum} {Sound} {Synthesis}},
	shorttitle = {{CRASH}},
	abstract = {In this paper, we propose a novel score-base generative model for unconditional raw audio synthesis. Our proposal builds upon the latest developments on diffusion process modeling with stochastic differential equations, which already demonstrated promising results on image generation. We motivate novel heuristics for the choice of the diffusion processes better suited for audio generation, and consider the use of a conditional U-Net to approximate the score function. While previous approaches on diffusion models on audio were mainly designed as speech vocoders in medium resolution, our method termed CRASH (Controllable Raw Audio Synthesis with High-resolution) allows us to generate short percussive sounds in 44.1kHz in a controllable way. Through extensive experiments, we showcase on a drum sound generation task the numerous sampling schemes offered by our method (unconditional generation, deterministic generation, inpainting, interpolation, variations, class-conditional sampling) and propose the class-mixing sampling, a novel way to generate "hybrid" sounds. Our proposed method closes the gap with GAN-based methods on raw audio, while offering more flexible generation capabilities with lighter and easier-to-train models.},
	urldate = {2022-03-10},
	journal = {arXiv:2106.07431 [cs, eess]},
	author = {Rouard, Simon and Hadjeres, Gaëtan},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07431 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{rodet_synthesis_2002,
	title = {Synthesis and processing of the singing voice},
	booktitle = {Proc. 1st {IEEE} benelux workshop on model based processing and coding of audio ({MPCA}-2002)},
	author = {Rodet, Xavier},
	year = {2002},
	keywords = {⛔ No DOI found},
	pages = {15--31},
}

@article{resset_computer_1985,
	title = {Computer {Music} {Experiments} 1964 - ...},
	volume = {9},
	issn = {0148-9267},
	number = {1},
	urldate = {2023-08-15},
	journal = {Computer Music Journal},
	author = {Resset, Jean-Claude},
	year = {1985},
	note = {JSTOR: 4617918
Publisher: The MIT Press},
	keywords = {⛔ No DOI found},
	pages = {11--18},
}

@inproceedings{renault_differentiable_2022,
	address = {Vienna, Austria},
	title = {Differentiable {Piano} {Model} for {Midi}-to-{Audio} {Performance} {Synthesis}},
	abstract = {Recent neural-based synthesis models have achieved impressive results for musical instrument sound generation. In particular, the Differentiable Digital Signal Processing (DDSP) framework enables the usage of spectral modeling analysis and synthesis techniques in fully differentiable architectures. Yet currently, it has only been used for modeling monophonic instruments. Leveraging the interpretability and modularity of this framework, the present work introduces a polyphonic differentiable model for piano sound synthesis, conditioned on Musical Instrument Digital Interface (MIDI) inputs. The model architecture is motivated by high-level acoustic modeling knowledge of the instrument which, in tandem with the sound structure priors inherent to the DDSP components, makes for a lightweight, interpretable and realistic sounding piano model. The proposed model has been evaluated in a listening test, demonstrating improved sound quality compared to a benchmark neural-based piano model, with significantly less parameters and even with reduced training data. The same listening test indicates that physical-modeling-based models still achieve better quality, but the differentiability of our lightened approach encourages its usage in other musical tasks dealing with polyphonic audio and symbolic data.},
	booktitle = {Proceedings of the 25th international {Conference} on digital {Audio} {Effects}},
	author = {Renault, Lenny and Mignot, Rémi and Roebel, Axel},
	year = {2022},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@inproceedings{ren_deepsinger_2020,
	address = {Virtual Event CA USA},
	title = {{DeepSinger}: {Singing} {Voice} {Synthesis} with data {Mined} {From} the web},
	isbn = {978-1-4503-7998-4},
	shorttitle = {{DeepSinger}},
	doi = {10.1145/3394486.3403249},
	urldate = {2023-07-24},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on knowledge {Discovery} \& data {Mining}},
	publisher = {ACM},
	author = {Ren, Yi and Tan, Xu and Qin, Tao and Luan, Jian and Zhao, Zhou and Liu, Tie-Yan},
	month = aug,
	year = {2020},
	pages = {1979--1989},
}

@article{ren_comprehensive_2022,
	title = {A comprehensive {Survey} of neural {Architecture} {Search}: {Challenges} and solutions},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Comprehensive} {Survey} of {Neural} {Architecture} {Search}},
	doi = {10.1145/3447582},
	abstract = {Deep learning has made substantial breakthroughs in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers' prior knowledge and experience. And due to the limitations of humans' inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture. Neural Architecture Search ( NAS ) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. In addition, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.},
	number = {4},
	urldate = {2023-08-09},
	journal = {ACM Computing Surveys},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
	month = may,
	year = {2022},
	pages = {1--34},
}

@article{raitio_phase_2016,
	title = {Phase perception of the glottal excitation and its relevance in statistical parametric speech synthesis},
	volume = {81},
	issn = {01676393},
	doi = {10.1016/j.specom.2016.01.007},
	abstract = {While the characteristics of the amplitude spectrum of the voiced excitation have been studied widely both in natural and synthetic speech, the role of the excitation phase has remained less explored. This contradicts findings observed in sound perception studies indicating that humans are not phase deaf. Especially in speech synthesis, phase information is often omitted for simplicity. This study investigates the impact of phase information of the excitation signal of voiced speech and its relevance in statistical parametric speech synthesis. The experiments in the study involve, firstly, converting the pitch-synchronously computed original phase spectra of the excitation waveforms (either glottal flow waveforms or residuals) to either zero phase, cyclostationary random phase, or random phase. Secondly, the quality of synthetic speech in each case is compared in subjective listening tests to the corresponding signal excited with the original, natural phase. Experiments are conducted with natural, vocoded, and synthetic speech using voice material from various speakers with varying speaking styles, such as breathy, normal, and Lombard speech. The results indicate that the phase spectrum of the voiced excitation has a perceptually relevant effect in natural, vocoded, and synthetic speech, and utilizing the phase information in speech synthesis leads to improved speech quality.},
	urldate = {2023-08-02},
	journal = {Speech Communication},
	author = {Raitio, Tuomo and Juvela, Lauri and Suni, Antti and Vainio, Martti and Alku, Paavo},
	month = jul,
	year = {2016},
	pages = {104--119},
}

@inproceedings{roth_comparison_2011,
	title = {A comparison of parametric {Optimization} {Techniques} for musical {Instrument} {Tone} {Matching}},
	abstract = {Parametric optimisation techniques are compared in their abilities to elicit parameter settings for sound synthesis algorithms which cause them to emit sounds as similar as possible to target sounds. A hill climber, a genetic algorithm, a neural net and a data driven approach are compared. The error metric used is the Euclidean distance in MFCC feature space. This metric is justified on the basis of its success in previous work. The genetic algorithm offers the best results with the FM and...},
	urldate = {2023-06-22},
	booktitle = {Audio {Engineering} {Society} {Convention} 130},
	publisher = {Audio Engineering Society},
	author = {Roth, Martin and Yee-King, Matthew},
	month = may,
	year = {2011},
}

@article{ramirez_deep_2020,
	title = {Deep learning for black-box modeling of audio effects},
	volume = {10},
	issn = {20763417},
	doi = {10.3390/app10020638},
	abstract = {Virtual analog modeling of audio effects consists of emulating the sound of an audio processor reference device. This digital simulation is normally done by designing mathematical models of these systems. It is often difficult because it seeks to accurately model all components within the effect unit, which usually contains various nonlinearities and time-varying components. Most existing methods for audio effects modeling are either simplified or optimized to a very specific circuit or type of audio effect and cannot be efficiently translated to other types of audio effects. Recently, deep neural networks have been explored as black-box modeling strategies to solve this task, i.e., by using only input-output measurements. We analyse different state-of-the-art deep learning models based on convolutional and recurrent neural networks, feedforward WaveNet architectures and we also introduce a new model based on the combination of the aforementioned models. Through objective perceptual-based metrics and subjective listening tests we explore the performance of these models when modeling various analog audio effects. Thus, we show virtual analog models of nonlinear effects, such as a tube preamplifier; nonlinear effects with memory, such as a transistor-based limiter and nonlinear time-varying effects, such as the rotating horn and rotating woofer of a Leslie speaker cabinet.},
	number = {2},
	journal = {Applied Sciences (Switzerland)},
	author = {Ramírez, Marco A.Martínez and Benetos, Emmanouil and Reiss, Joshua D.},
	year = {2020},
	keywords = {Audio effects, Black-box modeling, Deep learning, Leslie speaker, Nonlinear, Time-varying, Transistor-based limiter, Tube amplifier},
}

@misc{ramachandran_fast_2017,
	title = {Fast {Generation} for {Convolutional} {Autoregressive} {Models}},
	abstract = {Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a na{\textbackslash}"\{i\}ve fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to \$21{\textbackslash}times\$ and \$183{\textbackslash}times\$ speedups respectively.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Paine, Tom Le and Khorrami, Pooya and Babaeizadeh, Mohammad and Chang, Shiyu and Zhang, Yang and Hasegawa-Johnson, Mark A. and Campbell, Roy H. and Huang, Thomas S.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06001 [cs, stat]
Issue: arXiv:1704.06001},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{rabiner_theory_2011,
	address = {Upper Saddle River},
	edition = {1st ed},
	title = {Theory and applications of digital speech processing},
	isbn = {978-0-13-603428-5},
	publisher = {Pearson},
	author = {Rabiner, Lawrence R. and Schafer, Ronald W.},
	year = {2011},
	note = {tex.lccn: TK7882.S65 R32 2011},
	keywords = {Speech processing systems},
}

@misc{pelinski_pipeline_2023,
	title = {Pipeline for recording datasets and running neural networks on the bela embedded hardware platform},
	abstract = {Deploying deep learning models on embedded devices is an arduous task: oftentimes, there exist no platform-specific instructions, and compilation times can be considerably large due to the limited computational resources available on-device. Moreover, many music-making applications demand real-time inference. Embedded hardware platforms for audio, such as Bela, offer an entry point for beginners into physical audio computing; however, the need for cross-compilation environments and low-level software development tools for deploying embedded deep learning models imposes high entry barriers on non-expert users. We present a pipeline for deploying neural networks in the Bela embedded hardware platform. In our pipeline, we include a tool to record a multichannel dataset of sensor signals. Additionally, we provide a dockerised cross-compilation environment for faster compilation. With this pipeline, we aim to provide a template for programmers and makers to prototype and experiment with neural networks for real-time embedded musical applications.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Pelinski, Teresa and Diaz, Rodrigo and Temprano, Adán L. Benito and McPherson, Andrew},
	month = jun,
	year = {2023},
	note = {arXiv: 2306.11389 [cs, eess]
Issue: arXiv:2306.11389},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499 [cs]
Issue: arXiv:1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@inproceedings{oord_parallel_2018,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the 35th international {Conference} on machine {Learning}},
	publisher = {PMLR},
	author = {Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3918--3926},
}

@article{puckette_low-dimensional_nodate,
	title = {Low-dimensional parameter mapping using spectral envelopes},
	abstract = {We explore the technique of controlling synthesis using an instrumental or other sound source. The range of spectra available from the sound source, and also that available from the synthesis technique, are estimated and the first is mapped to the second. Suitable synthesis parameters for the synthesis algorithms are found by searching a database of known output spectra. A simple experiment illustrates the technique.},
	author = {Puckette, Miller},
	keywords = {⛔ No DOI found},
}

@inproceedings{prenger_waveglow_2019,
	title = {Waveglow: {A} {Flow}-based {Generative} {Network} for {Speech} {Synthesis}},
	shorttitle = {Waveglow},
	doi = {10.1109/ICASSP.2019.8683143},
	abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow [1] and WaveNet [2] in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online [3].},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Audio Synthesis, Deep Learning, Generative models, Text-to-speech},
	pages = {3617--3621},
}

@misc{polyak_unsupervised_2020,
	title = {Unsupervised {Cross}-{Domain} {Singing} {Voice} {Conversion}},
	abstract = {We present a wav-to-wav generative model for the task of singing voice conversion from any identity. Our method utilizes both an acoustic model, trained for the task of automatic speech recognition, together with melody extracted features to drive a waveform-based generator. The proposed generative architecture is invariant to the speaker's identity and can be trained to generate target singers from unlabeled training data, using either speech or singing sources. The model is optimized in an end-to-end fashion without any manual supervision, such as lyrics, musical notes or parallel samples. The proposed approach is fully-convolutional and can generate audio in real-time. Experiments show that our method significantly outperforms the baseline methods while generating convincingly better audio samples than alternative attempts.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Polyak, Adam and Wolf, Lior and Adi, Yossi and Taigman, Yaniv},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02830 [cs, eess]
Issue: arXiv:2008.02830},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{parker_digital_2013,
	title = {A digital model of the {Buchla} lowpass-gate},
	booktitle = {Proc. {Int}. {Conf}. {Digital} audio effects ({DAFx}-13), maynooth, ireland},
	author = {Parker, Julian and D'Angelo, Stephano},
	year = {2013},
	keywords = {⛔ No DOI found},
	pages = {278--285},
}

@article{pakarinen_review_2009,
	title = {A review of digital techniques for modeling vacuum-tube guitar amplifiers},
	volume = {33},
	doi = {10.1162/comj.2009.33.2.85},
	number = {2},
	journal = {Computer Music Journal},
	author = {Pakarinen, Jyri and Yeh, David T},
	year = {2009},
	note = {Publisher: JSTOR},
	pages = {85--100},
}

@inproceedings{okamoto_subband_2017,
	title = {Subband wavenet with overlapped single-sideband filterbanks},
	doi = {10.1109/ASRU.2017.8269005},
	abstract = {Compared with conventional vocoders, deep neural network-based raw audio generative models, such as WaveNet and SampleRNN, can more naturally synthesize speech signals, although the synthesis speed is a problem, especially with high sampling frequency. This paper provides subband WaveNet based on multirate signal processing for high-speed and high-quality synthesis with raw audio generative models. In the training stage, speech waveforms are decomposed and decimated into subband short waveforms with a low sampling rate, and each subband WaveNet network is trained using each subband stream. In the synthesis stage, each generated signal is up-sampled and integrated into a fullband speech signal. The results of objective and subjective experiments for unconditional WaveNet with a sampling frequency of 32 kHz indicate that the proposed subband WaveNet with a square-root Hann window-based overlapped 9-channel single-sideband filterbank can realize about four times the synthesis speed and improve the synthesized speech quality more than the conventional fullband WaveNet.},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and understanding {Workshop} ({ASRU})},
	author = {Okamoto, Takuma and Tachibana, Kentaro and Toda, Tomoki and Shiga, Yoshinori and Kawai, Hisashi},
	month = dec,
	year = {2017},
	keywords = {Acoustics, Amplitude modulation, Hidden Markov models, Speech, Speech synthesis, Training, Vocoders, WaveNet, multirate signal processing, single-sideband filterbank, subband processing},
	pages = {698--704},
}

@inproceedings{okamoto_investigation_2018,
	title = {An investigation of subband {Wavenet} {Vocoder} {Covering} {Entire} {Audible} {Frequency} {Range} with limited {Acoustic} {Features}},
	doi = {10.1109/ICASSP.2018.8462237},
	abstract = {Although a WaveNet vocoder can synthesize more natural-sounding speech waveforms than conventional vocoders with sampling frequencies of 16 and 24 kHz, it is difficult to directly extend the sampling frequency to 48 kHz to cover the entire human audible frequency range for higher-quality synthesis because the model size becomes too large to train with a consumer GPU. For a WaveNet vocoder with a sampling frequency of 48 kHz with a consumer GPU, this paper introduces a subband WaveNet architecture to a speaker-dependent WaveNet vocoder and proposes a subband WaveNet vocoder. In experiments, each conditional subband WaveNet with a sampling frequency of 8 kHz was well trained using a consumer GPU. The results of subjective evaluations with a Japanese male speech corpus indicate that the proposed subband WaveNet vocoder with 36-dimensional simple acoustic features significantly outperformed the conventional source-filter model-based vocoders including STRAIGHT with 86-dimensional features.},
	booktitle = {2018 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Okamoto, Takuma and Tachibana, Kentaro and Toda, Tomoki and Shiga, Yoshinori and Kawai, Hisashi},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Amplitude modulation, Frequency synthesizers, Graphics processing units, Speech synthesis, Training, Vocoders, entire audible frequency range, multirate signal processing, subband WaveNet, vocoder},
	pages = {5654--5658},
}

@article{nistal_darkgan_2021,
	title = {{DarkGAN}: {Exploiting} {Knowledge} {Distillation} for comprehensible {Audio} {Synthesis} with {GANs}},
	shorttitle = {{DarkGAN}},
	abstract = {Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called "soft labels") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.},
	urldate = {2022-03-09},
	journal = {arXiv:2108.01216 [cs, eess]},
	author = {Nistal, Javier and Lattner, Stefan and Richard, Gaël},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.01216 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{nishimura_singing_2016,
	title = {Singing {Voice} {Synthesis} {Based} on {Deep} {Neural} {Networks}},
	doi = {10.21437/Interspeech.2016-1027},
	abstract = {Singing voice synthesis techniques have been proposed based on a hidden Markov model (HMM). In these approaches, the spectrum, excitation, and duration of singing voices are simultaneously modeled with context-dependent HMMs and waveforms are generated from the HMMs themselves. However, the quality of the synthesized singing voices still has not reached that of natural singing voices. Deep neural networks (DNNs) have largely improved on conventional approaches in various research areas including speech recognition, image recognition, speech synthesis, etc. The DNN-based text-to-speech (TTS) synthesis can synthesize high quality speech. In the DNN-based TTS system, a DNN is trained to represent the mapping function from contextual features to acoustic features, which are modeled by decision tree-clustered context dependent HMMs in the HMM-based TTS system. In this paper, we propose singing voice synthesis based on a DNN and evaluate its effectiveness. The relationship between the musical score and its acoustic features is modeled in frames by a DNN. For the sparseness of pitch context in a database, a musical-note-level pitch normalization and linear-interpolation techniques are used to prepare the excitation features. Subjective experimental results show that the DNN-based system outperformed the HMM-based system in terms of naturalness.},
	urldate = {2023-07-24},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Nishimura, Masanari and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = sep,
	year = {2016},
	pages = {2478--2482},
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers diff audio},
	url = {https://www.overleaf.com/project/64664066e45486c6899a1a0a},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	urldate = {2023-08-04},
}

@article{ning_review_2019,
	title = {A {Review} of {Deep} {Learning} {Based} {Speech} {Synthesis}},
	volume = {9},
	issn = {2076-3417},
	doi = {10.3390/app9194050},
	abstract = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
	number = {19},
	urldate = {2023-07-17},
	journal = {Applied Sciences},
	author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
	month = jan,
	year = {2019},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, end-to-end, speech synthesis, text analysis},
	pages = {4050},
}

@inproceedings{nercessian_neural_2020,
	address = {Vienna, Austria},
	title = {Neural {Parametric} {Equalizer} {Matching} {Using} {Differentiable} {Biquads}},
	abstract = {This paper proposes a neural network for carrying out parametric equalizer (EQ) matching. The novelty of this neural network solution is that it can be optimized directly in the frequency domain by means of differentiable biquads, rather than relying solely on a loss on parameter values which does not correlate directly with the system output. We compare the performance of the proposed neural network approach with that of a baseline algorithm based on a convex relaxation of the problem. It is observed that the neural network can provide better matching than the baseline approach because it directly attempts to solve the non-convex problem. Moreover, we show that the same network trained with only a parameter loss is insufficient for the task, despite the fact that it matches underlying EQ parameters better than one trained with a combination of spectral and parameter losses.},
	booktitle = {Proceedings of the 23rd international {Conference} on digital {Audio} {Effects}},
	author = {Nercessian, Shahan},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {8},
}

@inproceedings{nercessian_lightweight_2021,
	address = {Toronto, ON, Canada},
	title = {Lightweight and interpretable {Neural} {Modeling} of an audio {Distortion} {Effect} {Using} {Hyperconditioned} {Differentiable} {Biquads}},
	isbn = {978-1-72817-605-5},
	doi = {10.1109/ICASSP39728.2021.9413996},
	urldate = {2022-03-10},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Nercessian, Shahan and Sarroff, Andy and Werner, Kurt James},
	month = jun,
	year = {2021},
	pages = {890--894},
}

@inproceedings{nercessian_end--end_2021,
	title = {End-to-{End} {Zero}-{Shot} {Voice} {Conversion} {Using} a {DDSP} {Vocoder}},
	doi = {10.1109/WASPAA52581.2021.9632754},
	abstract = {In this paper, we propose a zero-shot voice conversion algorithm using a neural vocoder based on differential digital signal processing. The vocoder does not require auto-regression, and its lightweight, differentiable nature allows the proposed system to be trained in an end-to-end fashion. This enables the use of more perceptually relevant objective functions for model training, and allows feature conversion and vocoder sub-networks to internally learn their own acoustic representation in a data-driven manner. We illustrate the effectiveness of the proposed algorithm by both qualitative and quantitative means, with comparisons to some of our previous works.},
	booktitle = {2021 {IEEE} {Workshop} on applications of signal {Processing} to audio and acoustics ({WASPAA})},
	author = {Nercessian, Shahan},
	month = oct,
	year = {2021},
	note = {ISSN: 1947-1629},
	keywords = {Acoustics, Conferences, Digital signal processing, Linear programming, Signal processing algorithms, Training, Vocoders, differential digital signal processing, end-to-end training, voice conversion, zero-shot learning},
	pages = {1--5},
}

@misc{nakamura_singing_2019,
	title = {Singing voice synthesis based on convolutional neural networks},
	abstract = {The present paper describes a singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. In these systems, the relationship between musical score feature sequences and acoustic feature sequences extracted from singing voices is modeled by DNNs. Then, an acoustic feature sequence of an arbitrary musical score is output in units of frames by the trained DNNs, and a natural trajectory of a singing voice is obtained by using a parameter generation algorithm. As singing voices contain rich expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated in units of segments that consist of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Experimental results in a subjective listening test show that the proposed architecture can synthesize natural sounding singing voices.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Nakamura, Kazuhiro and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.06868 [cs, eess]
Issue: arXiv:1904.06868},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{nachmani_unsupervised_2019,
	title = {Unsupervised {Singing} {Voice} {Conversion}},
	abstract = {We present a deep learning method for singing voice conversion. The proposed network is not conditioned on the text or on the notes, and it directly converts the audio of one singer to the voice of another. Training is performed without any form of supervision: no lyrics or any kind of phonetic features, no notes, and no matching samples between singers. The proposed network employs a single CNN encoder for all singers, a single WaveNet decoder, and a classifier that enforces the latent representation to be singer-agnostic. Each singer is represented by one embedding vector, which the decoder is conditioned on. In order to deal with relatively small datasets, we propose a new data augmentation scheme, as well as new training losses and protocols that are based on backtranslation. Our evaluation presents evidence that the conversion produces natural signing voices that are highly recognizable as the target singer.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Nachmani, Eliya and Wolf, Lior},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.06590 [cs, eess, stat]
Issue: arXiv:1904.06590},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{mumuni_data_2022,
	title = {Data augmentation: {A} comprehensive survey of modern approaches},
	volume = {16},
	issn = {25900056},
	shorttitle = {Data augmentation},
	doi = {10.1016/j.array.2022.100258},
	language = {english},
	urldate = {2023-08-23},
	journal = {Array},
	author = {Mumuni, Alhassan and Mumuni, Fuseini},
	month = dec,
	year = {2022},
	pages = {100258},
}

@article{moog_voltage_1965,
	title = {Voltage controlled electronic music modules},
	volume = {13},
	number = {3},
	journal = {J. Audio Eng. Soc},
	author = {Moog, Robert A.},
	year = {1965},
	keywords = {⛔ No DOI found},
	pages = {200--206},
}

@article{mohammadi_overview_2017,
	title = {An overview of voice conversion systems},
	volume = {88},
	issn = {01676393},
	doi = {10.1016/j.specom.2017.01.008},
	urldate = {2023-07-20},
	journal = {Speech Communication},
	author = {Mohammadi, Seyed Hamidreza and Kain, Alexander},
	month = apr,
	year = {2017},
	pages = {65--82},
}

@inproceedings{nercessian_differentiable_2023,
	title = {Differentiable {WORLD} {Synthesizer}-{Based} {Neural} {Vocoder} {With} {Application} {To} {End}-{To}-{End} {Audio} {Style} {Transfer}},
	abstract = {We propose a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks such as (singing) voice conversion and the DDSP timbre transfer task. Our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We extend the baseline synthesizer by appending lightweight black-box postnets applying further processing in order to improve fidelity. An alternative differentiable approach relies on the extraction of the...},
	urldate = {2023-06-21},
	booktitle = {Audio {Engineering} {Society} {Convention} 154},
	publisher = {Audio Engineering Society},
	author = {Nercessian, Shahan},
	month = may,
	year = {2023},
}

@article{mv_sfnet_2020,
	title = {{SFNet}: {A} {Computationally} {Efficient} {Source} {Filter} {Model} {Based} {Neural} {Speech} {Synthesis}},
	volume = {27},
	issn = {1070-9908, 1558-2361},
	shorttitle = {{SFNet}},
	doi = {10.1109/LSP.2020.3005031},
	urldate = {2023-07-26},
	journal = {IEEE Signal Processing Letters},
	author = {Mv, Achuth Rao and Ghosh, Prasanta Kumar},
	year = {2020},
	pages = {1170--1174},
}

@article{murray_augmentative_2009,
	title = {Augmentative and alternative communication: {A} review of current issues},
	volume = {19},
	issn = {17517222},
	shorttitle = {Augmentative and alternative communication},
	doi = {10.1016/j.paed.2009.05.003},
	language = {english},
	number = {10},
	urldate = {2023-08-24},
	journal = {Paediatrics and Child Health},
	author = {Murray, Janice and Goldbart, Juliet},
	month = oct,
	year = {2009},
	pages = {464--468},
}

@article{morise_world_2016,
	title = {{WORLD}: {A} {Vocoder}-{Based} {High}-{Quality} {Speech} {Synthesis} {System} for real-{Time} {Applications}},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	shorttitle = {{WORLD}},
	doi = {10.1587/transinf.2015EDP7457},
	abstract = {A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of realtime applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.},
	number = {7},
	urldate = {2023-08-16},
	journal = {IEICE Transactions on Information and Systems},
	author = {Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
	year = {2016},
	pages = {1877--1884},
}

@misc{mor_universal_2018,
	title = {A {Universal} {Music} {Translation} {Network}},
	abstract = {We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.},
	urldate = {2023-08-03},
	publisher = {arXiv},
	author = {Mor, Noam and Wolf, Lior and Polyak, Adam and Taigman, Yaniv},
	month = may,
	year = {2018},
	note = {arXiv: 1805.07848 [cs, stat]
Issue: arXiv:1805.07848},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
}

@misc{mitcheltree_modulation_2023,
	title = {Modulation {Extraction} for {LFO}-driven {Audio} {Effects}},
	abstract = {Low frequency oscillator (LFO) driven audio effects such as phaser, flanger, and chorus, modify an input signal using time-varying filters and delays, resulting in characteristic sweeping or widening effects. It has been shown that these effects can be modeled using neural networks when conditioned with the ground truth LFO signal. However, in most cases, the LFO signal is not accessible and measurement from the audio signal is nontrivial, hindering the modeling process. To address this, we propose a framework capable of extracting arbitrary LFO signals from processed audio across multiple digital audio effects, parameter settings, and instrument configurations. Since our system imposes no restrictions on the LFO signal shape, we demonstrate its ability to extract quasiperiodic, combined, and distorted modulation signals that are relevant to effect modeling. Furthermore, we show how coupling the extraction model with a simple processing network enables training of end-to-end black-box models of unseen analog or digital LFO-driven audio effects using only dry and wet audio pairs, overcoming the need to access the audio effect or internal LFO signal. We make our code available and provide the trained audio effect models in a real-time VST plugin.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Mitcheltree, Christopher and Steinmetz, Christian J. and Comunità, Marco and Reiss, Joshua D.},
	month = may,
	year = {2023},
	note = {arXiv: 2305.13262 [cs, eess]
Issue: arXiv:2305.13262},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{michelashvili_hierarchical_2020,
	title = {Hierarchical {Timbre}-painting and {Articulation} {Generation}},
	abstract = {We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre\_painting.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 21th international {Society} for music {Information} {Retrieval} {Conference}},
	author = {Michelashvili, Michael M. and Wolf, Lior},
	month = oct,
	year = {2020},
}

@misc{mehri_samplernn_2017,
	title = {{SampleRNN}: {An} {Unconditional} {End}-to-{End} {Neural} {Audio} {Generation} {Model}},
	shorttitle = {{SampleRNN}},
	abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2017},
	note = {arXiv: 1612.07837 [cs]
Issue: arXiv:1612.07837},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
}

@article{moffat_approaches_2019,
	title = {Approaches in {Intelligent} {Music} {Production}},
	doi = {10.3390/arts8040125},
	number = {Bromham 2016},
	journal = {Arts},
	author = {Moffat, David and Sandler, Mark B},
	year = {2019},
	keywords = {adaptive audio effects, artificial intelligence, audio, audio processing, automatic mixing, intelligent music production, machine learning, processing},
	pages = {1--13},
}

@inproceedings{mitcheltree_serumrnn_2021,
	title = {{SerumRNN}: {Step} by {Step} {Audio} {VST} {Effect} {Programming}},
	booktitle = {Artificial intelligence in music, sound, art and design: 10th international conference, {EvoMUSART} 2021, held as part of {EvoStar} 2021, virtual event, april 7–9, 2021, proceedings 10},
	publisher = {Springer},
	author = {Mitcheltree, Christopher and Koike, Hideki},
	year = {2021},
	keywords = {⛔ No DOI found},
	pages = {218--234},
}

@misc{mccarthy_hooligan_2020,
	title = {{HooliGAN}: {Robust}, {High} {Quality} {Neural} {Vocoding}},
	shorttitle = {{HooliGAN}},
	abstract = {Recent developments in generative models have shown that deep learning combined with traditional digital signal processing (DSP) techniques could successfully generate convincing violin samples [1], that source-excitation combined with WaveNet yields high-quality vocoders [2, 3] and that generative adversarial network (GAN) training can improve naturalness [4, 5]. By combining the ideas in these models we introduce HooliGAN, a robust vocoder that has state of the art results, finetunes very well to smaller datasets ({\textless}30 minutes of speechdata) and generates audio at 2.2MHz on GPU and 35kHz on CPU. We also show a simple modification to Tacotron-basedmodels that allows seamless integration with HooliGAN. Results from our listening tests show the proposed model's ability to consistently output high-quality audio with a variety of datasets, big and small. We provide samples at the following demo page: https://resemble-ai.github.io/hooligan\_demo/},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {McCarthy, Ollie and Ahmed, Zohaib},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02493 [cs, eess]
Issue: arXiv:2008.02493},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{mcadams_hearing_1979,
	title = {Hearing {Musical} {Streams}},
	volume = {3},
	issn = {0148-9267},
	number = {4},
	urldate = {2023-08-10},
	journal = {Computer Music Journal},
	author = {McAdams, Stephen and Bregman, Albert},
	year = {1979},
	note = {JSTOR: 4617866
Publisher: The MIT Press},
	keywords = {⛔ No DOI found},
	pages = {26--60},
}

@article{matsubara_comparison_2022,
	title = {Comparison of real-time multi-speaker neural vocoders on {CPUs}},
	volume = {43},
	issn = {1346-3969, 1347-5177},
	doi = {10.1250/ast.43.121},
	number = {2},
	urldate = {2023-08-14},
	journal = {Acoustical Science and Technology},
	author = {Matsubara, Keisuke and Okamoto, Takuma and Takashima, Ryoichi and Takiguchi, Tetsuya and Toda, Tomoki and Kawai, Hisashi},
	month = mar,
	year = {2022},
	pages = {121--124},
}

@misc{masuda_synthesizer_2021,
	title = {Synthesizer {Sound} {Matching} with {Differentiable} {DSP}},
	abstract = {While synthesizers have become commonplace in music production, many users find it difficult to control the parameters of a synthesizer to create the intended sound. In order to assist the user, the sound matching task aims to estimate synthesis parameters that produce a sound closest to the query sound. Recently, neural networks have been employed for this task. These neural networks are trained on paired data of synthesis parameters and the corresponding output sound, optimizing a loss of synthesis parameters. However, synthesis parameters are only indirectly correlated with the audio output. Another problem is that query made by the user usually consists of real-world sounds, different from the synthesizer output used during training. In this paper, we propose a novel approach to the problem of synthesizer sound matching by implementing a basic subtractive synthesizer using differentiable DSP modules. This synthesizer has interpretable controls and is similar to those used in music production. We can then train an estimator network by directly optimizing the spectral similarity of the synthesized output. Furthermore, we can train the network on real-world sounds whose ground-truth synthesis parameters are unavailable. We pre-train the network with parameter loss and fine-tune the model with spectral loss using real-world sounds. We show that the proposed method finds better matches compared to baseline models.},
	urldate = {2023-06-03},
	author = {Masuda, Naotake and Saito, Daisuke},
	month = nov,
	year = {2021},
	doi = {10.5281/zenodo.5624609},
	note = {Place: Online},
}

@inproceedings{masuda_quality_2021,
	title = {Quality {Diversity} for {Synthesizer} {Sound} {Matching}},
	doi = {10.23919/DAFx51585.2021.9768271},
	abstract = {It is difficult to adjust the parameters of a complex synthesizer to create the desired sound. As such, sound matching, the estimation of synthesis parameters that can replicate a certain sound, is a task that has often been researched, utilizing optimization methods such as genetic algorithm (GA). In this paper, we introduce a novelty-based objective for GA-based sound matching. Our contribution is two-fold. First, we show that the novelty objective is able to improve the quality of sound matching by maintaining phenotypic diversity in the population. Second, we introduce a quality diversity approach to the problem of sound matching, aiming to find a diverse set of matching sounds. We show that the novelty objective is effective in producing high-performing solutions that are diverse in terms of specified audio features. This approach allows for a new way of discovering sounds and exploring the capabilities of a synthesizer.},
	booktitle = {2021 24th international {Conference} on digital {Audio} {Effects} ({DAFx})},
	author = {Masuda, Naotake and Saito, Daisuke},
	month = sep,
	year = {2021},
	note = {ISSN: 2413-6689},
	keywords = {Estimation, Genetic algorithms, Optimization methods, Sociology, Statistics, Synthesizers, Task analysis},
	pages = {300--307},
}

@inproceedings{martinez_ramirez_differentiable_2021,
	address = {Toronto, ON, Canada},
	title = {Differentiable {Signal} {Processing} {With} {Black}-{Box} {Audio} {Effects}},
	isbn = {978-1-72817-605-5},
	doi = {10.1109/ICASSP39728.2021.9415103},
	urldate = {2021-06-21},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Martinez Ramirez, Marco A. and Wang, Oliver and Smaragdis, Paris and Bryan, Nicholas J.},
	month = jun,
	year = {2021},
	pages = {66--70},
}

@article{marelli_timefrequency_2010,
	title = {Time–frequency {Synthesis} of noisy {Sounds} {With} {Narrow} {Spectral} {Components}},
	volume = {18},
	issn = {1558-7924},
	doi = {10.1109/TASL.2010.2040532},
	abstract = {The inverse fast Fourier transform (FFT) method was proposed to alleviate the computational complexity of the additive sound synthesis method in real-time applications, and consists in synthesizing overlapping blocks of samples in the frequency domain. However, its application is limited by its inherent tradeoff between time and frequency resolution. In this paper, we propose an alternative to the inverse FFT method for synthesizing colored noise. The proposed approach uses subband signal processing to generate time-frequency noise with an autocorrelation function such that the noise obtained after converting it to time domain has the desired power spectral density. We show that the inverse FFT method can be interpreted as a particular case of the proposed method, and therefore, the latter offers some extra design flexibility. Exploiting this property, we present experimental results showing that the proposed method can offer a better tradeoff between time and frequency resolution, at the expense of some extra computations.},
	number = {8},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Marelli, Damián and Aramaki, Mitsuko and Kronland-Martinet, Richard and Verron, Charles},
	month = nov,
	year = {2010},
	keywords = {Acoustic noise, Additive synthesis, Colored noise, Computational complexity, Fast Fourier transforms, Frequency domain analysis, Noise generators, Power generation, Signal processing, Signal resolution, Signal synthesis, audio systems, colored noise synthesis, frequency-domain synthesis, time–frequency analysis},
	pages = {1929--1940},
}

@inproceedings{liu_neural_2020,
	title = {Neural {Homomorphic} {Vocoder}},
	doi = {10.21437/Interspeech.2020-3188},
	urldate = {2023-06-21},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Liu, Zhijun and Chen, Kuan and Yu, Kai},
	month = oct,
	year = {2020},
	pages = {240--244},
}

@inproceedings{liu_fastsvc_2021,
	title = {Fastsvc: {Fast} {Cross}-{Domain} {Singing} {Voice} {Conversion} {With} {Feature}-{Wise} {Linear} {Modulation}},
	shorttitle = {Fastsvc},
	doi = {10.1109/ICME51207.2021.9428161},
	abstract = {This paper presents FastSVC, a light-weight cross-domain singing voice conversion (SVC) system, which can achieve high conversion performance, with inference speed 4x faster than real-time on CPUs. FastSVC uses Conformer-based phoneme recognizer to extract singer-agnostic linguistic features from singing signals. A feature-wise linear modulation based generator is used to synthesize waveform directly from linguistic features, leveraging information from sine-excitation signals and loudness features. The waveform generator can be trained conveniently using a multi-resolution spectral loss and an adversarial loss. Experimental results show that the proposed FastSVC system, compared with a computationally heavy baseline system, can achieve comparable conversion performance in some scenarios and significantly better conversion performance in other scenarios. Moreover, the proposed FastSVC system achieves desirable cross-lingual singing conversion performance. The inference speed of the FastSVC system is 3x and 70x faster than the baseline system on GPUs and CPUs, respectively.},
	booktitle = {2021 {IEEE} {International} {Conference} on multimedia and expo ({ICME})},
	author = {Liu, Songxiang and Cao, Yuewen and Hu, Na and Su, Dan and Meng, Helen},
	month = jul,
	year = {2021},
	note = {ISSN: 1945-788X},
	keywords = {Conferences, Feature extraction, Linguistics, Modulation, Multimedia systems, Signal generators, Singing voice conversion, Static VAr compensators, cross-domain, generative adversarial network},
	pages = {1--6},
}

@inproceedings{le_groux_perceptsynth_2008,
	address = {Las Vegas, NV, USA},
	title = {Perceptsynth: {Mapping} perceptual musical features to sound synthesis parameters},
	isbn = {978-1-4244-1483-3 978-1-4244-1484-0},
	shorttitle = {Perceptsynth},
	doi = {10.1109/ICASSP.2008.4517562},
	abstract = {This paper presents a new system that allows for intuitive control of an additive sound synthesis model from perceptually relevant high-level sonic features. We suggest a general framework for the extraction, abstraction, reproduction and transformation of timbral characteristics of a sound analyzed from recordings. We propose a method to train, tune and evaluate our system in an automatic, consistent and reproducible fashion, and show that this system yields various original audio and musical applications.},
	urldate = {2019-12-19},
	booktitle = {2008 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing}},
	publisher = {IEEE},
	author = {Le Groux, Sylvain and Verschure, Paul FMJ},
	month = mar,
	year = {2008},
	keywords = {to read this week},
	pages = {125--128},
}

@article{masuda_improving_2023,
	title = {Improving semi-{Supervised} {Differentiable} {Synthesizer} {Sound} {Matching} for practical {Applications}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2023.3237161},
	abstract = {While synthesizers have become commonplace in music production, many users find it difficult to control the parameters of a synthesizer to create a sound as they intended. In order to assist the user, the sound matching task aims to estimate synthesis parameters that produce a sound that is as close as possible to the query sound. Recently, neural networks have been employed for this task. These neural networks are trained on paired data of synthesis parameters and the corresponding output sound, optimizing a loss of synthesis parameters. However, query by the user usually consists of real-world sounds, different from the synthesizer output sounds used as training data. In a previous work, the authors presented a sound matching method where the synthesizer is implemented using differentiable DSP. The estimator network could then be trained by directly optimizing the spectral similarity between the original sound and the output sound. Furthermore, the network could be trained on real-world sounds whose ground-truth synthesis parameters are unavailable. This method was shown to improve the match quality in both objective and subjective measures. In this work, we experiment with different synthesizer configurations and extend this approach to a more practical synthesizer with effect modules and envelope generators. We propose a novel training strategy where the network is fully trained using both parameter loss and spectral loss. We show that models trained using this strategy is able to utilize the chorus effect effectively while models that switch completely to spectral loss underutilizes the chorus effect.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Masuda, Naotake and Saito, Daisuke},
	year = {2023},
	keywords = {Artificial neural networks, Audio synthesis, Estimation, Genetic algorithms, Speech processing, Synthesizers, Task analysis, Training, music information retrieval, neural network, semi-supervised learning, synthesizer},
	pages = {863--875},
}

@misc{lorenzo-trueba_towards_2019,
	title = {Towards achieving robust universal neural vocoding},
	abstract = {This paper explores the potential universality of neural vocoders. We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is shown to be capable of generating speech of consistently good quality (98\% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality. When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75\%. These results are shown to be consistent across languages, regardless of them being seen during training (e.g. English or Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric).},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Lorenzo-Trueba, Jaime and Drugman, Thomas and Latorre, Javier and Merritt, Thomas and Putrycz, Bartosz and Barra-Chicote, Roberto and Moinet, Alexis and Aggarwal, Vatsal},
	month = jul,
	year = {2019},
	note = {arXiv: 1811.06292 [cs, eess]
Issue: arXiv:1811.06292},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{liu_diffsinger_2022,
	title = {{DiffSinger}: {Singing} {Voice} {Synthesis} via shallow {Diffusion} {Mechanism}},
	shorttitle = {{DiffSinger}},
	abstract = {Singing voice synthesis (SVS) systems are built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing. In this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain that iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generate realistic outputs. To further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we propose boundary prediction methods to locate the intersection and determine the shallow step adaptively. The evaluations conducted on a Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Extensional experiments also prove the generalization of our methods on text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io. Codes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this work: "Diffsinger: Diffusion acoustic model for singing voice synthesis".},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Liu, Jinglin and Li, Chengxi and Ren, Yi and Chen, Feiyang and Zhao, Zhou},
	month = mar,
	year = {2022},
	note = {arXiv: 2105.02446 [cs, eess]
Issue: arXiv:2105.02446},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@book{lerch_introduction_2023,
	address = {Hoboken, New Jersey},
	edition = {Second edition},
	title = {An introduction to audio content analysis: {Music} information retrieval tasks \& applications},
	isbn = {978-1-119-89097-3 978-1-119-89096-6},
	shorttitle = {An introduction to audio content analysis},
	abstract = {"This book explores algorithms that extract the relevant content information from a digital audio signal and interpret the extracted information for applications such as music recommendation, music tutoring, or music generation. This second edition covers an even broader range of tasks aimed at extracting all forms of musical content from the audio. After a general introduction in the process and goals of audio content analysis, the text focuses on tasks that can be grouped into a content category: tonal analysis, intensity analysis, and temporal analysis. The following chapters often span multiple of these categories and cover the alignment of two audio sequences, the classification of musical genre, mood, and instruments, the computation of music similarity, and the assessment of music performance"–},
	publisher = {Wiley-IEEE Press},
	author = {Lerch, Alexander},
	collaborator = {Sons, John Wiley \&},
	year = {2023},
	note = {tex.lccn: TK7881.4},
	keywords = {Computational auditory scene analysis, Computer sound processing, Content analysis (Communication), Data processing},
}

@article{lee_differentiable_2022,
	title = {Differentiable {Artificial} {Reverberation}},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3193298},
	abstract = {Artificial reverberation (AR) models play a central role in various audio applications. Therefore, estimating the AR model parameters (ARPs) of a reference reverberation is a crucial task. Although a few recent deep-learning-based approaches have shown promising performance, their non-end-to-end training scheme prevents them from fully exploiting the potential of deep neural networks. This motivates the introduction of differentiable artificial reverberation (DAR) models, allowing loss gradients to be back-propagated end-to-end. However, implementing the AR models with their difference equations “as is” in the deep learning framework severely bottlenecks the training speed when executed with a parallel processor like GPU due to their infinite impulse response (IIR) components. We tackle this problem by replacing the IIR filters with finite impulse response (FIR) approximations with the frequency-sampling method. Using this technique, we implement three DAR models—differentiable Filtered Velvet Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and Delay Network (DN). For each AR model, we train its ARP estimation networks for analysis-synthesis (RIR-to-ARP) and blind estimation (reverberant-speech-to-ARP) task in an end-to-end manner with its DAR model counterpart. Experiment results show that the proposed method achieves consistent performance improvement over the non-end-to-end approaches in both objective metrics and subjective listening test results.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Lee, Sungho and Choi, Hyeong-Seok and Lee, Kyogu},
	year = {2022},
	keywords = {Digital signal processing, Estimation, Finite impulse response filters, IIR filters, Psychoacoustic models, Reverberation, Task analysis, Training, acoustics, artificial reverberation, deep learning, reverberation},
	pages = {2541--2556},
}

@inproceedings{kumar_melgan_2019,
	title = {{MelGAN}: {Generative} {Adversarial} {Networks} for conditional {Waveform} {Synthesis}},
	volume = {32},
	shorttitle = {{MelGAN}},
	abstract = {Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.},
	urldate = {2023-08-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brébisson, Alexandre and Bengio, Yoshua and Courville, Aaron C},
	year = {2019},
	keywords = {⛔ No DOI found},
}

@article{krumhansl_why_1989,
	title = {Why is musical timbre so hard to understand},
	volume = {9},
	urldate = {2022-11-07},
	journal = {Structure and perception of electroacoustic sound and music},
	author = {Krumhansl, Carol L},
	year = {1989},
	keywords = {⛔ No DOI found},
	pages = {43--55},
}

@inproceedings{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for efficient and high {Fidelity} {Speech} {Synthesis}},
	volume = {33},
	shorttitle = {{HiFi}-{GAN}},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {17022--17033},
}

@misc{kong_diffwave_2021,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.09761 [cs, eess, stat]
Issue: arXiv:2009.09761},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{kobayashi_design_1990,
	title = {Design of {IIR} digital filters with arbitrary log magnitude function by {WLS} techniques},
	volume = {38},
	issn = {0096-3518},
	doi = {10.1109/29.103060},
	abstract = {The authors propose a technique for designing IIR (infinite impulse response) digital filters to have an arbitrary log magnitude frequency response. The technique is based on an iterative weighted least-squares (WLS) approach in the frequency domain. A weight updating procedure is introduced to obtain a nearly optimal approximation to the given log magnitude function in the least-squares sense. The weighting function is updated using the results of the previous iteration in such a way that the weighted error approximates the log magnitude error. Filter coefficients at each iteration are efficiently computed using a fast recursive algorithm for a set of linear equations derived from the WLS problem. Several design examples demonstrate the rapid convergence of the design algorithm. The algorithm is extended to equiripple approximation by means of a minor modification of the weight updating procedure.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Kobayashi, T. and Imai, S.},
	month = feb,
	year = {1990},
	keywords = {Algorithm design and analysis, Convergence, Digital filters, Equations, Error correction, Frequency domain analysis, Frequency response, IIR filters, Iterative methods, Nonlinear filters},
	pages = {247--252},
}

@article{klatt_review_1987,
	title = {Review of text-to-speech conversion for {English}},
	volume = {82},
	issn = {0001-4966},
	doi = {10.1121/1.395275},
	abstract = {LINGUISTIC REPRESENTATION: /•"o)'et hzz s'up./ locatioonfsthesteype'sofboundariEeasc. hsyllabolefa wordin a sentencecanbeassigneda strengthor stresslevel. Differencesin assignedstressmakesomesyllablesstandout fromtheothers.The stresspatternhasaneffectonthedurationsof soundsand on the pitchchangesoveran utterance (fundamentalfrequencyof vocal cord vibrations,or fo ). The phonologicaclomponenot f thegrammarconvertsphonemic representationsand information about stressand boundarytypesinto ( 1) a stringof phoneticsegmentsplus (2) a superimposepdatternof timing,intensity,andfo motionsrathelatterthreeaspectsbeingknownassentencperosody.},
	number = {3},
	urldate = {2023-07-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Klatt, Dennis H.},
	month = sep,
	year = {1987},
	pages = {737--793},
}

@inproceedings{kim_neural_2019,
	address = {Brighton, United Kingdom},
	title = {Neural {Music} {Synthesis} for {Flexible} {Timbre} {Control}},
	doi = {10.1109/ICASSP.2019.8683596},
	abstract = {The recent success of raw audio waveform synthesis models like WaveNet motivates a new approach for music synthesis, in which the entire process — creating audio samples from a score and instrument information — is modeled using generative neural networks. This paper describes a neural music synthesis model with flexible timbre controls, which consists of a recurrent neural network conditioned on a learned instrument embedding followed by a WaveNet vocoder. The learned embedding space successfully captures the diverse variations in timbres within a large dataset and enables timbre control and morphing by interpolating between instruments in the embedding space. The synthesis quality is evaluated both numerically and perceptually, and an interactive web demo is presented.},
	urldate = {2019-12-21},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing}},
	author = {Kim, Jong Wook and Bittner, Rachel and Kumar, Aparna and Bello, Juan Pablo},
	year = {2019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, unread},
	pages = {176--180},
}

@inproceedings{kuznetsov_differentiable_2020,
	title = {Differentiable {IIR} filters for machine learning applications},
	abstract = {In this paper we present an approach to using traditional digital IIR filter structures inside deep-learning networks trained using backpropagation. We establish the link between such structures and recurrent neural networks. Three different differentiable IIR filter topologies are presented and compared against each other and an established baseline. Additionally, a simple Wiener-Hammerstein model using differentiable IIRs as its filtering component is presented and trained on a guitar signal played through a Boss DS-1 guitar pedal.},
	booktitle = {{DaFX}},
	author = {Kuznetsov, Boris and Parker, Julian D and Esqueda, Fabián},
	year = {2020},
	keywords = {⛔ No DOI found},
}

@misc{kumar_high-fidelity_2023,
	title = {High-{Fidelity} {Audio} {Compression} with {Improved} {RVQGAN}},
	abstract = {Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves {\textasciitilde}90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Kumar, Rithesh and Seetharaman, Prem and Luebs, Alejandro and Kumar, Ishaan and Kumar, Kundan},
	month = jun,
	year = {2023},
	note = {arXiv: 2306.06546 [cs, eess]
Issue: arXiv:2306.06546},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{kobayashi_statistical_2014,
	title = {Statistical singing voice conversion with direct waveform modification based on the spectrum differential},
	doi = {10.21437/Interspeech.2014-539},
	abstract = {This paper presents a novel statistical singing voice conversion (SVC) technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms. SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice due to various factors, such as analysis and modeling errors in the vocoderbased framework. To alleviate this degradation, we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers' singing voices. The differential spectral feature is directly estimated using a differential Gaussian mixture model (GMM) that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC. The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC.},
	urldate = {2023-07-25},
	booktitle = {Interspeech 2014},
	publisher = {ISCA},
	author = {Kobayashi, Kazuhiro and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
	month = sep,
	year = {2014},
	pages = {2514--2518},
}

@inproceedings{kim_glow-tts_2020,
	title = {Glow-{TTS}: {A} {Generative} {Flow} for text-to-{Speech} via monotonic {Alignment} {Search}},
	volume = {33},
	shorttitle = {Glow-{TTS}},
	abstract = {Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.},
	urldate = {2023-07-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {8067--8077},
}

@article{khan_concatenative_2016,
	title = {Concatenative speech synthesis: {A} review},
	volume = {136},
	doi = {10.5120/ijca2016907992},
	number = {3},
	journal = {International Journal of Computer Applications},
	author = {Khan, Rubeena A and Chitode, Janardan Shrawan},
	year = {2016},
	note = {Publisher: Citeseer},
	pages = {1--6},
}

@inproceedings{kato_neural_2018,
	address = {Salt Lake City, UT},
	title = {Neural {3D} {Mesh} {Renderer}},
	isbn = {978-1-5386-6420-9},
	doi = {10.1109/CVPR.2018.00411},
	abstract = {For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.},
	urldate = {2023-06-21},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on computer {Vision} and pattern {Recognition}},
	publisher = {IEEE},
	author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
	month = jun,
	year = {2018},
	pages = {3907--3916},
}

@inproceedings{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24 kHz 16-bit audio 4 times faster than real time on a GPU. Secondly, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds past sparsity levels of more than 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile phone CPU in real time. Finally, we describe a new dependency scheme for sampling that lets us trade a constant number of non-local, distant dependencies for the ability to generate samples in batches. The Batch WaveRNN produces 8 samples per step without loss of quality and offers orthogonal ways of further increasing sampling efficiency.},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the 35th international {Conference} on machine {Learning}},
	publisher = {PMLR},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2410--2419},
}

@inproceedings{juvela_waveform_2019,
	title = {Waveform generation for text-to-speech {Synthesis} {Using} {Pitch}-synchronous {Multi}-scale {Generative} {Adversarial} {Networks}},
	doi = {10.1109/ICASSP.2019.8683271},
	abstract = {The state-of-the-art in text-to-speech (TTS) synthesis has recently improved considerably due to novel neural waveform generation methods, such as WaveNet. However, these methods suffer from their slow sequential inference process, while their parallel versions are difficult to train and even more computationally expensive. Meanwhile, generative adversarial networks (GANs) have achieved impressive results in image generation and are making their way into audio applications; parallel inference is among their lucrative properties. By adopting recent advances in GAN training techniques, this investigation studies waveform generation for TTS in two domains (speech signal and glottal excitation). Listening test results show that while direct waveform generation with GAN is still far behind WaveNet, a GAN-based glottal excitation model can achieve quality and voice similarity on par with a WaveNet vocoder.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Juvela, Lauri and Bollepalli, Bajibabu and Yamagishi, Junichi and Alku, Paavo},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, GAN, Gallium nitride, Generative adversarial networks, Generators, Neural vocoding, Speech synthesis, Training, Vocoders, glottal excitation model, text-to-speech},
	pages = {6915--6919},
}

@misc{juvela_speaker-independent_2018,
	title = {Speaker-independent raw waveform model for glottal excitation},
	abstract = {Recent speech technology research has seen a growing interest in using WaveNets as statistical vocoders, i.e., generating speech waveforms from acoustic features. These models have been shown to improve the generated speech quality over classical vocoders in many tasks, such as text-to-speech synthesis and voice conversion. Furthermore, conditioning WaveNets with acoustic features allows sharing the waveform generator model across multiple speakers without additional speaker codes. However, multi-speaker WaveNet models require large amounts of training data and computation to cover the entire acoustic space. This paper proposes leveraging the source-filter model of speech production to more effectively train a speaker-independent waveform generator with limited resources. We present a multi-speaker 'GlotNet' vocoder, which utilizes a WaveNet to generate glottal excitation waveforms, which are then used to excite the corresponding vocal tract filter to produce speech. Listening tests show that the proposed model performs favourably to a direct WaveNet vocoder trained with the same model architecture and data.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Juvela, Lauri and Tsiaras, Vassilis and Bollepalli, Bajibabu and Airaksinen, Manu and Yamagishi, Junichi and Alku, Paavo},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09593 [cs, eess, stat]
Issue: arXiv:1804.09593},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{juvela_reducing_2017,
	title = {Reducing mismatch in training of {DNN}-{Based} {Glottal} {Excitation} {Models} in a statistical {Parametric} {Text}-to-{Speech} {System}},
	doi = {10.21437/Interspeech.2017-848},
	abstract = {Neural network-based models that generate glottal excitation waveforms from acoustic features have been found to give improved quality in statistical parametric speech synthesis. Until now, however, these models have been trained separately from the acoustic model. This creates mismatch between training and synthesis, as the synthesized acoustic features used for the excitation model input differ from the original inputs, with which the model was trained on. Furthermore, due to the errors in predicting the vocal tract filter, the original excitation waveforms do not provide perfect reconstruction of the speech waveform even if predicted without error. To address these issues and to make the excitation model more robust against errors in acoustic modeling, this paper proposes two modifications to the excitation model training scheme. First, the excitation model is trained in a connected manner, with inputs generated by the acoustic model. Second, the target glottal waveforms are re-estimated by performing glottal inverse filtering with the predicted vocal tract filters. The results show that both of these modifications improve performance measured in MSE and MFCC distortion, and slightly improve the subjective quality of the synthetic speech.},
	urldate = {2023-07-04},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Juvela, Lauri and Bollepalli, Bajibabu and Yamagishi, Junichi and Alku, Paavo},
	month = aug,
	year = {2017},
	pages = {1368--1372},
}

@book{keller_fundamentals_1994,
	address = {Chichester [England] ; New York},
	title = {Fundamentals of speech synthesis and speech recognition: {Basic} concepts, state of the art, and future challenges},
	isbn = {978-0-471-94449-2},
	shorttitle = {Fundamentals of speech synthesis and speech recognition},
	publisher = {Wiley},
	editor = {Keller, Eric},
	year = {1994},
	note = {tex.lccn: TK7882.S65 F86 1994},
	keywords = {Automatic speech recognition, Speech synthesis},
}

@inproceedings{kawamura_differentiable_2022,
	address = {Singapore, Singapore},
	title = {Differentiable digital {Signal} {Processing} {Mixture} {Model} for synthesis {Parameter} {Extraction} from mixture of harmonic {Sounds}},
	isbn = {978-1-66540-540-9},
	doi = {10.1109/ICASSP43922.2022.9746399},
	urldate = {2023-03-26},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kawamura, Masaya and Nakamura, Tomohiko and Kitamura, Daichi and Saruwatari, Hiroshi and Takahashi, Yu and Kondo, Kazunobu},
	month = may,
	year = {2022},
	pages = {941--945},
}

@inproceedings{kaneko_istftnet_2022,
	title = {{ISTFTNET}: {Fast} and lightweight {Mel}-{Spectrogram} {Vocoder} {Incorporating} {Inverse} {Short}-{Time} {Fourier} {Transform}},
	shorttitle = {{ISTFTNET}},
	doi = {10.1109/ICASSP43922.2022.9746713},
	abstract = {In recent text-to-speech synthesis and voice conversion systems, a mel-spectrogram is commonly applied as an intermediate representation, and the necessity for a mel-spectrogram vocoder is increasing. A mel-spectrogram vocoder must solve three inverse problems: recovery of the original-scale magnitude spectrogram, phase reconstruction, and frequency-to-time conversion. A typical convolutional mel-spectrogram vocoder solves these problems jointly and implicitly using a convolutional neural network, including temporal upsampling layers, when directly calculating a raw waveform. Such an approach allows skipping redundant processes during waveform synthesis (e.g., the direct reconstruction of high-dimensional original-scale spectrograms). By contrast, the approach solves all problems in a black box and cannot effectively employ the time-frequency structures existing in a mel-spectrogram. We thus propose iSTFTNet, which replaces some output-side layers of the mel-spectrogram vocoder with the inverse short-time Fourier transform (iSTFT) after sufficiently reducing the frequency dimension using upsampling layers, reducing the computational cost from black-box modeling and avoiding redundant estimations of high-dimensional spectrograms. During our experiments, we applied our ideas to three HiFi-GAN variants and made the models faster and more lightweight with a reasonable speech quality.1},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Kaneko, Takuhiro and Tanaka, Kou and Kameoka, Hirokazu and Seki, Shogo},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Computational modeling, Estimation, Fast Fourier transforms, Frequency conversion, Frequency estimation, Time-frequency analysis, Vocoders, Waveform synthesis, convolutional neural network, generative adversarial networks, inverse short-time Fourier transform, mel-spectrogram vocoder},
	pages = {6207--6211},
}

@article{juvela_glotnetraw_2019,
	title = {{GlotNet}—{A} {Raw} {Waveform} {Model} for the glottal {Excitation} in statistical {Parametric} {Speech} {Synthesis}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2906484},
	abstract = {Recently, generative neural network models which operate directly on raw audio, such as WaveNet, have improved the state of the art in text-to-speech synthesis (TTS). Moreover, there is increasing interest in using these models as statistical vocoders for generating speech waveforms from various acoustic features. However, there is also a need to reduce the model complexity, without compromising the synthesis quality. Previously, glottal pulseforms (i.e., time-domain waveforms corresponding to the source of human voice production mechanism) have been successfully synthesized in TTS by glottal vocoders using straightforward deep feedforward neural networks. Therefore, it is natural to extend the glottal waveform modeling domain to use the more powerful WaveNet-like architecture. Furthermore, due to their inherent simplicity, glottal excitation waveforms permit scaling down the waveform generator architecture. In this study, we present a raw waveform glottal excitation model, called GlotNet, and compare its performance with the corresponding direct speech waveform model, WaveNet, using equivalent architectures. The models are evaluated as part of a statistical parametric TTS system. Listening test results show that both approaches are rated highly in voice similarity to the target speaker, and obtain similar quality ratings with large models. Furthermore, when the model size is reduced, the quality degradation is less severe for GlotNet.},
	number = {6},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Juvela, Lauri and Bollepalli, Bajibabu and Tsiaras, Vassilis and Alku, Paavo},
	month = jun,
	year = {2019},
	keywords = {Acoustics, Computational modeling, Glottal source model, Hidden Markov models, Neural networks, Speech synthesis, Vocoders, WaveNet, text-to-speech},
	pages = {1019--1030},
}

@inproceedings{juvela_gelp_2019,
	title = {{GELP}: {GAN}-{Excited} {Linear} {Prediction} for speech {Synthesis} from mel-{Spectrogram}},
	shorttitle = {{GELP}},
	doi = {10.21437/Interspeech.2019-2008},
	urldate = {2023-06-21},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Juvela, Lauri and Bollepalli, Bajibabu and Yamagishi, Junichi and Alku, Paavo},
	month = sep,
	year = {2019},
	keywords = {read},
	pages = {694--698},
}

@inproceedings{jonason_control-synthesis_2020,
	title = {The control-synthesis approach for making expressive and controllable neural music synthesizers},
	abstract = {Deep neural networks have been successfully applied to audio synthesis. Such neural audio generation models can efficiently learn from data how to imitate a variety of instruments, such as piano and violin. However, effective control of these models is difficult. We introduce the “control-synthesis approach” to make neural audio synthesizers more controllable. This approach transforms user input into intermediate features to condition a neural audio synthesis model. We demonstrate this approach by implementing MIDI-controllable neural audio synthesizers and generating several examples for audition.},
	booktitle = {Proceedings of the 2020 {AI} {Music} {Creativity} {Conference}},
	author = {Jonason, Nicolas and Sturm, Bob L T and Thome, Carl},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {9},
}

@misc{ito_lj_2017,
	title = {The {LJ} speech dataset},
	author = {Ito, Keith and Johnson, Linda},
	year = {2017},
}

@inproceedings{huovilainen_new_2005,
	title = {New approaches to digital subtractive synthesis},
	booktitle = {{ICMC}},
	author = {Huovilainen, Antti and Välimäki, Vesa},
	year = {2005},
	keywords = {⛔ No DOI found},
}

@misc{huang_timbretron_2019,
	title = {{TimbreTron}: {A} {WaveNet}({CycleGAN}({CQT}({Audio}))) pipeline for musical {Timbre} {Transfer}},
	shorttitle = {{TimbreTron}},
	abstract = {In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies "image" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Huang, Sicong and Li, Qiyang and Anil, Cem and Bao, Xuchan and Oore, Sageev and Grosse, Roger B.},
	month = may,
	year = {2019},
	note = {arXiv: 1811.09620 [cs, eess, stat]
Issue: arXiv:1811.09620},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{jin_fftnet_2018,
	title = {Fftnet: {A} {Real}-{Time} {Speaker}-{Dependent} {Neural} {Vocoder}},
	shorttitle = {Fftnet},
	doi = {10.1109/ICASSP.2018.8462431},
	abstract = {We introduce FFTNet, a deep learning approach synthesizing audio waveforms. Our approach builds on the recent WaveNet project, which showed that it was possible to synthesize a natural sounding audio waveform directly from a deep convolutional neural network. FFTNet offers two improvements over WaveNet. First it is substantially faster, allowing for real-time synthesis of audio waveforms. Second, when used as a vocoder, the resulting speech sounds more natural, as measured via a “mean opinion score” test.},
	booktitle = {2018 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Jin, Zeyu and Finkelstein, Adam and Mysore, Gautham J. and Lu, Jingwan},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Computer architecture, Convolution, FFTNet, Machine learning, Microsoft Windows, Real-time systems, Training, Vocoders, WaveNet, neural networks, vocoder},
	pages = {2251--2255},
}

@article{jehan_audio-driven_nodate,
	title = {An {Audio}-{Driven} {Perceptually} {Meaningful} {Timbre} {Synthesizer}},
	abstract = {A real-time synthesis engine that models and predicts the timbre of acoustic instruments based on perceptual features is presented. The timbre characteristics and the mapping between control and timbre parameters are inferred from recorded musical data. In the synthesis step, timbre data is predicted based on new control data enabling applications such as synthesis and cross-synthesis of acoustic instruments and timbre morphing between instrument families. The system is fully implemented in the Max/MSP environment.},
	author = {Jehan, Tristan and Schoner, Bernd},
	keywords = {⛔ No DOI found},
}

@inproceedings{isola_image--image_2017,
	address = {Honolulu, HI},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	isbn = {978-1-5386-0457-1},
	doi = {10.1109/CVPR.2017.632},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.},
	language = {english},
	urldate = {2023-08-25},
	booktitle = {2017 {IEEE} {Conference} on computer {Vision} and pattern {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = jul,
	year = {2017},
	pages = {5967--5976},
}

@inproceedings{hunt_unit_1996,
	title = {Unit selection in a concatenative speech synthesis system using a large speech database},
	volume = {1},
	doi = {10.1109/ICASSP.1996.541110},
	abstract = {One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.},
	booktitle = {1996 {IEEE} {International} {Conference} on acoustics, speech, and signal {Processing} {Conference} {Proceedings}},
	author = {Hunt, A.J. and Black, A.W.},
	month = may,
	year = {1996},
	note = {ISSN: 1520-6149},
	keywords = {Control system synthesis, Costs, Databases, Laboratories, Natural languages, Network synthesis, Speech recognition, Speech synthesis, State estimation, Viterbi algorithm},
	pages = {373--376 vol. 1},
}

@misc{huang_singing_2023,
	title = {The {Singing} {Voice} {Conversion} {Challenge} 2023},
	abstract = {We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Huang, Wen-Chin and Violeta, Lester Phillip and Liu, Songxiang and Shi, Jiatong and Toda, Tomoki},
	month = jul,
	year = {2023},
	note = {arXiv: 2306.14422 [cs, eess]
Issue: arXiv:2306.14422},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{hsu_towards_2020,
	title = {Towards {Robust} {Neural} {Vocoding} for {Speech} {Generation}: {A} {Survey}},
	shorttitle = {Towards {Robust} {Neural} {Vocoding} for {Speech} {Generation}},
	abstract = {Recently, neural vocoders have been widely used in speech synthesis tasks, including text-to-speech and voice conversion. However, when encountering data distribution mismatch between training and inference, neural vocoders trained on real data often degrade in voice quality for unseen scenarios. In this paper, we train four common neural vocoders, including WaveNet, WaveRNN, FFTNet, Parallel WaveGAN alternately on five different datasets. To study the robustness of neural vocoders, we evaluate the models using acoustic features from seen/unseen speakers, seen/unseen languages, a text-to-speech model, and a voice conversion model. We found out that the speaker variety is much more important for achieving a universal vocoder than the language. Through our experiments, we show that WaveNet and WaveRNN are more suitable for text-to-speech models, while Parallel WaveGAN is more suitable for voice conversion applications. Great amount of subjective MOS results in naturalness for all vocoders are presented for future studies.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Hsu, Po-chun and Wang, Chun-hsuan and Liu, Andy T. and Lee, Hung-yi},
	month = aug,
	year = {2020},
	note = {arXiv: 1912.02461 [cs, eess]
Issue: arXiv:1912.02461},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{horner_machine_1993,
	title = {Machine tongues {XVI}. {Genetic} algorithms and their application to {FM} matching synthesis},
	volume = {17},
	issn = {01489267},
	doi = {10.2307/3680541},
	abstract = {Historically, frequency modulation (FM) synthesis has required trial and error to create emulations of natural sounds. This article presents a genetic-algorithm-based technique which determines optimized param-eters for reconstruction through FM synthesis of a sound having harmonic partials. We obtain the best results by using invariant modulation indices and a multiple carrier formant FM synthesis model. We present our results along with extensions and gener-alizations of the technique. Background},
	number = {4},
	journal = {Computer Music Journal},
	author = {Horner, Andrew and Beauchamp, James and Haken, Lippold},
	year = {1993},
	pages = {17--29},
}

@article{hono_sinsy_2021,
	title = {Sinsy: {A} {Deep} {Neural} {Network}-{Based} {Singing} {Voice} {Synthesis} {System}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Sinsy},
	doi = {10.1109/TASLP.2021.3104165},
	urldate = {2023-06-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hono, Yukiya and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	year = {2021},
	pages = {2803--2815},
}

@misc{huang_counterpoint_2019,
	title = {Counterpoint by {Convolution}},
	abstract = {Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE (Uria et al., 2014), which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling, based on both log-likelihood and human evaluation.},
	urldate = {2023-08-03},
	publisher = {arXiv},
	author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07227 [cs, eess, stat]
Issue: arXiv:1903.07227},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, H.5.5, I.2, Statistics - Machine Learning},
}

@inproceedings{huang_active_2014,
	address = {Haifa Israel},
	title = {Active learning of intuitive control knobs for synthesizers using gaussian processes},
	isbn = {978-1-4503-2184-6},
	doi = {10.1145/2557500.2557544},
	urldate = {2023-06-23},
	booktitle = {Proceedings of the 19th international conference on intelligent {User} {Interfaces}},
	publisher = {ACM},
	author = {Huang, Cheng-Zhi Anna and Duvenaud, David and Arnold, Kenneth C. and Partridge, Brenton and Oberholtzer, Josiah W. and Gajos, Krzysztof Z.},
	month = feb,
	year = {2014},
	pages = {115--124},
}

@article{hono_periodnet_2021-1,
	title = {{PeriodNet}: {A} {Non}-{Autoregressive} {Raw} {Waveform} {Generative} {Model} {With} a structure {Separating} {Periodic} and aperiodic {Components}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{PeriodNet}},
	doi = {10.1109/ACCESS.2021.3118033},
	urldate = {2023-07-25},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Hono, Yukiya and Takaki, Shinji and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	year = {2021},
	pages = {137599--137612},
}

@inproceedings{hono_periodnet_2021,
	address = {Toronto, ON, Canada},
	title = {Periodnet: {A} {Non}-{Autoregressive} {Waveform} {Generation} {Model} with a structure {Separating} {Periodic} and aperiodic {Components}},
	isbn = {978-1-72817-605-5},
	shorttitle = {Periodnet},
	doi = {10.1109/ICASSP39728.2021.9414401},
	urldate = {2023-06-22},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Hono, Yukiya and Takaki, Shinji and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = jun,
	year = {2021},
	pages = {6049--6053},
}

@misc{hayes_sinusoidal_2022,
	title = {Sinusoidal {Frequency} {Estimation} by {Gradient} {Descent}},
	abstract = {Sinusoidal parameter estimation is a fundamental task in applications from spectral analysis to time-series forecasting. Estimating the sinusoidal frequency parameter by gradient descent is, however, often impossible as the error function is non-convex and densely populated with local minima. The growing family of differentiable signal processing methods has therefore been unable to tune the frequency of oscillatory components, preventing their use in a broad range of applications. This work presents a technique for joint sinusoidal frequency and amplitude estimation using the Wirtinger derivatives of a complex exponential surrogate and any first order gradient-based optimizer, enabling end to-end training of neural network controllers for unconstrained sinusoidal models.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2210.14476},
	note = {arXiv: 2210.14476 [cs, eess]
Issue: arXiv:2210.14476
tex.copyright: All rights reserved},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@article{hayes_responsibility_2023,
	title = {The responsibility problem in neural networks with unordered targets},
	abstract = {We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data.},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gyorgy},
	year = {2023},
	keywords = {⛔ No DOI found},
}

@inproceedings{hayes_neural_2021,
	address = {Online},
	title = {Neural {Waveshaping} {Synthesis}},
	booktitle = {Proceedings of the 22nd international {Society} for music {Information} {Retrieval} {Conference}},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = nov,
	year = {2021},
	note = {tex.copyright: All rights reserved},
}

@misc{hawthorne_enabling_2019,
	title = {Enabling factorized {Piano} {Music} {Modeling} and generation with the {MAESTRO} {Dataset}},
	abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\textasciitilde}0.1 ms to {\textasciitilde}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\textasciitilde}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
	urldate = {2023-08-01},
	publisher = {arXiv},
	author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
	month = jan,
	year = {2019},
	note = {arXiv: 1810.12247 [cs, eess, stat]
Issue: arXiv:1810.12247},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@inproceedings{govalkar_comparison_2019,
	title = {A comparison of recent {Neural} {Vocoders} for speech {Signal} {Reconstruction}},
	doi = {10.21437/SSW.2019-2},
	abstract = {In recent years, text-to-speech (TTS) synthesis has benefited from advanced machine learning approaches. Most prominently, since the introduction of the WaveNet architecture, neural vocoders have exhibited superior performance in terms of the naturalness of synthesized speech signals in comparison to traditional vocoders. In this paper, a fair comparison of recent neural vocoders is presented in a signal reconstruction scenario. That means we use such techniques to resynthesize speech waveforms from mel-scaled spectrograms, a compact and generally non-invertible representation of the underlying audio signal. In that context, we conduct listening tests according to the well established MUSHRA standard and compare the attained results to similar studies. Weighing off the perceptual quality to the computational requirements, our findings shall serve as a guideline to both practitioners and researchers in speech synthesis.},
	urldate = {2023-08-16},
	booktitle = {10th {ISCA} {Workshop} on {Speech} {Synthesis} ({SSW} 10)},
	publisher = {ISCA},
	author = {Govalkar, Prachi and Fischer, Johannes and Zalkow, Frank and Dittmar, Christian},
	month = sep,
	year = {2019},
	pages = {7--12},
}

@misc{hawthorne_multi-instrument_2022,
	title = {Multi-instrument {Music} {Synthesis} with {Spectrogram} {Diffusion}},
	abstract = {An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr{\textbackslash}'echet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Hawthorne, Curtis and Simon, Ian and Roberts, Adam and Zeghidour, Neil and Gardner, Josh and Manilow, Ethan and Engel, Jesse},
	month = dec,
	year = {2022},
	note = {arXiv: 2206.05408 [cs, eess]
Issue: arXiv:2206.05408},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{hantrakul_fast_2019,
	address = {Delft, The Netherlands},
	title = {Fast and {Flexible} {Neural} {Audio} {Synthesis}},
	booktitle = {Proceedings of the 20th international {Society} for music {Information} {Retrieval} {Conference}},
	author = {Hantrakul, Lamtharn and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
	keywords = {unread, ⛔ No DOI found},
	pages = {524--530},
}

@misc{han_perceptual-neural-physical_2023,
	title = {Perceptual-{Neural}-{Physical} {Sound} {Matching}},
	abstract = {Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis. Deep neural networks have achieved promising results in matching sustained harmonic tones. However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion. We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast as P-loss during training. We instantiate PNP with physical modeling synthesis as decoder and joint time-frequency scattering transform (JTFS) as spectral representation. We demonstrate its potential on matching synthetic drum sounds in comparison with other loss functions.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Han, Han and Lostanlen, Vincent and Lagrange, Mathieu},
	month = mar,
	year = {2023},
	note = {arXiv: 2301.02886 [cs, eess]
Issue: arXiv:2301.02886},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{hagiwara_modeling_2022,
	title = {Modeling {Animal} {Vocalizations} through {Synthesizers}},
	abstract = {Modeling real-world sound is a fundamental problem in the creative use of machine learning and many other fields, including human speech processing and bioacoustics. Transformer-based generative models and some prior work (e.g., DDSP) are known to produce realistic sound, although they have limited control and are hard to interpret. As an alternative, we aim to use modular synthesizers, i.e., compositional, parametric electronic musical instruments, for modeling non-music sounds. However, inferring synthesizer parameters given a target sound, i.e., the parameter inference task, is not trivial for general sounds, and past research has typically focused on musical sound. In this work, we optimize a differentiable synthesizer from TorchSynth in order to model, emulate, and creatively generate animal vocalizations. We compare an array of optimization methods, from gradient-based search to genetic algorithms, for inferring its parameters, and then demonstrate how one can control and interpret the parameters for modeling non-music sounds.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Hagiwara, Masato and Cusimano, Maddie and Liu, Jen-Yu},
	month = oct,
	year = {2022},
	note = {arXiv: 2210.10857 [cs, eess]
Issue: arXiv:2210.10857},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{guo_improving_2022,
	title = {Improving adversarial {Waveform} {Generation} {Based} {Singing} {Voice} {Conversion} with harmonic {Signals}},
	doi = {10.1109/ICASSP43922.2022.9746709},
	abstract = {Adversarial waveform generation has been a popular approach as the backend of singing voice conversion (SVC) to generate high-quality singing audio. However, the instability of GAN also leads to other problems, such as pitch jitters and U/V errors. It affects the smoothness and continuity of harmonics, hence degrades the conversion quality seriously. This paper proposes to feed harmonic signals to the SVC model in advance to enhance audio generation. We extract the sine excitation from the pitch, and filter it with a linear time-varying (LTV) filter estimated by a neural network. Both these two harmonic signals are adopted as the inputs to generate the singing waveform. In our experiments, two mainstream models, MelGAN and ParallelWaveGAN, are investigated to validate the effectiveness of the proposed approach. We conduct a MOS test on clean and noisy test sets. The result shows that both signals significantly improve SVC in fidelity and timbre similarity. Besides, the case analysis further validates that this method enhances the smoothness and continuity of harmonics in the generated audio, and the filtered excitation better matches the target audio.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Guo, Haohan and Zhou, Zhiping and Meng, Fanbo and Liu, Kai},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {GAN, Harmonic analysis, Matched filters, Maximum likelihood detection, Nonlinear filters, Signal processing, Static VAr compensators, Timbre, harmonic signal, neural vocoder, sine excitation, singing voice conversion},
	pages = {6657--6661},
}

@misc{guo_dent-ddsp_2022,
	title = {{DENT}-{DDSP}: {Data}-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition},
	shorttitle = {{DENT}-{DDSP}},
	abstract = {The performances of automatic speech recognition (ASR) systems degrade drastically under noisy conditions. Explicit distortion modelling (EDM), as a feature compensation step, is able to enhance ASR systems under such conditions by simulating the in-domain noisy speeches from the clean counterparts. Yet, existing distortion models are either non-trainable or unexplainable and often lack controllability and generalization ability. In this paper, we propose a fully explainable and controllable model: DENT-DDSP to achieve EDM. DENT-DDSP utilizes novel differentiable digital signal processing (DDSP) components and requires only 10 seconds of training data to achieve high fidelity. The experiment shows that the simulated noisy data from DENT-DDSP achieves the highest simulation fidelity compared to other baseline models in terms of multi-scale spectral loss (MSSL). Moreover, to validate whether the data simulated by DENT-DDSP are able to replace the scarce in-domain noisy data in the noise-robust ASR tasks, several downstream ASR models with the same architecture are trained using the simulated data and the real data. The experiment shows that the model trained with the simulated noisy data from DENT-DDSP achieves similar performances to the benchmark with a 2.7{\textbackslash}\% difference in terms of word error rate (WER). The code of the model is released online.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Guo, Z. and Chen, C. and Chng, E. S.},
	month = aug,
	year = {2022},
	note = {arXiv: 2208.00987 [cs, eess]
Issue: arXiv:2208.00987},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{germain_equalization_2016,
	title = {Equalization matching of speech recordings in real-world environments},
	doi = {10.1109/ICASSP.2016.7471747},
	abstract = {When different parts of speech content such as voice-overs and narration are recorded in real-world environments with different acoustic properties and background noise, the difference in sound quality between the recordings is typically quite audible and therefore undesirable. We propose an algorithm to equalize multiple such speech recordings so that they sound like they were recorded in the same environment. As the timbral content of the speech and background noise typically differ considerably, a simple equalization matching results in a noticeable mismatch in the output signals. A single equalization filter affects both timbres equally and thus cannot disambiguate the competing matching equations of each source. We propose leveraging speech enhancement methods in order to separate speech and background noise, independently apply equalization filtering to each source, and recombine the outputs. By independently equalizing the separated sources, our method is able to better disambiguate the matching equations associated with each source. Therefore the resulting matched signals are perceptually very similar. Additionally, by retaining the background noise in the final output signals, most artifacts from speech enhancement methods are considerably reduced and in general perceptually masked. Subjective listening tests show that our approach significantly outperforms simple equalization matching.},
	booktitle = {2016 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Germain, François G. and Mysore, Gautham J. and Fujioka, Takako},
	month = mar,
	year = {2016},
	note = {ISSN: 2379-190X},
	keywords = {Degradation, Distortion, Equalization matching, Hardware, Insulation life, Noise measurement, Speech, Speech enhancement, speech enhancement, voice-overs},
	pages = {609--613},
}

@misc{gomez_deep_2018,
	title = {Deep learning for singing {Processing}: {Achievements}, challenges and impact on singers and listeners},
	shorttitle = {Deep {Learning} for {Singing} {Processing}},
	abstract = {This paper summarizes some recent advances on a set of tasks related to the processing of singing using state-of-the-art deep learning techniques. We discuss their achievements in terms of accuracy and sound quality, and the current challenges, such as availability of data and computing resources. We also discuss the impact that these advances do and will have on listeners and singers when they are integrated in commercial applications.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Gómez, Emilia and Blaauw, Merlijn and Bonada, Jordi and Chandna, Pritish and Cuesta, Helena},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03046 [cs, eess, stat]
Issue: arXiv:1807.03046},
	keywords = {97M80, Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.06576 [cs, q-bio]
Issue: arXiv:1508.06576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@misc{ganis_real-time_2021,
	title = {Real-time {Timbre} {Transfer} and {Sound} {Synthesis} using {DDSP}},
	abstract = {Neural audio synthesis is an actively researched topic, having yielded a wide range of techniques that leverages machine learning architectures. Google Magenta elaborated a novel approach called Differential Digital Signal Processing (DDSP) that incorporates deep neural networks with preconditioned digital signal processing techniques, reaching state-of-the-art results especially in timbre transfer applications. However, most of these techniques, including the DDSP, are generally not applicable in real-time constraints, making them ineligible in a musical workflow. In this paper, we present a real-time implementation of the DDSP library embedded in a virtual synthesizer as a plug-in that can be used in a Digital Audio Workstation. We focused on timbre transfer from learned representations of real instruments to arbitrary sound inputs as well as controlling these models by MIDI. Furthermore, we developed a GUI for intuitive high-level controls which can be used for post-processing and manipulating the parameters estimated by the neural network. We have conducted a user experience test with seven participants online. The results indicated that our users found the interface appealing, easy to understand, and worth exploring further. At the same time, we have identified issues in the timbre transfer quality, in some components we did not implement, and in installation and distribution of our plugin. The next iteration of our design will address these issues. Our real-time MATLAB and JUCE implementations are available at https://github.com/SMC704/juce-ddsp and https://github.com/SMC704/matlab-ddsp , respectively.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Ganis, Francesco and Knudesn, Erik Frej and Lyster, Søren V. K. and Otterbein, Robin and Südholt, David and Erkut, Cumhur},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.07220 [cs, eess]
Issue: arXiv:2103.07220},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{esling_bridging_2018,
	address = {Paris, France},
	title = {Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces},
	abstract = {Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.},
	booktitle = {Proceedings of the 19th international {Society} for music {Information} {Retrieval} {Conference}},
	author = {Esling, Philippe and Chemla, Axel and Bitton, Adrien},
	month = sep,
	year = {2018},
	pages = {175--181},
}

@book{farnell_designing_2010,
	title = {Designing sound},
	publisher = {Mit Press},
	author = {Farnell, Andy},
	year = {2010},
}

@misc{fabbro_speech_2020,
	title = {Speech {Synthesis} and {Control} {Using} {Differentiable} {DSP}},
	abstract = {Modern text-to-speech systems are able to produce natural and high-quality speech, but speech contains factors of variation (e.g. pitch, rhythm, loudness, timbre){\textbackslash} that text alone cannot contain. In this work we move towards a speech synthesis system that can produce diverse speech renditions of a text by allowing (but not requiring) explicit control over the various factors of variation. We propose a new neural vocoder that offers control of such factors of variation. This is achieved by employing differentiable digital signal processing (DDSP) (previously used only for music rather than speech), which exposes these factors of variation. The results show that the proposed approach can produce natural speech with realistic timbre, and individual factors of variation can be freely controlled.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Fabbro, Giorgio and Golkov, Vladimir and Kemp, Thomas and Cremers, Daniel},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.15084 [cs, eess]
Issue: arXiv:2010.15084},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{espic_direct_2017,
	title = {Direct modelling of magnitude and phase {Spectra} for statistical {Parametric} {Speech} {Synthesis}},
	doi = {10.21437/Interspeech.2017-1647},
	abstract = {We propose a simple new representation for the FFT spectrum tailored to statistical parametric speech synthesis. It consists of four feature streams that describe magnitude, phase and fundamental frequency using real numbers. The proposed feature extraction method does not attempt to decompose the speech structure (e.g., into source+filter or harmonics+noise). By avoiding the simplifications inherent in decomposition, we can dramatically reduce the “phasiness” and “buzziness” typical of most vocoders. The method uses simple and computationally cheap operations and can operate at a lower frame rate than the 200 frames-per-second typical in many systems. It avoids heuristics and methods requiring approximate or iterative solutions, including phase unwrapping.},
	urldate = {2023-08-14},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Espic, Felipe and Botinhao, Cassia Valentini and King, Simon},
	month = aug,
	year = {2017},
	pages = {1383--1387},
}

@article{esling_flow_2020,
	title = {Flow synthesizer: {Universal} {Audio} {Synthesizer} {Control} with normalizing {Flows}},
	volume = {10},
	shorttitle = {Flow {Synthesizer}},
	doi = {10.3390/app10010302},
	abstract = {The ubiquity of sound synthesizers has reshaped modern music production, and novel music genres are now sometimes even entirely defined by their use. However, the increasing complexity and number of parameters in modern synthesizers make them extremely hard to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Recently, we introduced a novel formulation of audio synthesizer control based on learning an organized latent audio space of the synthesizer\&rsquo;s capabilities, while constructing an invertible mapping to the space of its parameters. We showed that this formulation allows to simultaneously address automatic parameters inference, macro-control learning, and audio-based preset exploration within a single model. We showed that this formulation can be efficiently addressed by relying on Variational Auto-Encoders (VAE) and Normalizing Flows (NF). In this paper, we extend our results by evaluating our proposal on larger sets of parameters and show its superiority in both parameter inference and audio reconstruction against various baseline models. Furthermore, we introduce disentangling flows, which allow to learn the invertible mapping between two separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We show that the model disentangles the major factors of audio variations as latent dimensions, which can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer, while smoothly mapping to its parameters. Finally, we introduce an open-source implementation of our models inside a real-time Max4Live device that is readily available to evaluate creative applications of our proposal.},
	number = {1},
	urldate = {2020-07-09},
	journal = {Applied Sciences},
	author = {Esling, Philippe and Masuda, Naotake and Bardet, Adrien and Despres, Romeo and Chemla-Romeu-Santos, Axel},
	month = dec,
	year = {2020},
	keywords = {audio synthesizer, creative AI, generative models, machine learning, music information retrieval, normalizing flows, probabilistic graphical models, variational inference},
	pages = {302},
}

@article{elman_finding_1990,
	title = {Finding {Structure} in {Time}},
	volume = {14},
	issn = {03640213},
	doi = {10.1207/s15516709cog1402_1},
	number = {2},
	urldate = {2023-07-06},
	journal = {Cognitive Science},
	author = {Elman, Jeffrey L.},
	month = mar,
	year = {1990},
	pages = {179--211},
}

@article{dudley_vocoder_1939,
	title = {The vocoder},
	volume = {18},
	author = {Dudley, WH},
	year = {1939},
	pages = {122},
}

@inproceedings{engel_self-supervised_2020,
	title = {Self-supervised {Pitch} {Detection} by {Inverse} {Audio} {Synthesis}},
	abstract = {Audio scene understanding, parsing sound into a hierarchy of meaningful parts, is an open problem in representation learning. Sound is a particularly challenging domain due to its high dimensionality, sequential dependencies and hierarchical structure. Differentiable Digital Signal Processing (DDSP) greatly simplifies the forward problem of generating audio by introducing differentiable synthesizer and effects modules that combine strong signal priors with end-to-end learning. Here, we focus on the inverse problem, inferring synthesis parameters to approximate an audio scene. We demonstrate that DDSP modules can enable a new approach to self-supervision, generating synthetic audio with differentiable synthesizers and training feature extractor networks to infer the synthesis parameters. By building a hierarchy from sinusoidal to harmonic representations, we show that it possible to use such an inverse modeling approach to disentangle pitch from timbre, an important task in audio scene understanding.},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	author = {Engel, Jesse and Swavely, Rigel and Roberts, Adam},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {9},
}

@inproceedings{engel_neural_2017,
	address = {Sydney, Australia},
	title = {Neural audio synthesis of musical notes with {WaveNet} autoencoders},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2020-07-09},
	booktitle = {Proceedings of the 34th international {Conference} on machine {Learning} - volume 70},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
	month = aug,
	year = {2017},
	pages = {1068--1077},
}

@inproceedings{engel_gansynth_2018,
	title = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
	shorttitle = {{GANSynth}},
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
	urldate = {2023-08-08},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	month = sep,
	year = {2018},
}

@inproceedings{dupre_spatial_2021,
	address = {Bologna, Italy},
	title = {Spatial sound {Design} in a car {Cockpit}: {Challenges} and perspectives},
	isbn = {978-1-66540-998-8},
	shorttitle = {Spatial {Sound} {Design} in a {Car} {Cockpit}},
	doi = {10.1109/I3DA48870.2021.9610910},
	urldate = {2023-08-23},
	booktitle = {2021 immersive and {3D} {Audio}: {From} architecture to automotive ({I3DA})},
	publisher = {IEEE},
	author = {Dupre, Theophile and Denjean, Sebastien and Aramaki, Mitsuko and Kronland-Martinet, Richard},
	month = sep,
	year = {2021},
	pages = {1--5},
}

@article{dudley_speaking_1950,
	title = {The {Speaking} {Machine} of {Wolfgang} von {Kempelen}},
	volume = {22},
	issn = {0001-4966, 1520-8524},
	doi = {10.1121/1.1906583},
	abstract = {The physiological motions involved in speaking can be indicated to the eye or to the ear. For the eye suitably chosen symbols may be written to indicate the physiological positions assumed informing each sound; for the ear synthetic sounds may be produced by motions in a mechanism built to simulate the speech organs. The degree of phonetic success may be estimated in the case of the visible symbols by listening to sounds formed when the indicated physiological processes are carried out, and in the case of the speech-simulating mechanism by comparing the synthetic speech produced to normally spoken speech. Significant advances along both the visual and the aural lines are described from earliest times down to the present. Wolfgang von Kempelen produced the first speaking machine worthy of the name around 1780. This paper gives his background, a description of the apparatus he built, and a discussion of the methods used in producing the various sounds, fitting his work into the over-all picture of speech-imitating devices from the speaking of idols of ancient times down to the automatic electrical reconstructing of speech in the vocoder. For portraying to the eye the physiological characteristics of speech there are discussed the more outstanding methods from claimed symbolic alphabets of ancient languages down to the recent spectrographic visible speech.},
	number = {2},
	urldate = {2023-08-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Dudley, Homer and Tarnoczy, T. H.},
	month = mar,
	year = {1950},
	pages = {151--166},
}

@inproceedings{douwes_is_2023,
	address = {Rhodes Island, Greece},
	title = {Is quality {Enough}? {Integrating} {Energy} {Consumption} in a large-{Scale} {Evaluation} of neural {Audio} {Synthesis} {Models}},
	isbn = {978-1-72816-327-7},
	doi = {10.1109/ICASSP49357.2023.10096975},
	urldate = {2023-06-03},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Douwes, Constance and Bindi, Giovanni and Caillon, Antoine and Esling, Philippe and Briot, Jean-Pierre},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@article{defossez_sing_2018,
	title = {{SING}: {Symbol}-to-{Instrument} {Neural} {Generator}},
	shorttitle = {{SING}},
	abstract = {Recent progress in deep learning for audio synthesis opens the way to models that directly produce the waveform, shifting away from the traditional paradigm of relying on vocoders or MIDI synthesizers for speech or music generation. Despite their successes, current state-of-the-art neural audio synthesizers such as WaveNet and SampleRNN suffer from prohibitive training and inference times because they are based on autoregressive models that generate audio samples one at a time at a rate of 16kHz. In this work, we study the more computationally efficient alternative of generating the waveform frame-by-frame with large strides. We present SING, a lightweight neural audio synthesizer for the original task of generating musical notes given desired instrument, pitch and velocity. Our model is trained end-to-end to generate notes from nearly 1000 instruments with a single decoder, thanks to a new loss function that minimizes the distances between the log spectrograms of the generated and target waveforms. On the generalization task of synthesizing notes for pairs of pitch and instrument not seen during training, SING produces audio with significantly improved perceptual quality compared to a state-of-the-art autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is about 32 times faster for training and 2, 500 times faster for inference.},
	urldate = {2021-03-16},
	journal = {arXiv:1810.09785 [cs, eess, stat]},
	author = {Défossez, Alexandre and Zeghidour, Neil and Usunier, Nicolas and Bottou, Léon and Bach, Francis},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.09785 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, ⛔ No DOI found},
}

@misc{diaz_rigid-body_2022,
	title = {Rigid-{Body} {Sound} {Synthesis} with {Differentiable} {Modal} {Resonators}},
	abstract = {Physical models of rigid bodies are used for sound synthesis in applications from virtual environments to music production. Traditional methods such as modal synthesis often rely on computationally expensive numerical solvers, while recent deep learning approaches are limited by post-processing of their results. In this work we present a novel end-to-end framework for training a deep neural network to generate modal resonators for a given 2D shape and material, using a bank of differentiable IIR filters. We demonstrate our method on a dataset of synthetic objects, but train our model using an audio-domain objective, paving the way for physically-informed synthesisers to be learned directly from recordings of real-world objects.},
	urldate = {2022-11-04},
	publisher = {arXiv},
	author = {Diaz, Rodrigo and Hayes, Ben and Saitis, Charalampos and Fazekas, György and Sandler, Mark},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2210.15306},
	note = {arXiv: 2210.15306 [cs, eess]
Issue: arXiv:2210.15306
tex.copyright: All rights reserved},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{de_man_ten_2017,
	title = {Ten {Years} of {Automatic} {Mixing}},
	abstract = {Reflecting on a decade of Automatic Mixing systems for multitrack music processing, this paper positions the topic in the wider field of Intelligent Music Production, and seeks to motivate the existing and continued work in this area. Tendencies such as the introduction of machine learning and the increasing complexity of automated systems become ap- parent from examining a short history of relevant work, and several categories of applications are identified. Based on this systematic review, we highlight some promising direc- tions for future research for the next ten years of Automatic Mixing},
	number = {October},
	journal = {Proceedings of the 3rd Workshop on Intelligent Music Production},
	author = {De Man, Brecht and Reiss, Joshua D. and Stables, Ryan},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@book{de_man_intelligent_2019,
	title = {Intelligent music production},
	publisher = {Routledge},
	author = {De Man, Brecht and Stables, Ryan and Reiss, Joshua D},
	year = {2019},
}

@misc{dai_music_2018-1,
	title = {Music {Style} {Transfer}: {A} {Position} {Paper}},
	shorttitle = {Music {Style} {Transfer}},
	abstract = {Led by the success of neural style transfer on visual arts, there has been a rising trend very recently in the effort of music style transfer. However, "music style" is not yet a well-defined concept from a scientific point of view. The difficulty lies in the intrinsic multi-level and multi-modal character of music representation (which is very different from image representation). As a result, depending on their interpretation of "music style", current studies under the category of "music style transfer", are actually solving completely different problems that belong to a variety of sub-fields of Computer Music. Also, a vanilla end-to-end approach, which aims at dealing with all levels of music representation at once by directly adopting the method of image style transfer, leads to poor results. Thus, we vitally propose a more scientifically-viable definition of music style transfer by breaking it down into precise concepts of timbre style transfer, performance style transfer and composition style transfer, as well as to connect different aspects of music style transfer with existing well-established sub-fields of computer music studies. In addition, we discuss the current limitations of music style modeling and its future directions by drawing spirit from some deep generative models, especially the ones using unsupervised learning and disentanglement techniques.},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Dai, Shuqi and Zhang, Zheng and Xia, Gus G.},
	month = jul,
	year = {2018},
	note = {arXiv: 1803.06841 [cs, eess]
Issue: arXiv:1803.06841},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{cramer_look_2019,
	title = {Look, listen, and learn {More}: {Design} {Choices} for deep {Audio} {Embeddings}},
	shorttitle = {Look, {Listen}, and {Learn} {More}},
	doi = {10.1109/ICASSP.2019.8682475},
	abstract = {A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L3-Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L3-Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3-Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3-Net embedding model as well as pre-trained models are made freely available online.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Cramer, Aurora Linh and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Audio classification, Computational modeling, Data models, Spectrogram, Task analysis, Training, Training data, Videos, deep audio embeddings, deep learning, machine listening, transfer learning},
	pages = {3852--3856},
}

@inproceedings{covert_vacuum-tube_2013,
	title = {A vacuum-tube guitar amplifier model using a recurrent neural network},
	doi = {10.1109/SECON.2013.6567472},
	booktitle = {2013 proceedings of {IEEE} southeastcon},
	publisher = {IEEE},
	author = {Covert, John and Livingston, David L},
	year = {2013},
	pages = {1--5},
}

@article{cooley_algorithm_1965,
	title = {An algorithm for the machine calculation of complex {Fourier} series},
	volume = {19},
	doi = {10.1090/S0025-5718-1965-0178586-1},
	number = {90},
	journal = {Mathematics of computation},
	author = {Cooley, James W and Tukey, John W},
	year = {1965},
	pages = {297--301},
}

@article{cook_singing_1996,
	title = {Singing voice {Synthesis}: {History}, current {Work}, and future {Directions}},
	volume = {20},
	issn = {0148-9267},
	shorttitle = {Singing {Voice} {Synthesis}},
	doi = {10.2307/3680822},
	number = {3},
	urldate = {2023-07-22},
	journal = {Computer Music Journal},
	author = {Cook, Perry R.},
	year = {1996},
	note = {JSTOR: 3680822
Publisher: The MIT Press},
	pages = {38--46},
}

@misc{comunita_neural_2021,
	title = {Neural synthesis of footsteps {Sound} {Effects} with generative {Adversarial} {Networks}},
	abstract = {Footsteps are among the most ubiquitous sound effects in multimedia applications. There is substantial research into understanding the acoustic features and developing synthesis models for footstep sound effects. In this paper, we present a first attempt at adopting neural synthesis for this task. We implemented two GAN-based architectures and compared the results with real recordings as well as six traditional sound synthesis methods. Our architectures reached realism scores as high as recorded samples, showing encouraging results for the task at hand.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Comunità, Marco and Phan, Huy and Reiss, Joshua D.},
	month = dec,
	year = {2021},
	note = {arXiv: 2110.09605 [cs, eess]
Issue: arXiv:2110.09605},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{comunita_modelling_2022,
	title = {Modelling black-box audio effects with time-varying feature modulation},
	abstract = {Deep learning approaches for black-box modelling of audio effects have shown promise, however, the majority of existing work focuses on nonlinear effects with behaviour on relatively short time-scales, such as guitar amplifiers and distortion. While recurrent and convolutional architectures can theoretically be extended to capture behaviour at longer time scales, we show that simply scaling the width, depth, or dilation factor of existing architectures does not result in satisfactory performance when modelling audio effects such as fuzz and dynamic range compression. To address this, we propose the integration of time-varying feature-wise linear modulation into existing temporal convolutional backbones, an approach that enables learnable adaptation of the intermediate activations. We demonstrate that our approach more accurately captures long-range dependencies for a range of fuzz and compressor implementations across both time and frequency domain metrics. We provide sound examples, source code, and pretrained models to faciliate reproducibility.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Comunità, Marco and Steinmetz, Christian J. and Phan, Huy and Reiss, Joshua D.},
	month = nov,
	year = {2022},
	note = {arXiv: 2211.00497 [cs, eess]
Issue: arXiv:2211.00497},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{chowdhury_rtneural_2021,
	title = {{RTNeural}: {Fast} neural inferencing for real-time systems},
	journal = {arXiv preprint arXiv:2106.03037},
	author = {Chowdhury, Jatin},
	year = {2021},
	note = {arXiv: 2106.03037},
	keywords = {⛔ No DOI found},
}

@misc{choi_proposal_2022-1,
	title = {A {Proposal} for {Foley} {Sound} {Synthesis} {Challenge}},
	abstract = {"Foley" refers to sound effects that are added to multimedia during post-production to enhance its perceived acoustic properties, e.g., by simulating the sounds of footsteps, ambient environmental sounds, or visible objects on the screen. While foley is traditionally produced by foley artists, there is increasing interest in automatic or machine-assisted techniques building upon recent advances in sound synthesis and generative models. To foster more participation in this growing research area, we propose a challenge for automatic foley synthesis. Through case studies on successful previous challenges in audio and machine learning, we set the goals of the proposed challenge: rigorous, unified, and efficient evaluation of different foley synthesis systems, with an overarching goal of drawing active participation from the research community. We outline the details and design considerations of a foley sound synthesis challenge, including task definition, dataset requirements, and evaluation criteria.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Choi, Keunwoo and Oh, Sangshin and Kang, Minsung and McFee, Brian},
	month = jul,
	year = {2022},
	note = {arXiv: 2207.10760 [cs, eess]
Issue: arXiv:2207.10760},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{choi_nansy_2022,
	title = {{NANSY}++: {Unified} {Voice} {Synthesis} with neural {Analysis} and synthesis},
	shorttitle = {{NANSY}++},
	abstract = {Various applications of voice synthesis have been developed independently despite the fact that they generate "voice" as output in common. In addition, most of the voice synthesis models still require a large number of audio data paired with annotated labels (e.g., text transcription and music score) for training. To this end, we propose a unified framework of synthesizing and manipulating voice signals from analysis features, dubbed NANSY++. The backbone network of NANSY++ is trained in a self-supervised manner that does not require any annotations paired with audio. After training the backbone network, we efficiently tackle four voice applications - i.e. voice conversion, text-to-speech, singing voice synthesis, and voice designing - by partially modeling the analysis features required for each task. Extensive experiments show that the proposed framework offers competitive advantages such as controllability, data efficiency, and fast training convergence, while providing high quality synthesis. Audio samples: tinyurl.com/8tnsy3uc.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Choi, Hyeong-Seok and Yang, Jinhyeok and Lee, Juheon and Kim, Hyeongju},
	month = nov,
	year = {2022},
	note = {arXiv: 2211.09407 [cs, eess]
Issue: arXiv:2211.09407},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{choi_foley_2023,
	title = {Foley {Sound} {Synthesis} at the {DCASE} 2023 {Challenge}},
	abstract = {The addition of Foley sound effects during post-production is a common technique used to enhance the perceived acoustic properties of multimedia content. Traditionally, Foley sound has been produced by human Foley artists, which involves manual recording and mixing of sound. However, recent advances in sound synthesis and generative models have generated interest in machine-assisted or automatic Foley synthesis techniques. To promote further research in this area, we have organized a challenge in DCASE 2023: Task 7 - Foley Sound Synthesis. Our challenge aims to provide a standardized evaluation framework that is both rigorous and efficient, allowing for the evaluation of different Foley synthesis systems. We received 17 submissions, and performed both objective and subjective evaluation to rank them according to three criteria: audio quality, fit-to-category, and diversity. Through this challenge, we hope to encourage active participation from the research community and advance the state-of-the-art in automatic Foley synthesis. In this technical report, we provide a detailed overview of the Foley sound synthesis challenge, including task definition, dataset, baseline, evaluation scheme and criteria, challenge result, and discussion.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Choi, Keunwoo and Im, Jaekwon and Heller, Laurie and McFee, Brian and Imoto, Keisuke and Okamoto, Yuki and Lagrange, Mathieu and Takamichi, Shinosuke},
	month = jun,
	year = {2023},
	note = {arXiv: 2304.12521 [cs, eess]
Issue: arXiv:2304.12521},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{cho_survey_2021,
	title = {A survey on recent {Deep} {Learning}-driven {Singing} {Voice} {Synthesis} {Systems}},
	doi = {10.1109/AIVR52153.2021.00067},
	abstract = {Singing voice synthesis (SVS) is a task that aims to generate audio signals according to musical scores and lyrics. With its multifaceted nature concerning music and language, producing singing voices indistinguishable from that of human singers has always remained an unfulfilled pursuit. Nonetheless, the advancements of deep learning techniques have brought about a substantial leap in the quality and naturalness of synthesized singing voice. This paper aims to review some of the state-of-the-art deep learning-driven SVS systems. We intend to summarize their deployed model architectures and identify the strengths and limitations for each of the introduced systems. Thereby, we picture the recent advancement trajectory of this field and conclude the challenges left to be resolved both in commercial applications and academic research.},
	booktitle = {2021 {IEEE} {International} {Conference} on artificial {Intelligence} and virtual {Reality} ({AIVR})},
	author = {Cho, Yin-Ping and Yang, Fu-Rong and Chang, Yung-Chuan and Cheng, Ching-Ting and Wang, Xiao-Han and Liu, Yi-Wen},
	month = nov,
	year = {2021},
	keywords = {Benchmark testing, Conferences, Deep learning, Music, Solid modeling, Trajectory, Virtual reality, deep learning, review paper, sining voice synthesis},
	pages = {319--323},
}

@inproceedings{childers_voice_1985,
	title = {Voice conversion: {Factors} responsible for quality},
	volume = {10},
	shorttitle = {Voice conversion},
	doi = {10.1109/ICASSP.1985.1168479},
	abstract = {A flexible analysis-synthesis system with signal dependent features is described and used to realize some desired voice characteristics in synthesized speech. The intelligibility of synthetic speech appears to depend on the ability to reproduce dynamic sounds such as stops, whereas the quality of voice is mainly determined by the true reproduction of voiced segments. We describe our work in converting the speech of one speaker to sound like that of another. A number of factors are important for maintaining the quality of the voice during this conversion process. These factors are derived from both the speech and electroglottograph signals.},
	booktitle = {{ICASSP} '85. {IEEE} {International} {Conference} on acoustics, speech, and signal {Processing}},
	author = {Childers, D. and Yegnanarayana, B. and Wu, Ke},
	month = apr,
	year = {1985},
	keywords = {Feature extraction, Information analysis, Linear predictive coding, Loudspeakers, Performance analysis, Production systems, Pulse shaping methods, Shape, Signal analysis, Speech analysis},
	pages = {748--751},
}

@article{colonel_reverse_2021,
	title = {Reverse engineering of a recording mix with differentiable digital signal processing},
	volume = {150},
	issn = {0001-4966},
	doi = {10.1121/10.0005622},
	number = {1},
	urldate = {2023-06-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Colonel, Joseph T. and Reiss, Joshua},
	month = jul,
	year = {2021},
	pages = {608--619},
}

@inproceedings{colonel_direct_2022,
	title = {Direct design of biquad {Filter} {Cascades} with deep {Learning} by sampling {Random} {Polynomials}},
	doi = {10.1109/ICASSP43922.2022.9747660},
	abstract = {Designing infinite impulse response filters to match an arbitrary magnitude response requires specialized techniques. Methods like modified Yule-Walker are relatively efficient, but may not be sufficiently accurate in matching high order responses. On the other hand, iterative optimization techniques often enable superior performance, but come at the cost of longer run-times and are sensitive to initial conditions, requiring manual tuning. In this work, we address some of these limitations by learning a direct mapping from the target magnitude response to the filter coefficient space with a neural network trained on millions of random filters. We demonstrate our approach enables both fast and accurate estimation of filter coefficients given a desired response. We investigate training with different families of random filters, and find training with a variety of filter families enables better generalization when estimating real-world filters, using head-related transfer functions and guitar cabinets as case studies. We compare our method against existing methods including modified Yule-Walker and gradient descent and show our approach is, on average, both faster and more accurate.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Colonel, Joseph T. and Steinmetz, Christian J. and Michelen, Marcus and Reiss, Joshua D.},
	month = may,
	year = {2022},
	keywords = {Deep learning, Estimation, IIR filters, Matched filters, Neural networks, Stochastic processes, Training, Transfer functions, random polynomials},
	pages = {3104--3108},
}

@article{chowning_synthesis_1973,
	title = {The synthesis of complex audio spectra by means of frequency modulation},
	volume = {21},
	number = {7},
	journal = {J. Audio Eng. Soc},
	author = {Chowning, John M.},
	year = {1973},
	keywords = {⛔ No DOI found},
	pages = {526--534},
}

@inproceedings{chen_automatic_2022,
	title = {Automatic {DJ} {Transitions} with differentiable {Audio} {Effects} and generative {Adversarial} {Networks}},
	doi = {10.1109/ICASSP43922.2022.9746663},
	abstract = {A central task of a Disc Jockey (DJ) is to create a mixset of music with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. The generator uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such a way that the resulting mix resembles real mixes created by human DJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Chen, Bo-Yu and Hsu, Wei-Han and Liao, Wei-Hsiang and Ramírez, Marco A. Martínez and Mitsufuji, Yuki and Yang, Yi-Hsuan},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Conferences, DJ mix, Digital signal processing, Equalizers, Generative adversarial networks, Generators, Pipelines, audio effects, deep learning, differentiable signal processing, generative adversarial network},
	pages = {466--470},
}

@inproceedings{castellon_towards_2020,
	address = {Vancouver, Canada},
	title = {Towards realistic {MIDI} instrument synthesizers},
	abstract = {Despite decades of research and widespread commercial interest, synthesis of realistic musical instrument audio from real-time MIDI input remains elusive. In this work, we propose a learning-based system for creating bespoke, MIDI-driven instrument synthesizers. Our system takes as training data audio recordings of a particular instrument and, optionally, scores corresponding to those recordings. To prepare the data, we use an off-the-shelf transcription (or score alignment) strategy to create time-aligned MIDI and audio. Then, we train recurrent neural networks to learn a distribution over instrument audio given MIDI. We demonstrate our method on the task of violin synthesis. Our user study indicates that our proposed system can learn a real-time-capable, MIDI-controllable violin synthesizer which produces fairly realistic audio given only a few minutes of amateur recordings.},
	booktitle = {4th {Workshop} on {Machine} {Learning} for {Creativity} and {Design}},
	author = {Castellon, Rodrigo and Donahue, Chris and Liang, Percy},
	year = {2020},
	keywords = {⛔ No DOI found},
}

@inproceedings{chen_sound2synth_2022,
	title = {{Sound2Synth}: {Interpreting} sound via {FM} synthesizer parameters estimation},
	doi = {10.24963/ijcai.2022/682},
	booktitle = {Proceedings of the thirty-first international joint conference on artificial intelligence, {IJCAI}-22},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Chen, Zui and Jing, Yansen and Yuan, Shengcheng and Xu, Yifei and Wu, Jian and Zhao, Hang},
	editor = {Raedt, Lud De},
	month = jul,
	year = {2022},
	pages = {4921--4928},
}

@misc{chen_hifisinger_2020,
	title = {{HiFiSinger}: {Towards} {High}-{Fidelity} {Neural} {Singing} {Voice} {Synthesis}},
	shorttitle = {{HiFiSinger}},
	abstract = {High-fidelity singing voices usually require higher sampling rate (e.g., 48kHz) to convey expression and emotion. However, higher sampling rate causes the wider frequency band and longer waveform sequences and throws challenges for singing voice synthesis (SVS) in both frequency and time domains. Conventional SVS systems that adopt small sampling rate cannot well address the above challenges. In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voice. HiFiSinger consists of a FastSpeech based acoustic model and a Parallel WaveGAN based vocoder to ensure fast training and inference and also high voice quality. To tackle the difficulty of singing modeling caused by high sampling rate (wider frequency band and longer waveform), we introduce multi-scale adversarial training in both the acoustic model and vocoder to improve singing modeling. Specifically, 1) To handle the larger range of frequencies caused by higher sampling rate, we propose a novel sub-frequency GAN (SF-GAN) on mel-spectrogram generation, which splits the full 80-dimensional mel-frequency into multiple sub-bands and models each sub-band with a separate discriminator. 2) To model longer waveform sequences caused by higher sampling rate, we propose a multi-length GAN (ML-GAN) for waveform generation to model different lengths of waveform sequences with separate discriminators. 3) We also introduce several additional designs and findings in HiFiSinger that are crucial for high-fidelity voices, such as adding F0 (pitch) and V/UV (voiced/unvoiced flag) as acoustic features, choosing an appropriate window/hop size for mel-spectrogram, and increasing the receptive field in vocoder for long vowel modeling. Experiment results show that HiFiSinger synthesizes high-fidelity singing voices with much higher quality: 0.32/0.44 MOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Chen, Jiawei and Tan, Xu and Luan, Jian and Qin, Tao and Liu, Tie-Yan},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.01776 [cs, eess]
Issue: arXiv:2009.01776},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{chan_spectral_1982,
	title = {Spectral estimation via the high-order {Yule}-{Walker} equations},
	volume = {30},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1982.1163946},
	abstract = {A new spectral estimator is developed from the high-order Yule-Walker equations and pseudoinverse solutions to the equations. The approach circumvents the usual difficulties associated with inverting ill-conditioned matrices and allows the choice of high model order. It also provides high resolution spectral estimates as substantiated by simulation results. A byproduct of this spectral estimator is a constant false alarm rate detector for a single sinusoid in noise. The detector's receiver operating characteristics curves are constructed from histograms, based on 1000 runs, of the detection statistics.},
	number = {5},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Chan, Y. and Langford, R.},
	month = oct,
	year = {1982},
	keywords = {Covariance matrix, Detectors, Energy resolution, Equations, Military computing, Radar detection, Radar signal processing, Signal processing algorithms, Signal resolution, White noise},
	pages = {689--698},
}

@misc{caspe_ddx7_2022,
	title = {{DDX7}: {Differentiable} {FM} {Synthesis} of {Musical} {Instrument} {Sounds}},
	shorttitle = {{DDX7}},
	abstract = {FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.},
	urldate = {2022-09-18},
	publisher = {arXiv},
	author = {Caspe, Franco and McPherson, Andrew and Sandler, Mark},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2208.06169},
	note = {arXiv: 2208.06169 [cs, eess]
Issue: arXiv:2208.06169},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, H.5.5, I.2.6},
}

@inproceedings{casebeer_auto-dsp_2021,
	title = {Auto-{DSP}: {Learning} to {Optimize} {Acoustic} {Echo} {Cancellers}},
	shorttitle = {Auto-{DSP}},
	doi = {10.1109/WASPAA52581.2021.9632678},
	abstract = {Adaptive filtering algorithms are commonplace in signal processing and have wide-ranging applications from single-channel denoising to multi -channel acoustic echo cancellation and adaptive beamforming. Such algorithms typically operate via specialized online, iterative optimization methods and have achieved tremendous success, but require expert knowledge, are slow to develop, and are difficult to customize. In our work, we present a new method to automatically learn adaptive filtering update rules directly from data. To do so, we frame adaptive filtering as a differentiable operator and train a learned optimizer to output a gradient descent-based update rule from data via backpropagation through time. We demonstrate our general approach on an acoustic echo cancellation task (single-talk with noise) and show that we can learn high-performing adaptive filters for a variety of common linear and non-linear mul-tidelayed block frequency domain filter architectures. We also find that our learned update rules exhibit fast convergence, can optimize in the presence of nonlinearities, and are robust to acoustic scene changes despite never encountering any during training.},
	booktitle = {2021 {IEEE} {Workshop} on applications of signal {Processing} to audio and acoustics ({WASPAA})},
	author = {Casebeer, Jonah and Bryan, Nicholas J. and Smaragdis, Paris},
	month = oct,
	year = {2021},
	note = {ISSN: 1947-1629},
	keywords = {Adaptive filters, Echo cancellers, Filtering, Frequency-domain analysis, Optimization methods, Signal processing algorithms, Training, acoustic echo cancellation, adaptive filtering, adaptive optimization, learning to learn, meta-learning},
	pages = {291--295},
}

@inproceedings{casanova_yourtts_2022,
	title = {{YourTTS}: {Towards} {Zero}-{Shot} {Multi}-{Speaker} {TTS} and zero-{Shot} {Voice} {Conversion} for everyone},
	shorttitle = {{YourTTS}},
	abstract = {YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 39th international {Conference} on machine {Learning}},
	publisher = {PMLR},
	author = {Casanova, Edresson and Weber, Julian and Shulby, Christopher D. and Junior, Arnaldo Candido and Gölge, Eren and Ponti, Moacir A.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {2709--2720},
}

@inproceedings{cartwright_synthassist_2014,
	title = {{SynthAssist}: {Querying} an audio synthesizer by vocal imitation},
	booktitle = {New interfaces for musical expression},
	author = {Cartwright, M. and Pardo, Bryan},
	year = {2014},
	keywords = {⛔ No DOI found},
}

@misc{carson_differentiable_2023,
	title = {Differentiable grey-box {Modelling} of phaser {Effects} using frame-based {Spectral} {Processing}},
	abstract = {Machine learning approaches to modelling analog audio effects have seen intensive investigation in recent years, particularly in the context of non-linear time-invariant effects such as guitar amplifiers. For modulation effects such as phasers, however, new challenges emerge due to the presence of the low-frequency oscillator which controls the slowly time-varying nature of the effect. Existing approaches have either required foreknowledge of this control signal, or have been non-causal in implementation. This work presents a differentiable digital signal processing approach to modelling phaser effects in which the underlying control signal and time-varying spectral response of the effect are jointly learned. The proposed model processes audio in short frames to implement a time-varying filter in the frequency domain, with a transfer function based on typical analog phaser circuit topology. We show that the model can be trained to emulate an analog reference device, while retaining interpretable and adjustable parameters. The frame duration is an important hyper-parameter of the proposed model, so an investigation was carried out into its effect on model accuracy. The optimal frame length depends on both the rate and transient decay-time of the target effect, but the frame length can be altered at inference time without a significant change in accuracy.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Carson, Alistair and Valentini-Botinhao, Cassia and King, Simon and Bilbao, Stefan},
	month = jun,
	year = {2023},
	doi = {10.48550/arXiv.2306.01332},
	note = {arXiv: 2306.01332 [cs, eess]
Issue: arXiv:2306.01332},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{carney_tone_2021,
	title = {Tone {Transfer}: {In}-{Browser} {Interactive} {Neural} {Audio} {Synthesis}},
	shorttitle = {Tone {Transfer}},
	urldate = {2023-06-03},
	booktitle = {{IUI}},
	author = {Carney, Michelle and Li, Chong and Toh, Edwin and Yu, Ping and Engel, Jesse},
	year = {2021},
	keywords = {⛔ No DOI found},
}

@inproceedings{campolucci_intrinsically_1998,
	title = {Intrinsically stable {IIR} filters and {IIR}-{MLP} neural networks for signal processing},
	volume = {2},
	doi = {10.1109/ICASSP.1998.675473},
	abstract = {This paper presents a new technique to control stability of IIR adaptive filters based on the idea of intrinsically stable operations that makes it possible to continually adapt the coefficients with no need of a stability test or pole projection. The coefficients are adapted in a way that intrinsically assures the poles to be in the unit circle. This makes it possible to use a higher step size (also named learning rate here) potentially improving the fastness of adaptation with respect to methods that employ a bound on the learning rate or methods that simply do not control stability. This method can be applied to various realizations: direct forms, cascade or parallel of second order sections, lattice forms. It can be implemented to adapt a simple IIR adaptive filter or a locally recurrent neural network such as the IIR-MLP.},
	booktitle = {Proceedings of the 1998 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing}, {ICASSP} '98 ({Cat}. {No}.{98CH36181})},
	author = {Campolucci, P. and Piazza, F.},
	month = may,
	year = {1998},
	note = {ISSN: 1520-6149},
	keywords = {Adaptive filters, Adaptive signal processing, Electronic mail, Finite impulse response filter, IIR filters, Neural networks, Recurrent neural networks, Signal processing, Signal processing algorithms, Stability},
	pages = {1149--1152 vol.2},
}

@inproceedings{campolucci_fast_1996,
	address = {Atlanta, GA, USA},
	title = {Fast adaptive {IIR}-{MLP} neural networks for signal processing applications},
	volume = {6},
	isbn = {978-0-7803-3192-1},
	doi = {10.1109/ICASSP.1996.550790},
	urldate = {2023-07-05},
	booktitle = {1996 {IEEE} {International} {Conference} on acoustics, speech, and signal {Processing} {Conference} {Proceedings}},
	publisher = {IEEE},
	author = {Campolucci, P. and Uncini, A. and Piazza, F.},
	year = {1996},
	pages = {3529--3532},
}

@inproceedings{campolucci_-line_1995,
	address = {Perth, WA, Australia},
	title = {On-line learning algorithms for neural networks with {IIR} synapses},
	volume = {2},
	isbn = {978-0-7803-2768-9},
	doi = {10.1109/ICNN.1995.487532},
	urldate = {2023-07-19},
	booktitle = {Proceedings of {ICNN}'95 - international {Conference} on neural {Networks}},
	publisher = {IEEE},
	author = {Campolucci, P. and Piazza, F. and Uncini, A.},
	year = {1995},
	pages = {865--870},
}

@inproceedings{cakir_musical_2018,
	title = {Musical instrument {Synthesis} and morphing in multidimensional {Latent} {Space} {Using} {Variational}, convolutional {Recurrent} {Autoencoders}},
	booktitle = {Proceedings of the {Audio} {Engineerings} {Society} 145th {Convention}},
	author = {Çakır, Emre and Virtanen, Tuomas},
	month = oct,
	year = {2018},
}

@misc{braun_dawdreamer_2021,
	title = {{DawDreamer}: {Bridging} the gap {Between} {Digital} {Audio} {Workstations} and python {Interfaces}},
	shorttitle = {{DawDreamer}},
	abstract = {Audio production techniques which previously only existed in GUI-constrained digital audio workstations, livecoding environments, or C++ APIs are now accessible with our new Python module called DawDreamer. DawDreamer therefore bridges the gap between real sound engineers and coders imitating them with offline batch-processing. Like contemporary modules in this domain, DawDreamer can create directed acyclic graphs of audio processors such as VSTs which generate or manipulate audio streams. DawDreamer can also dynamically compile and execute code from Faust, a powerful signal processing language which can be deployed to many platforms and microcontrollers. We discuss DawDreamer's unique features in detail and potential applications across music information retrieval including source separation, transcription, and audio effect parameter inference. We provide fully cross-platform PyPI installers, a Linux Dockerfile, and an example Jupyter notebook.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Braun, David},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2111.09931},
	note = {arXiv: 2111.09931 [cs, eess]
Issue: arXiv:2111.09931},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: {Composable} transformations of {Python}+{NumPy} programs},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
	year = {2018},
}

@article{bond-taylor_deep_2021,
	title = {Deep generative {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, normalizing {Flows}, energy-{Based} and autoregressive {Models}},
	shorttitle = {Deep {Generative} {Modelling}},
	abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
	urldate = {2021-03-24},
	journal = {arXiv:2103.04922 [cs, stat]},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.04922 [cs, stat]},
	keywords = {68T01 (Primary), 68T07 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, G.3, I.4.0, I.5.0, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{caillon_rave_2021,
	title = {{RAVE}: {A} variational autoencoder for fast and high-quality neural audio synthesis},
	shorttitle = {{RAVE}},
	abstract = {Deep generative models applied to audio have improved by a large margin the state-of-the-art in many speech and music related tasks. However, as raw waveform modelling remains an inherently difficult task, audio generative models are either computationally intensive, rely on low sampling rates, are complicated to control or restrict the nature of possible signals. Among those models, Variational AutoEncoders (VAE) give control over the generation by exposing latent variables, although they usually suffer from low synthesis quality. In this paper, we introduce a Realtime Audio Variational autoEncoder (RAVE) allowing both fast and high-quality audio waveform synthesis. We introduce a novel two-stage training procedure, namely representation learning and adversarial fine-tuning. We show that using a post-training analysis of the latent space allows a direct control between the reconstruction fidelity and the representation compactness. By leveraging a multi-band decomposition of the raw waveform, we show that our model is the first able to generate 48kHz audio signals, while simultaneously running 20 times faster than real-time on a standard laptop CPU. We evaluate synthesis quality using both quantitative and qualitative subjective experiments and show the superiority of our approach compared to existing models. Finally, we present applications of our model for timbre transfer and signal compression. All of our source code and audio examples are publicly available.},
	urldate = {2022-03-08},
	journal = {arXiv:2111.05011 [cs, eess]},
	author = {Caillon, Antoine and Esling, Philippe},
	month = dec,
	year = {2021},
	note = {arXiv: 2111.05011 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ⛔ No DOI found},
}

@inproceedings{bristow-johnson_wavetable_1996,
	title = {Wavetable synthesis 101, {A} fundamental perspective},
	booktitle = {Audio engineering society convention 101},
	author = {Bristow-Johnson, Robert},
	month = nov,
	year = {1996},
}

@article{bonada_synthesis_2007,
	title = {Synthesis of the singing {Voice} by performance {Sampling} and spectral {Models}},
	volume = {24},
	issn = {1558-0792},
	doi = {10.1109/MSP.2007.323266},
	abstract = {This paper introduces the concept of synthesis based on performance sampling. It explains that although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument. The paper presents a singing voice synthesizer, pointing out the main issues and complexities emerging along its design. Although the current system is able to generate convincing results in certain situations, there is still much room for improvements, especially in the areas of expression, spectral modeling and sonic space design. However, computer singing is definitely coming close to becoming indistinguishable from human performances},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Bonada, Jordi and Serra, Xavier},
	month = mar,
	year = {2007},
	keywords = {Databases, Engines, Instruments, Keyboards, Sampling methods, Signal sampling, Signal synthesis, Synthesizers},
	pages = {67--79},
}

@inproceedings{blaauw_sequence--sequence_2020,
	title = {Sequence-to-sequence {Singing} {Synthesis} {Using} the feed-{Forward} {Transformer}},
	doi = {10.1109/ICASSP40776.2020.9053944},
	abstract = {We propose a sequence-to-sequence singing synthesizer, which avoids the need for training data with pre-aligned phonetic and acoustic features. Rather than the more common approach of a content-based attention mechanism combined with an autoregressive decoder, we use a different mechanism suitable for feed-forward synthesis. Given that phonetic timings in singing are highly constrained by the musical score, we derive an approximate initial alignment with the help of a simple duration model. Then, using a decoder based on a feed-forward variant of the Transformer model, a series of self-attention and convolutional layers refines the result of the initial alignment to reach the target acoustic features. Advantages of this approach include faster inference and avoiding the exposure bias issues that affect autoregressive models trained by teacher forcing. We evaluate the effectiveness of this model compared to an autoregressive baseline, the importance of self-attention, and the importance of the accuracy of the duration model.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on acoustics, speech and signal {Processing} ({ICASSP})},
	author = {Blaauw, Merlijn and Bonada, Jordi},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Decoding, Phonetics, Singing synthesis, Synthesizers, Timbre, Timing, Training, Training data, feed-forward, self-attention, sequence-to-sequence, transformer},
	pages = {7229--7233},
}

@inproceedings{bitton_neural_2020,
	address = {Santiago, Chile},
	title = {Neural {Granular} {Sound} {Synthesis}},
	abstract = {Granular sound synthesis is a popular audio generation technique based on rearranging sequences of small waveform windows. In order to control the synthesis, all grains in a given corpus are analyzed through a set of acoustic descriptors. This provides a representation reflecting some form of local similarities across the grains. However, the quality of this grain space is bound by that of the descriptors. Its traversal is not continuously invertible to signal and does not render any structured temporality. We demonstrate that generative neural networks can implement granular synthesis while alleviating most of its shortcomings. We efficiently replace its audio descriptor basis by a probabilistic latent space learned with a Variational Auto-Encoder. A major advantage of our proposal is that the resulting grain space is invertible, meaning that we can continuously synthesize sound when traversing its dimensions. It also implies that original grains are not stored for synthesis. To learn structured paths inside this latent space, we add a higher-level temporal embedding trained on arranged grain sequences. The model can be applied to many types of libraries, including pitched notes or unpitched drums and environmental noises. We experiment with the common granular synthesis processes and enable new ones.},
	urldate = {2020-09-04},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Bitton, Adrien and Esling, Philippe and Harada, Tatsuya},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01393},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, read},
}

@misc{bitton_modulated_2018,
	title = {Modulated variational auto-encoders for many-to-many musical timbre transfer},
	abstract = {Generative models have been successfully applied to image style transfer and domain translation. However, there is still a wide gap in the quality of results when learning such tasks on musical audio. Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models. In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. We define timbre transfer as applying parts of the auditory properties of a musical instrument onto another. First, we show that we can achieve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM). Then, we alleviate the need for additional adversarial networks by replacing the usual translation criterion by a Maximum Mean Discrepancy (MMD) objective. This allows a faster and more stable training along with a controllable latent space encoder. By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers. Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis from a reduced set of control parameters. We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains. We show that this architecture allows for generative controls in multi-domain transfer, yet remaining light, fast to train and effective on small datasets.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Bitton, Adrien and Esling, Philippe and Chemla-Romeu-Santos, Axel},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00222 [cs, eess]
Issue: arXiv:1810.00222},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{birkholz_modeling_2013,
	title = {Modeling consonant-{Vowel} {Coarticulation} for articulatory {Speech} {Synthesis}},
	volume = {8},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0060603},
	abstract = {A central challenge for articulatory speech synthesis is the simulation of realistic articulatory movements, which is critical for the generation of highly natural and intelligible speech. This includes modeling coarticulation, i.e., the context-dependent variation of the articulatory and acoustic realization of phonemes, especially of consonants. Here we propose a method to simulate the context-sensitive articulation of consonants in consonant-vowel syllables. To achieve this, the vocal tract target shape of a consonant in the context of a given vowel is derived as the weighted average of three measured and acoustically-optimized reference vocal tract shapes for that consonant in the context of the corner vowels /a/, /i/, and /u/. The weights are determined by mapping the target shape of the given context vowel into the vowel subspace spanned by the corner vowels. The model was applied for the synthesis of consonant-vowel syllables with the consonants /b/, /d/, /g/, /l/, /r/, /m/, /n/ in all combinations with the eight long German vowels. In a perception test, the mean recognition rate for the consonants in the isolated syllables was 82.4\%. This demonstrates the potential of the approach for highly intelligible articulatory speech synthesis.},
	number = {4},
	urldate = {2023-08-16},
	journal = {PLOS ONE},
	author = {Birkholz, Peter},
	month = apr,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Acoustics, Consonants, Magnetic resonance imaging, Phonemes, Speech, Syllables, Tongue, Vowels},
	pages = {e60603},
}

@book{bilbao_numerical_2009,
	title = {Numerical sound {Synthesis}: {Finite} {Difference} {Schemes} and simulation in musical {Acoustics}},
	isbn = {978-0-470-74902-9},
	shorttitle = {Numerical {Sound} {Synthesis}},
	abstract = {Digital sound synthesis has long been approached using standard digital filtering techniques. Newer synthesis strategies, however, make use of physical descriptions of musical instruments, and allow for much more realistic and complex sound production and thereby synthesis becomes a problem of simulation. This book has a special focus on time domain finite difference methods presented within an audio framework. It covers time series and difference operators, and basic tools for the construction and analysis of finite difference schemes, including frequency-domain and energy-based methods, with special attention paid to problems inherent to sound synthesis. Various basic lumped systems and excitation mechanisms are covered, followed by a look at the 1D wave equation, linear bar and string vibration, acoustic tube modelling, and linear membrane and plate vibration. Various advanced topics, such as the nonlinear vibration of strings and plates, are given an elaborate treatment. Key features: Includes a historical overview of digital sound synthesis techniques, highlighting the links between the various physical modelling methodologies. A pedagogical presentation containing over 150 problems and programming exercises, and numerous figures and diagrams, and code fragments in the MATLAB® programming language helps the reader with limited experience of numerical methods reach an understanding of this subject. Offers a complete treatment of all of the major families of musical instruments, including certain audio effects. Numerical Sound Synthesis is suitable for audio and software engineers, and researchers in digital audio, sound synthesis and more general musical acoustics. Graduate students in electrical engineering, mechanical engineering or computer science, working on the more technical side of digital audio and sound synthesis, will also find this book of interest.},
	publisher = {John Wiley \& Sons},
	author = {Bilbao, Stefan},
	month = sep,
	year = {2009},
	note = {tex.googlebooks: 3q5nGRd4UysC},
	keywords = {Computers / Programming / Algorithms, Mathematics / General, Technology \& Engineering / Electrical},
}

@misc{blaauw_neural_2017-1,
	title = {A {Neural} {Parametric} {Singing} {Synthesizer}},
	abstract = {We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Blaauw, Merlijn and Bonada, Jordi},
	month = aug,
	year = {2017},
	note = {arXiv: 1704.03809 [cs]
Issue: arXiv:1704.03809},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@inproceedings{bhattacharya_optimization_2020,
	title = {Optimization of cascaded parametric peak and shelving filters with backpropagation algorithm},
	booktitle = {Proc. {Digit}. {Audio} effects},
	author = {Bhattacharya, Purbaditya and Nowak, Patrick and Zölzer, Udo},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {101--108},
}

@article{benbarka_seeing_2021,
	title = {Seeing {Implicit} {Neural} {Representations} as {Fourier} {Series}},
	abstract = {Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.},
	urldate = {2022-03-14},
	journal = {arXiv:2109.00249 [cs]},
	author = {Benbarka, Nuri and Höfer, Timon and ul-moqeet Riaz, Hamd and Zell, Andreas},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.00249 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
}

@article{barkan_inversynth_2019,
	title = {{InverSynth}: {Deep} {Estimation} of synthesizer {Parameter} {Configurations} from audio {Signals}},
	volume = {27},
	shorttitle = {{InverSynth}},
	doi = {10.1109/TASLP.2019.2944568},
	abstract = {Sound synthesis is a complex field that requires domain expertise. Manual tuning of synthesizer parameters to match a specific sound can be an exhaustive task, even for experienced sound engineers. In this paper, we introduce InverSynth - an automatic method for synthesizer parameters tuning to match a given input sound. InverSynth is based on strided convolutional neural networks and is capable of inferring the synthesizer parameters configuration from the input spectrogram and even from the raw audio. The effectiveness InverSynth is demonstrated on a subtractive synthesizer with four frequency modulated oscillators, envelope generator and a gater effect. We present extensive quantitative and qualitative results that showcase the superiority InverSynth over several baselines. Furthermore, we show that the network depth is an important factor that contributes to the prediction accuracy.},
	number = {12},
	urldate = {2020-01-24},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Barkan, Oren and Tsiris, David and Katz, Ori and Koenigstein, Noam},
	month = dec,
	year = {2019},
	note = {arXiv: 1812.06349},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, unread},
	pages = {2385--2396},
}

@misc{barahona-rios_noisebandnet_2023,
	title = {{NoiseBandNet}: {Controllable} {Time}-{Varying} {Neural} {Synthesis} of sound {Effects} {Using} {Filterbanks}},
	shorttitle = {{NoiseBandNet}},
	abstract = {Controllable neural audio synthesis of sound effects is a challenging task due to the potential scarcity and spectro-temporal variance of the data. Differentiable digital signal processing (DDSP) synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. Here we propose NoiseBandNet, an architecture capable of synthesising and controlling sound effects by filtering white noise through a filterbank, thus going further than previous systems that make assumptions about the harmonic nature of sounds. We evaluate our approach via a series of experiments, modelling footsteps, thunderstorm, pottery, knocking, and metal sound effects. Comparing NoiseBandNet audio reconstruction capabilities to four variants of the DDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten evaluation categories, establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution. Finally, we introduce some potential creative uses of NoiseBandNet, by generating variations, performing loudness transfer, and by training it on user-defined control curves.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Barahona-Ríos, Adrián and Collins, Tom},
	month = jul,
	year = {2023},
	doi = {10.48550/arXiv.2307.08007},
	note = {arXiv: 2307.08007 [cs, eess]
Issue: arXiv:2307.08007},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{bakhturina_hi-fi_2021,
	title = {Hi-{Fi} {Multi}-{Speaker} {English} {TTS} {Dataset}},
	abstract = {This paper introduces a new multi-speaker English dataset for training text-to-speech models. The dataset is based on LibriVox audiobooks and Project Gutenberg texts, both in the public domain. The new dataset contains about 292 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz. To select speech samples with high quality, we considered audio recordings with a signal bandwidth of at least 13 kHz and a signal-to-noise ratio (SNR) of at least 32 dB. The dataset is publicly released at http://www.openslr.org/109/ .},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang, Yang},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.01497 [eess]
Issue: arXiv:2104.01497},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{back_simplified_1993,
	title = {A simplified {Gradient} {Algorithm} for {IIR} {Synapse} {Multilayer} {Perceptrons}},
	volume = {5},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1993.5.3.456},
	abstract = {A network architecture with a global feedforward local recurrent construction was presented recently as a new means of modeling nonlinear dynamic time series (Back and Tsoi 1991a). The training rule used was based on minimizing the least mean square (LMS) error and performed well, although the amount of memory required for large networks may become significant if a large number of feedback connections are used. In this note, a modified training algorithm based on a technique for linear filters is presented, simplifying the gradient calculations significantly. The memory requirements are reduced from O[n a (n a + n b )N s ] to O[(2n a + n b )N s ], where n a is the number of feedback delays, and N s is the total number of synapses. The new algorithm reduces the number of multiply-adds needed to train each synapse by n a at each time step. Simulations indicate that the algorithm has almost identical performance to the previous one.},
	number = {3},
	urldate = {2023-07-07},
	journal = {Neural Computation},
	author = {Back, Andrew D. and Tsoi, Ah Chung},
	month = may,
	year = {1993},
	pages = {456--462},
}

@inproceedings{axelrod_sample_2020,
	series = {Proceedings of machine learning research},
	title = {Sample amplification: {Increasing} dataset size even when learning is impossible},
	volume = {119},
	abstract = {Given data drawn from an unknown distribution, D, to what extent is it possible to “amplify” this dataset and faithfully output an even larger set of samples that appear to have been drawn from D? We formalize this question as follows: an (n,m) amplification procedure takes as input n independent draws from an unknown distribution D, and outputs a set of m \&gt; n “samples” which must be indistinguishable from m samples drawn iid from D. We consider this sample amplification problem in two fundamental settings: the case where D is an arbitrary discrete distribution supported on k elements, and the case where D is a d-dimensional Gaussian with unknown mean, and fixed covariance matrix. Perhaps surprisingly, we show a valid amplification procedure exists for both of these settings, even in the regime where the size of the input dataset, n, is significantly less than what would be necessary to learn distribution D to non-trivial accuracy. We also show that our procedures are optimal up to constant factors. Beyond these results, we describe potential applications of such data amplification, and formalize a number of curious directions for future research along this vein.},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Axelrod, Brian and Garg, Shivam and Sharan, Vatsal and Valiant, Gregory},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {442--451},
}

@article{arik_fast_2019,
	title = {Fast spectrogram {Inversion} {Using} {Multi}-{Head} {Convolutional} {Neural} {Networks}},
	volume = {26},
	issn = {1558-2361},
	doi = {10.1109/LSP.2018.2880284},
	abstract = {We propose the multi-head convolutional neural network (MCNN) for waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is employed with transposed convolution layers in parallel heads. MCNN enables significantly better utilization of modern multi-core processors than commonly used iterative algorithms like Griffin-Lim, and yields very fast (more than 300 × real time) runtime. For training of MCNN, we use a large-scale speech recognition dataset and losses defined on waveforms that are related to perceptual audio quality. We demonstrate that MCNN constitutes a very promising approach for high-quality speech synthesis, without any iterative algorithms or autoregression in computations.},
	number = {1},
	journal = {IEEE Signal Processing Letters},
	author = {Arık, Sercan Ö. and Jun, Heewoo and Diamos, Gregory},
	month = jan,
	year = {2019},
	keywords = {Adaptation models, Convolution, Convolutional neural networks, Phase reconstruction, Signal processing algorithms, Spectrogram, Time-frequency analysis, Training, convolutional neural networks, deep learning, short-time Fourier transform, spectrogram, speech synthesis, time-frequency signal processing},
	pages = {94--98},
}

@misc{ansi_psychoacoustic_1960,
	title = {Psychoacoustic {Terminology}: {Timbre}},
	publisher = {American National Standards Institute},
	author = {{ANSI}},
	year = {1960},
	note = {Place: New York, NY},
}

@misc{alonso_latent_2021,
	title = {Latent space {Explorations} of singing {Voice} {Synthesis} using {DDSP}},
	abstract = {Machine learning based singing voice models require large datasets and lengthy training times. In this work we present a lightweight architecture, based on the Differentiable Digital Signal Processing (DDSP) library, that is able to output song-like utterances conditioned only on pitch and amplitude, after twelve hours of training using small datasets of unprocessed audio. The results are promising, as both the melody and the singer's voice are recognizable. In addition, we present two zero-configuration tools to train new models and experiment with them. Currently we are exploring the latent space representation, which is included in the DDSP library, but not in the original DDSP examples. Our results indicate that the latent space improves both the identification of the singer as well as the comprehension of the lyrics. Our code is available at https://github.com/juanalonso/DDSP-singing-experiments with links to the zero-configuration notebooks, and our sound examples are at https://juanalonso.github.io/DDSP-singing-experiments/ .},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Alonso, Juan and Erkut, Cumhur},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.07197 [cs, eess]
Issue: arXiv:2103.07197},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{bai_empirical_2018,
	title = {An empirical {Evaluation} of generic {Convolutional} and recurrent {Networks} for sequence {Modeling}},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.01271 [cs]
Issue: arXiv:1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{back_fir_1991,
	title = {{FIR} and {IIR} {Synapses}, a new {Neural} {Network} {Architecture} for time {Series} {Modeling}},
	volume = {3},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1991.3.3.375},
	abstract = {A new neural network architecture involving either local feedforward global feedforward, and/or local recurrent global feedforward structure is proposed. A learning rule minimizing a mean square error criterion is derived. The performance of this algorithm (local recurrent global feedforward architecture) is compared with a local-feedforward global-feedforward architecture. It is shown that the local-recurrent global-feedforward model performs better than the local-feedforward global-feedforward model.},
	number = {3},
	urldate = {2023-07-05},
	journal = {Neural Computation},
	author = {Back, A. D. and Tsoi, A. C.},
	month = sep,
	year = {1991},
	pages = {375--385},
}

@article{atal_speech_1971,
	title = {Speech analysis and synthesis by linear {Prediction} of the speech {Wave}},
	volume = {50},
	issn = {0001-4966, 1520-8524},
	doi = {10.1121/1.1912679},
	abstract = {We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.},
	number = {2B},
	urldate = {2023-07-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Atal, B. S. and Hanauer, Suzanne L.},
	month = aug,
	year = {1971},
	pages = {637--655},
}

@article{tremblay2022flucoma,
	title = {Fluid corpus manipulation toolbox},
	author = {Tremblay, Pierre Alexandre and Green, Owen and Roma, Gerard and Bradbury, James and Moore, Theodore and Hart, Jacob and Harker, Alex},
	year = {2022},
	note = {Publisher: Zenodo},
	keywords = {FluCoMa},
}

@inproceedings{yang_torchaudio_2022,
	title = {Torchaudio: {Building} blocks for audio and speech processing},
	doi = {10.1109/icassp43922.2022.9747236},
	booktitle = {International conference on acoustics, speech and signal processing},
	author = {Yang, Yao-Yuan and Hira, Moto and Ni, Zhaoheng and Astafurov, Artyom and Chen, Caroline and Puhrsch, Christian and Pollack, David and Genzel, Dmitriy and Greenberg, Donny and Yang, Edward Z and {others}},
	year = {2022},
	pages = {6982--6986},
}

@misc{cycling2021maxsdk,
	title = {Max {SDK} documentation},
	url = {https://cycling74.com/sdk/max-sdk-8.2.0/},
	author = {{Cycling '74}},
	year = {2021},
}

@book{puckette2007theoryem,
	title = {The theory and techniques of electronic music},
	publisher = {World Scientific Publishing Company},
	author = {Puckette, Miller},
	year = {2007},
}

@inproceedings{schnell2009mubu,
	title = {{MuBu} and friends–assembling tools for content based real-time interactive audio processing in {Max}/{MSP}},
	booktitle = {International computer music conference},
	author = {Schnell, Norbert and Röbel, Axel and Schwarz, Diemo and Peeters, Geoffroy and Borghesi, Riccardo and {others}},
	year = {2009},
}

@inproceedings{smith2012mlstar,
	title = {Unsupervised play: {Machine} learning toolkit for max.},
	booktitle = {International conference on new interfaces for musical expression},
	author = {Smith, Benjamin D and Garnett, Guy E},
	year = {2012},
}

@misc{cycling2021mindevkit,
	title = {Min development kit documentation},
	url = {http://cycling74.github.io/min-devkit/},
	author = {{Cycling '74}},
	year = {2021},
}

@article{puckette2002max,
	title = {Max at seventeen},
	volume = {26},
	number = {4},
	journal = {Computer Music Journal},
	author = {Puckette, Miller},
	year = {2002},
	note = {Publisher: JSTOR},
	pages = {31--43},
}

@article{puckette1996puredata,
	title = {Pure {Data}: another integrated computer music environment},
	journal = {Proceedings of the Second Intercollege Computer Music Concerts},
	author = {Puckette, Miller and {others}},
	year = {1996},
	note = {Publisher: Tokyo, Japan},
	pages = {37--41},
}

@article{puckette1991max,
	title = {Combining event and signal processing in the {MAX} graphical programming environment},
	volume = {15},
	number = {3},
	journal = {Computer music journal},
	author = {Puckette, Miller},
	year = {1991},
	note = {Publisher: JSTOR},
	pages = {68--77},
}

@article{puckette1990max,
	title = {Max - {An} interactive graphic programming environment},
	journal = {Opcode Systems, Menlo Park, CA},
	author = {Puckette, Miller and Zicarelli, David},
	year = {1990},
}

@inproceedings{puckette1988patcher,
	title = {The patcher},
	booktitle = {Proceedings of the 1986 international computer music conference. {San} francisco},
	author = {Puckette, Miller},
	year = {1988},
	note = {tex.organization: Computer Music Association},
}

@inproceedings{alonso2021voiceddsp,
	title = {Explorations of singing voice synthesis using {DDSP}},
	booktitle = {18th sound and music computing conference, {SMC} 2021},
	author = {Alonso, Juan and Erkut, Cumhur},
	year = {2021},
	note = {tex.organization: Sound and Music Computing Network},
	pages = {183--190},
}

@inproceedings{zicarelli1992maxmsp,
	title = {An extensible real-time signal processing environment for {Max}},
	booktitle = {International computer music conference},
	author = {Zicarelli, David D},
	year = {1992},
}

@inproceedings{engel2018gansynth,
	title = {{GANSynth}: {Adversarial} neural audio synthesis},
	booktitle = {International conference on learning representations},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	year = {2018},
}

@inproceedings{lewis1988earlymusicml,
	title = {Creation by refinement: a creativity paradigm for gradient descent learning networks.},
	booktitle = {{IEEE} 1988 international conference on neural networks},
	author = {Lewis, John Peter},
	year = {1988},
	pages = {229--233},
}

@article{purwins2019deeplearningdsp,
	title = {Deep learning for audio signal processing},
	volume = {13},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {206--219},
}

@inproceedings{oord2016wavenet,
	title = {{WaveNet}: {A} generative model for raw audio},
	booktitle = {9th {ISCA} speech synthesis workshop},
	author = {van den Oord, Aäron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	year = {2016},
	pages = {125--125},
}

@inproceedings{kawamura2022ddspmixture,
	title = {Differentiable digital signal processing mixture model for synthesis parameter extraction from mixture of harmonic sounds},
	booktitle = {International conference on acoustics, speech and signal processing},
	author = {Kawamura, Masaya and Nakamura, Tomohiko and Kitamura, Daichi and Saruwatari, Hiroshi and Takahashi, Yu and Kondo, Kazunobu},
	year = {2022},
	note = {tex.organization: IEEE},
	pages = {941--945},
}

@inproceedings{masuda2021ddspsoundmatching,
	title = {Synthesizer sound matching with {Differentiable} {DSP}.},
	booktitle = {International symposium on music information retrieva},
	author = {Masuda, Naotake and Saito, Daisuke},
	year = {2021},
	pages = {428--434},
}

@inproceedings{todd1988earlymusicml,
	title = {A sequential network design for musical applications},
	booktitle = {Proceedings of the 1988 connectionist models summer school},
	author = {Todd, Peter},
	year = {1988},
	pages = {76--84},
}

@inproceedings{mehri2017samplernn,
	title = {{SampleRNN}: {An} unconditional end-to-end neural audio generation model},
	booktitle = {International conference on learning representations},
	author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
	year = {2017},
}

@inproceedings{donahue2018wavegan,
	title = {Adversarial audio synthesis},
	booktitle = {International conference on learning representations},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	year = {2019},
}

@article{shi2021surveynas,
	title = {A survey on audio synthesis and audio-visual multimodal processing},
	journal = {arXiv preprint arXiv:2108.00443},
	author = {Shi, Zhaofeng},
	year = {2021},
}

@misc{jonason2020controlsynthesis,
	title = {The control-synthesis approach for making expressive and controllable neural music synthesizers},
	author = {Jonason, Nicolas},
	year = {2020},
}

@inproceedings{ganis2021ddsp,
	title = {Real-time timbre transfer and sound synthesis using {DDSP}},
	doi = {10.5281/zenodo.5043235},
	booktitle = {{SMC} 2021 - proceedings of the 18th sound and music computing conference},
	author = {Ganis, Francesco and Knudsen, Erik F. and Lyster, Søren V.K. and Otterbein, Robin and Südholt, David and Erkut, Cumhur},
	year = {2021},
	note = {Edition: june},
	pages = {175--182},
}

@inproceedings{wu2021mididdsp,
	title = {{MIDI}-{DDSP}: {Detailed} control of musical performance via hierarchical modeling},
	booktitle = {International conference on learning representations},
	author = {Wu, Yusong and Manilow, Ethan and Deng, Yi and Swavely, Rigel and Kastner, Kyle and Cooijmans, Tim and Courville, Aaron and Huang, Cheng-Zhi Anna and Engel, Jesse},
	year = {2021},
}

@inproceedings{engel2017nsynth,
	title = {Neural audio synthesis of musical notes with {WaveNet} autoencoders},
	booktitle = {International conference on machine learning},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
	year = {2017},
	note = {tex.organization: PMLR},
	pages = {1068--1077},
}

@inproceedings{devis2021neurorack,
	title = {Neurorack: deep audio learning in hardware synthesizers},
	booktitle = {{EPFL} workshop on human factors in digital humanities},
	author = {Devis, Ninon and Esling, Philippe},
	year = {2021},
	note = {Number: CONF},
}

@article{caillon2022nntilde,
	title = {Streamable neural audio synthesis with non-causal convolutions},
	journal = {arXiv preprint arXiv:2204.07064},
	author = {Caillon, Antoine and Esling, Philippe},
	year = {2022},
}

@article{caillon2021rave,
	title = {{RAVE}: {A} variational autoencoder for fast and high-quality neural audio synthesis},
	journal = {arXiv preprint arXiv:2111.05011},
	author = {Caillon, Antoine and Esling, Philippe},
	year = {2021},
}

@inproceedings{carney2021tonetransfer,
	title = {Tone {Transfer}: {In}-browser interactive neural audio synthesis.},
	booktitle = {{IUI} workshops},
	author = {Carney, Michelle and Li, Chong and Toh, Edwin and Zada, Nida and Yu, Ping and Engel, Jesse},
	year = {2021},
}

@article{cho2014gru,
	title = {On the properties of neural machine translation: {Encoder}–{Decoder} approaches},
	journal = {Syntax, Semantics and Structure in Statistical Translation},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	year = {2014},
	note = {Publisher: Citeseer},
	pages = {103},
}

@article{ba2016layer,
	title = {Layer normalization},
	journal = {arXiv preprint arXiv:1607.06450},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
	year = {2016},
}

@misc{caillon2021ddsp,
	title = {{DDSP} {PyTorch}},
	url = {https://github.com/acids-ircam/ddsp_pytorch},
	publisher = {GitHub},
	author = {Caillon, Antoine},
	year = {2021},
}

@inproceedings{mcfee2015librosa,
	title = {librosa: {Audio} and music signal analysis in python},
	volume = {8},
	booktitle = {Proceedings of the 14th python in science conference},
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	year = {2015},
	note = {tex.organization: Citeseer},
	pages = {18--25},
}

@inproceedings{logan2000mfcc,
	title = {Mel frequency cepstral coefficients for music modeling},
	booktitle = {International symposium on music information retrieval},
	author = {Logan, Beth},
	year = {2000},
	note = {tex.organization: Citeseer},
}

@inproceedings{hantrakul2019loudness,
	title = {Fast and flexible neural audio synthesis},
	booktitle = {International symposium on music information retrieval},
	author = {Hantrakul, Lamtharn (Hanoi) and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
}

@inproceedings{harker2012hisstools,
	title = {The {HISSTools} impulse response toolbox: {Convolution} for the masses},
	booktitle = {Proceedings of the international computer music conference},
	author = {Harker, Alexander and Tremblay, Pierre Alexandre},
	year = {2012},
	note = {tex.organization: The International Computer Music Association},
	pages = {148--155},
}

@article{serra1990sms,
	title = {Spectral modeling synthesis: {A} sound analysis/synthesis system based on a deterministic plus stochastic decomposition},
	volume = {14},
	number = {4},
	journal = {Computer Music Journal},
	author = {Serra, Xavier and Smith, Julius},
	year = {1990},
	note = {Publisher: JSTOR},
	pages = {12--24},
}
